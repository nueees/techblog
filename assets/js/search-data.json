{
  
    
        "post0": {
            "title": "ELB (Elastic Load Balancing)",
            "content": "AWS Documentation . . 5. Networking &amp; Content Delivery . 5.1. ELB (Elastic Load Balancing) . ELB type . 1) Application Load Balancer: HTTP 요청 . 2) Network Load Balancer: 네트워크/전송 프로토콜(4계층 - TCP, UDP), high performance, low latency . 3) Classic Load Balancer: EC2 Classic instances에서만 권장 . . 5.2. VPC (Virtual Private Cloud) . 보안을 위해 AWS 리소스간 허용을 최소화하고 그룹별로 손쉽게 네트워크를 구성하기 위해 사용하는 AWS 계정 전용 가상 네트워크 . subnet . A range of IP addresses in your VPC. . network ACL . subnet간의 통신 규칙 관리 . routing table . A set of rules, called routes, that are used to determine where network traffic is directed. . internet gateway . A gateway that you attach to your VPC to enable communication between resources in your VPC and the internet. . VPC endpoint . Enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network. . NAT device . 인스턴스가 인터넷에 접근할 수 있도록 하고 인터넷 상에 호스트에서는 인스턴스로 접근할 수 없도록 막음 ( patch, data upload…) 1) NAT gateway: 대역폭 요구 늘어나면 auto scaling 2) NAT instance: bastion host로 사용해서 public IP 없이 인스턴스 연결 가능 . CIDR (Classless Inter-Domain Routing) block . Classless Inter-Domain Routing. An internet protocol address allocation and route aggregation methodology. IPv4 CIDR blocks . . 5.3. CloudFront .",
            "url": "https://nueees.github.io/techblog/aws/elb/2021/11/05/aws-elb.html",
            "relUrl": "/aws/elb/2021/11/05/aws-elb.html",
            "date": " • Nov 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "ECS (Elastic Container Service)",
            "content": "AWS Documentation . . 4. Containers . 4.1. ECS (Elastic Container Service) . . 4.2. EKS (Elastic Kubernetes Service) . . 4.3. ECR (Elastic Container Registry) . fully managed Docker container registry .",
            "url": "https://nueees.github.io/techblog/aws/ecs/2021/11/04/aws-ecs.html",
            "relUrl": "/aws/ecs/2021/11/04/aws-ecs.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "DynamoDB",
            "content": "AWS Documentation . . 3. Database . 3.1. DynamoDB . . 3.2. Aurora . . 3.3. RDS . . 3.4. Redshift .",
            "url": "https://nueees.github.io/techblog/aws/dynamodb/2021/11/03/aws-dynamodb.html",
            "relUrl": "/aws/dynamodb/2021/11/03/aws-dynamodb.html",
            "date": " • Nov 3, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "S3 (Simple Storage Service)",
            "content": "AWS Documentation . . 2. Storage . 2.1. S3 (Simple Storage Service) . Object Storage . 오브젝트 스토리지는 블록 스토리지와는 다르게 데이터 자체가 단독으로 구성됩니다. 고유 식별자가 존재하는 평면적인 주소 공간에 데이터 오브젝트 그 자체가 저장되므로 접근과 색인이 더 깔끔해집니다. 클라우드 서비스에서 기본적으로 사용되고 있으며, 찾고 싶은 데이터의 key(고유 식별자)만 알고 있다면, 색인을 이용해 접근이 편해집니다. . Lifecycle 구성 . Transition Action: 특정 시간이 지나면 이동 ( -&gt; Glacier) Expiration Action: 특정 시간이 지나면 삭제 . S3 type . Standard-Infrequent Access . 접근 빈도 낮고 필요시 빠른 접근해야하는 데이터 . One Zone-Infrequent Access . 단일 AZ에 중복 저장 후 Multi AZ에 저장 접근 빈도 낮은 데이터 . Glacier . 장기 보관(백업)용으로 검색 오래걸리나, Expedited Retrievals 사용시 빠르게 검색 가능 . Glacier Deep Archive . 장기 보관용이나 접근 빈도 낮을 때 사용 . . 2.3. EBS (Elastic Block Store) . Block Storage . 블록 스토리지는 파일이 균일한 블록에 저장되는 데이터 저장소를 의미합니다. 가장 일반적인 저장소의 역할을 수행합니다. 좀 더 자세하게 말하자면, 데이터를 블록 단위로 쪼개어서 별도로 ‘분리’ 해 저장하는 것을 의미합니다. 그 후 해당하는 데이터 요청 시 나누어진 데이터를 다시 결합해 제공,응답합니다. 주로 SAN(Storage Area Network) 환경에 배포됩니다.(컴퓨터에 HDD,SDD를 직접 꽃아 사용하는 것과 유사) . 단일 AZ에 데이터를 저장 . . EBS type . SSD: IOPS(Input/Output Operations Per Second)가 높고 high performance HDD: 처리량(Throughput)이 높고 작업 많을 때 . 추가로, 보안을 위해 KMS(Key Management Service)에서 제공되는 키 사용 가능, 스냅샷 기능 사용 가능 . . 2.4. EFS (Elastic File System) . 다중 AZ에 데이터를 저장, 연결 . . 2.5. Storage Gateway . on-premise에서 S3으로 migration (CDC)과 같은 Hybrid Cloud Storage 환경 load 작업 시 대용량 데이터 작업 시 -&gt; DataSync 사용 . 1) File Gateway NFS(Network File System), SMB(Server Message Block) protocol 지원 . 2) FSx File Gateway Windows File server 공유 할 때 . 3) Volume Gateway 자주 사용되는 데이터 caching 해서 사용, AWS에 비동기로 point-in-time 스냅샷 백업 시 . 4) Tape Gateway Glacier에 저장할 때 . . 2.6. DataSync . on-premise에서 S3, EFS(Elastic File System), FSx으로 대용량 load 작업 시 . . 2.7. FSx . Windows File system SMB(Server Message Block) protocol 지원 .",
            "url": "https://nueees.github.io/techblog/aws/s3/2021/11/02/aws-s3.html",
            "relUrl": "/aws/s3/2021/11/02/aws-s3.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "EC2 (Elastic Compute Cloud)",
            "content": "Coding Everybody, AWS Documentation . . 1. Compute . 1.1. EC2 (Elastic Compute Cloud) . EC2 Instance 생성 . 독립된 컴퓨터를 임대해주는 서비스 컴퓨터 1대 = Instance 1개 launch instance - 컴퓨터 생성 시, storage, memory, network interface, os 기본으로 제공 . EC2 Instance Type . Amazon Machine Image (AMI) . 1) OS : Linux(Ubuntu…), Window, … 2) Hardware spec . type (nano &lt; micro &lt; small &lt; medium &lt; large &lt; xlarge) : processor(vCPUs), Memory, Storage, Network Performance 우위에 따라 앞에 prefix가 다르게 붙음 | . EC2 Instance Configuration . Configure Instance Detail . 1) Number of Instances: 만들 인스턴스 개수 2) Purchasing option: 요금제 선택 (1) on-demand: second 단위 쓴 만큼 지불 (2) dedicated: hardware를 격리하여 사용하며, VPC(Virtual Private Cloud) 전용 가상 네트워크에서 주로 사용 (3) reserved: 1.3 year 약정으로 미리 지불 후 사용 (4) spot: 조정되는 spot price(시장 가격)에 맞게 실행되고 지불 3) Network: VPC(AWS 계정 전용 가상 네트워크) 설정 4) Shutdown behavior: 컴퓨터 껐을 때 인스턴스를 어떻게 할 것인지 (1) stop: 일시적으로 frizing (스토리지 비용만 나감) (2) terminated: 영구 삭제되고 재시작 안됨 (실수로 삭제할 수 있는 걸 방지하기 위한 protect termination 체크) (3) Instance hibernate (Elastic Block Store (EBS) only): memory, storage data 살려놓음 5) Monitoring: CPU, Memory 사용률을 자세히 확인 시 (추가 비용) . Add Storage . Size: size 설정 (default 8GB) Volume Type: SSD… IOPS: 저장 속도 Delete on Termination: 컴퓨터 폐기시 저장장치 폐기할지 (내장하드, 외장하드) . Tag Instance . EC2 resource 설명에 대한 필요한 값들 정의 Key: 이름 Value: Web server . Configure Security Group . 방화벽같은 기능 security group name 정한 뒤, 접속 정책 설정 type: SSH, HTTP, RDP(for windows)… protocol: TCP port range: 22 source: Anywhere(0.0.0.0/0), My IP(222.XXX.XX.XX/32), custom IP … . Review . 비밀번호 생성 (create a new key pair) 후 Launch . Networking &gt; Elastic Fabric Adapter &gt; Placement groups . 1) Cluster: 단일 AZ 내에 논리적 인스턴스 그룹 2) Partition: 여러 인스턴스 영역을 나누어 논리적 파티션을 나눔 (각 인스턴스 그룹이 서로 기본 하드웨어를 공유하지 않음) 3) Spread: 소규모의 인스턴스 그룹을 다른 기본 하드웨어로 물리적으로 분산하여 영역 나누어 관리 . EC2 Instance 관리 . Linux에서 instance 원격 제어 . 1) connect 버튼 클릭 2) open SSH client (원격 제어 프로그램) 실행 3) 인스턴스 생성 시 만든 패스워드 파일로 접속 . $ ssh -i &quot;aws_password.pem&quot; [사용자 계정]@[원격지 ip] . web server 사용 . $ sudo gpt-get install apache2 $ vi /var/www/html/index.html # 메인 페이지 수정 . AWS Marketplace 사용 (registry hub) . Wordpress . 1) launch instance 클릭 후 AMI를 Bitnami가 만든 Wordpress 선택 (추가 비용 드는 image도 있음) 2) 선택시 version, region, instance type, security group, key pair 지정 후 Launch with 1 click 3) deploy 후, public DNS로 들어가서 확인 4) document 확인해서 admin 가이드 확인 하면서 관리 필요 . . 1.2. AWS Elastic Beanstalk . automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. . . 1.3. Lambda .",
            "url": "https://nueees.github.io/techblog/aws/ec2/2021/11/01/aws-ec2.html",
            "relUrl": "/aws/ec2/2021/11/01/aws-ec2.html",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Data Lifecycle - Analyze & Visualize",
            "content": "Cloud Architecture Center . . . Process and analyze . 3.1. Processing large-scale data . source systems (Google Cloud Storage, Bigtable, Google Cloud SQL) 읽어온 큰 데이터를 처리하고 정규화하고 집계함. 데이터 양이 커서 클러스터로 분산 처리하거나 소프트웨어 툴들의 도움을 받음 . Dataproc . cluster에서 실행 (auto scaling) 기존 Hadoop, Hive, Spark 에플리캐이션과 연동 use case: Log processing, Reporting, On-demand Spark clusters, Machine learning . Dataflow . Serverless(cluster X), parallel 처리 (No-Ops) Spark나 Hadoop과 연동이 아닌 새로운 데이터 pipeline 처리 use case: MapReduce 안쓰는 parallel processing, User analytics, Data science, ETL, Log processing . Dataprep . UI-Driven Data Preparation (No-Ops, 필요시 scaling 가능) use case: Machine learning, Analytics . connect Dataprep to BigQuery . 1) Create Flow 2) Add dataset (import) 3) Select BigQuery (left pane) &amp; Create dataset 4) Import &amp; Add to Flow . inspect, process data . 1) Edit Recipe dataset의 sample을 Transformer view에서 확인 가능 (visualization) . execute job to load BigQuery . 1) click Run in Transformer page 2) click Edit on Create-CSV 3) select BigQuery then create table 4) name output table 5) click Update 6) click Run . job history에서 monitoring 가능 . . 3.2. Analyzing and querying data . BigQuery . 앞서 store 할 때 뿐만 아니라 분석할 때도 사용 use case: User analysis(adtech, clickstream, game telemetry), Device and operational metrics(IoT), BI . Machine learning . 처리된 결과를 확대시키거나 data-collection 최적화를 제공하기도 하고 결과 예측도 함 . 음성 인식 | 자연어 처리 | 번역 | 동영상 자동 분석 | AI 플랫폼 (TensorFlow) | . . Explore and visualize . . Cloud Architecture Center . . 4.1. Explore and visualize . Datalab . interactive한 web 기반 툴 pandas, numpy, scikit-learn 등의 다양한 toolkit 지원 . Data science ecosystem . Datalab 말고도, web 기반 툴인 Apache Zeppelin 지원 R 사용하면 Rstudio Server나 Microsoft ML Server 지원 Scala나 Java 사용하면 Jupyter 지원 . 4.2. Visualizing business intelligence results . Looker . BI platform . BI Engine . analysis service 관리 . Sheets . Spreadsheet visualization . Data Catalog . Data discovery and metadata management . Data Studio . Dashboarding and visualization over 200 connectors ( Google Analytics, BigQuery, Sheets, and external data sources…) . . Practice . 1) Intro . connect bigquery and data studio | visualize my data in data studio | . 2) getting set up . accesse public datasets ( catalog ) | set up my GCP ( my console ) | . 3) connecting data studio and bigquery . (1) Open data studio (2) Start with a Template (Blank template) (3) Add a data to report Connect to BigQuery (Allow permission to view BigQuery data) (4) Add table “san_francisco_311” (in public datasets) (5) Click Manage added data sources under Resources (6) Edit table fields of “311_service_requests” (7) “latitude”,”longitude” field from text to Latitude, Longitude in Geo . 4) creating visualizations . Click “Add a chart” and select “Treemap” | Place the “Treemap” chart and change the parameter (“category” field) | . 5) building a dashboard . Click “Add a chart”, select “Google Maps” and change its parameters (Dimension/Tooltip : “neighborhood”, Metric/Bubble size: “record count”) | Click “Add a chart” and select “Scorecard” | Select “Add a filter” in the Data panel and create filter (“Street and Sidewalk cleaning” in “category” field) | . 6) creating filters . Click “Filter Control” (set filter dimension to “neighborhood”) | Click “Arrange” and select “Make page-level” | . 7) test it and share it . practice_data_studio | . .",
            "url": "https://nueees.github.io/techblog/gcp/datastudio/2021/10/03/data-lifecycle-analyze-visualize.html",
            "relUrl": "/gcp/datastudio/2021/10/03/data-lifecycle-analyze-visualize.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Data Lifecycle - Store",
            "content": "Cloud Architecture Center . . . 2.1. Storing object data . bulk로 넣는 File 형식일 때 . Google Cloud Storage . 타 cloud storages이 제공하는 대부분의 특성을 가짐 . 1) Off-site management 2) Quick implementation 3) Cost-effective 4) Scalability 5) Business continuity . . 2.2. Storing database data . RDBMS와 NoSQL같은 DB 저장 시 . Relational (RDBMS) . Google Cloud SQL (MySQL, PostgreSQL) | Spanner (Horizontally scalable) | 기존 ecosystem (Microsoft SQL Server) | . Non-relational (NoSQL) . Google Bigtable(wide-column) | Firestore (Flexible, scalable) | 기존 ecosystem (MongoDB, Cassandra) | . . 2.3. Storing data warehouse data . 분석용도로 DW에 큰 data 저장 시 . BigQuery . managed DW . Feature . a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. . serverless cloud 서비스로 설치/운영이 필요없음 (NoOps) | RDBMS의 SQL 사용 (ANSI) | speed (in-memory BI Engine and machine learning built in) | handling big data supported by cloud scale infra | save 3 replication (data loss risk X) | batch / streaming . | easier than Hadoop, Spark: install, configure env, develop MapReduce logic… | . prerequisite: create Google Cloud project enable the BigQuery API create a service account (key) permissions to open the BigQuery Geo Viz permissions to open the Bucket1 . Interface . 1) Web UI . Query를 WEB UI 상으로 실행 . 3) Command-Line . Shell 사용해서 실행 . bq mk dataset . 3) REST API . Java나 Python 프로그래밍으로 구현 HTTP 실행 . from google.cloud import bigquery client = bigquery.Client() . 4) Geospatial analytics . Query를 WEB UI 상으로 실행해서 지도에서 표시 . example . 1) table . CREATE OR REPLACE TABLE ecommerce.revenue_transactions_20170801 #schema ( fullVisitorId STRING NOT NULL OPTIONS(description=&quot;Unique visitor ID&quot;), visitId STRING NOT NULL OPTIONS(description=&quot;ID of the session, not unique across all users&quot;), channelGrouping STRING NOT NULL OPTIONS(description=&quot;Channel e.g. Direct, Organic, Referral...&quot;), totalTransactionRevenue FLOAT64 NOT NULL OPTIONS(description=&quot;Revenue for the transaction&quot;) ) OPTIONS( description=&quot;Revenue transactions for 08/01/2017&quot; ) AS SELECT DISTINCT fullVisitorId, CAST(visitId AS STRING) AS visitId, channelGrouping, totalTransactionRevenue / 1000000 AS totalTransactionRevenue FROM `data-to-insights.ecommerce.all_sessions_raw` WHERE date = &#39;20170801&#39; AND totalTransactionRevenue IS NOT NULL #XX transactions ; . 2) view . CREATE OR REPLACE VIEW ecommerce.vw_large_transactions OPTIONS( description=&quot;large transactions for review&quot;, labels=[(&#39;org_unit&#39;,&#39;loss_prevention&#39;)], expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 90 DAY) # 90일 이내 생성된 뷰 조회 ) AS #standardSQL SELECT DISTINCT SESSION_USER() AS viewer_ldap, REGEXP_EXTRACT(SESSION_USER(), r&#39;@(.+)&#39;) AS domain, date, fullVisitorId, visitId, channelGrouping, totalTransactionRevenue / 1000000 AS totalTransactionRevenue, currencyCode, STRING_AGG(DISTINCT v2ProductName ORDER BY v2ProductName LIMIT 10) AS products_ordered FROM `data-to-insights.ecommerce.all_sessions_raw` WHERE (totalTransactionRevenue / 1000000) &gt; 1000 AND currencyCode = &#39;USD&#39; AND REGEXP_EXTRACT(SESSION_USER(), r&#39;@(.+)&#39;) IN (&#39;google.com&#39;) # session user가 google.com 인 경우만 조회 가능 GROUP BY 1,2,3,4,5,6,7,8 ORDER BY date DESC # latest transactions LIMIT 10; . session user 정보 조회 . SELECT SESSION_USER() AS viewer_ldap; . 3) partitioned table . CREATE OR REPLACE TABLE ecommerce.partition_by_day PARTITION BY date_formatted OPTIONS( description=&quot;a table partitioned by date&quot; ) AS SELECT DISTINCT PARSE_DATE(&quot;%Y%m%d&quot;, date) AS date_formatted, fullvisitorId FROM `data-to-insights.ecommerce.all_sessions_raw` . partition expiration 되게 하면 . CREATE OR REPLACE TABLE ecommerce.days_with_rain PARTITION BY date OPTIONS ( partition_expiration_days=60, # 60일 이후에 expire됨 description=&quot;weather stations with precipitation, partitioned by day&quot; ) AS SELECT DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) AS date, (SELECT ANY_VALUE(name) FROM `bigquery-public-data.noaa_gsod.stations` AS stations WHERE stations.usaf = stn) AS station_name, -- Stations may have multiple names prcp FROM `bigquery-public-data.noaa_gsod.gsod*` AS weather WHERE prcp &lt; 99.9 -- Filter unknown values AND prcp &gt; 0 -- Filter AND CAST(_TABLE_SUFFIX AS int64) &gt;= 2018 . 4) full outer join . SELECT DISTINCT website.productSKU AS website_SKU, inventory.SKU AS inventory_SKU FROM `data-to-insights.ecommerce.all_sessions_raw` AS website FULL JOIN `data-to-insights.ecommerce.products` AS inventory ON website.productSKU = inventory.SKU WHERE website.productSKU IS NULL OR inventory.SKU IS NULL . 5) cross join . CREATE OR REPLACE TABLE ecommerce.site_wide_promotion AS SELECT .05 AS discount; -- discount율 명시 테이블 만들고 INSERT INTO ecommerce.site_wide_promotion (discount) VALUES (.04), (.03); -- discount율 추가 SELECT DISTINCT productSKU, v2ProductCategory, discount FROM `data-to-insights.ecommerce.all_sessions_raw` AS website CROSS JOIN ecommerce.site_wide_promotion WHERE v2ProductCategory LIKE &#39;%Clearance%&#39; -- 각 물품별 .05,.04,.03 3가지 할인율로 나옴 (x3) . 6) duplication 제거 . #standardSQL # take the one name associated with a SKU WITH product_query AS ( SELECT DISTINCT v2ProductName, productSKU FROM `data-to-insights.ecommerce.all_sessions_raw` WHERE v2ProductName IS NOT NULL ) SELECT k.* FROM ( # aggregate the products into an array and # only take 1 result SELECT ARRAY_AGG(x LIMIT 1)[OFFSET(0)] k -- 1 row (0번째)만 가져오겠다 FROM product_query x GROUP BY productSKU # unique 해야하는 데이터 ); . Working with arrays . #standardSQL # how can we find the products with more than 1 sku? SELECT DISTINCT COUNT(DISTINCT productSKU) AS SKU_count, STRING_AGG(DISTINCT productSKU LIMIT 5) AS SKU, v2ProductName FROM `data-to-insights.ecommerce.all_sessions_raw` WHERE productSKU IS NOT NULL GROUP BY v2ProductName HAVING SKU_count &gt; 1 ORDER BY SKU_count DESC # product name is not unique (expected for variants) . 참고 . . for reducing the price . 1) query 하기 전에 preview 사용 (비용 X) 2) 사전에 query result size 체크 (오른쪽 display) 3) maximum billing limit 설정 4) prevent asterisk(*): column oriented storage로 wild card 사용시 column마다 압축 풀어서 가져와야 함 5) partition table 및 clustered index table 사용: random access 줄어듦 6) array 형 사용 7) 적절한 슬롯2 수 사용 ( on-demand / flat-rate pricing ) . . Cloud Data Fusion . Data pipeline 구축 및 관리 (fully-managed) . . 1) Pipelines create complex data processing workflows (both batch and realtime) using an intuitive UI (Directed Acylic Graph, DAG) . 2) Wrangler connect to data, and transform it using point-and-click transformation steps view, explore, and transform a small sample (10 MB) of your data in one place before running the logic on the entire dataset in the Pipeline Studio . 3) Metadata how datasets and programs are related to each other, full visibility into the impact of changes . 4) Hub distribute reusable applications, data, and code to all users in their organization . Wrangler . 1) create bucket . export BUCKET=$GOOGLE_CLOUD_PROJECT gsutil mb gs://$BUCKET . 2) data copy to bucker . gsutil cp gs://cloud-training/OCBL163/titanic.csv gs://$BUCKET . 3) Wangler로 preprocessing 작업 Google Cloud Storage에 있는 titanic raw가 있는 data 탭으로 가서 먼저 csv parsing을 함 . Preprocessing (CLI) . drop :body # 기존 body(:은 컬럼임) 날리기 fill-null-or-empty :Cabin &#39;none&#39; # Cabin 결측치 처리 send-to-error empty(Age) # Age empty면 에러 처리 parse-as-csv :Name &#39;,&#39; false # Name 컬럼 콤마 기준으로 컬럼을 두개로 나눔 drop Name fill-null-or-empty :Name_2 &#39;none&#39; rename Name_1 Last_Name rename Name_2 First_Name set-type :PassengerId integer parse-as-csv :First_Name &#39;.&#39; false drop First_Name drop First_Name_3 rename First_Name_1 Salutation fill-null-or-empty :First_Name_2 &#39;none&#39; rename First_Name_2 First_Name send-to-error !dq:isNumber(Age) || !dq:isInteger(Age) || (Age == 0 || Age &gt; 125) set-type :Age integer set-type :Fare double set-column Today_Fare (Fare * 23.4058)+1 generate-uuid id mask-shuffle First_Name . wrangler-docs 참고 . more에 가서 view schema 하면 해당 데이터 meta 정보 json으로 추출 가능 . Transformation steps 탭에 있는 다운로드 아이콘 누르면 preprocessing 내역 추출 가능 . 4) insight 탭으로 가서 columns별 데이터 distribution 시각화 된 것 확인 . . Pipelines Studio 내 Wrangler로 작업한 노드의 Properties에 들어가면, Wrangler로 한 작업들의 명세를 확인할 수 있음 . Pipelines . 1) target인 BigQuery에 dataset 미리 생성 (demo_cdf) 2) Data Fusion에서 create pipeline (Batch) 눌러서 Studio 이동 3) Sink 섹션에 있는 BigQuery를 배치해서 pipeline 연결하고 Properties 구성 (dataset: demo_cdf) 4) save 후 deploy 하고, run 5) summary에서 해당 job의 dash board 확인 가능 . elastic storage bins in Google Cloud Storage &#8617; . | parallel processing에 사용되는 virtual CPU 수 &#8617; . |",
            "url": "https://nueees.github.io/techblog/gcp/bigquery/2021/10/02/data-lifecycle-store.html",
            "relUrl": "/gcp/bigquery/2021/10/02/data-lifecycle-store.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Data Lifecycle - Ingest",
            "content": "Cloud Architecture Center . . . 0.1. Orchestration . Cloud Composer . Apache Airflow에 내장된 workflow orchestration service (위 그림 서비스들을 관리) . . 1.1. Ingesting app data . Data from app events, such as log files or user events, is typically collected in a push model, where the app calls an API to send the data to storage. . 1) Writing data to a file Cloud Storage에서 CSV file로 빼내거나 BigQuery로 DW에서 데이터를 뽑거나 . 2) Writing data to a database Cloud SQL 혹은 NoSQL인 Datastore 혹은 Bigtable로 데이터를 씀 . Cloud SQL . Fully managed relational database service for MySQL, PostgreSQL, and SQL Server . Datastore . a highly scalable NoSQL database for your web and mobile applications . Cloud Bigtable . A fully managed, scalable NoSQL database service for large analytical and operational workloads . Cloud Firestore . Easily develop rich applications using a fully managed, scalable, and serverless NoSQL document database . . 1.2. Ingesting streaming data . The data consists of a continuous stream of small, asynchronous messages. . 1) Streaming data as messages streaming 데이터를 Pub/Sub으로 보내서 받은 message를 processing 혹은 storing . Pub/Sub . Real-time messaging and ingestion for event-driven systems and streaming analytics . . 1.3. Ingesting bulk data . Large amounts of data are stored in a set of files that are transferred to storage in bulk. . 1) Scientific workloads Genetics data가 있는 VCF text file을 Cloud Storage에 upload (추후 Cloud Life Sciences로 처리) . 2) Migrating to the cloud on-premise Oracle DB에서 Google Cloud SQL DB로 migration 할 때 . 3) Backing up data Cloud Storage Transfer Service를 통해 replicating해서 백업 할 때 . 4) Importing legacy data 기존 legacy big data를 Bigquery (DW)로 import 할 때 . Cloud Storage . Object Storage . BigQuery . Serverless, highly scalable, and cost-effective multicloud data warehouse designed for business agility .",
            "url": "https://nueees.github.io/techblog/gcp/2021/10/01/data-lifecycle-ingest.html",
            "relUrl": "/gcp/2021/10/01/data-lifecycle-ingest.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Microservice",
            "content": "출처_Building Cloud private native 전문가 양성과정 교재 + 추가 MSA . 8. 클라우드 서비스 소개 . 8.1. 클라우드 서비스 개요 . 네트워크를 이용해서 사용자들이 원하는 방식으로 서비스를 제공 | 확장성 | 사용 모델 | 분산 | 개인 시스템의 성능에 구애받지 않음 | . 장점 . 초기 구성 비용 절감 | 초기 구성 시간 절약 | 확장성이 | . 단점 . 오랜 기간 사용할 경우 비용 부담 증가 | . . 8.2. 클라우드 서비스 분류 . SaaS (Software as a Service): 서비스로서 소프트웨어 애플리케이션 서비스를 제공 . 보통 IaaS, PaaS위에 올라가고, 중앙에서 호스팅되고 있는 소프트웨어를 웹 브라우저 같은 애플리케이션을 통해 사용 ex) Google Docs . PaaS (Platform as a Service): 서비스로서 플랫폼 소프트웨어를 제공 . SaaS의 개념을 개발 플랫폼으로 확장한 방식 플랫폼(OS)를 웹에서 쉽게 빌려 사용 확장성과 경제적 이유로 On-Premise환경을 Cloud로 확장 ex) Google APP Engine, OpenShift . IaaS (Infrastructure as a service):서비스로서 인프라 자원 제공 . Server, Storage, Network를 가상으로 만들어 사용자가 필요한 자원 사용 관리와 책임이 클라우드 소비자에게 존재 . ex) 대부분 퍼블릭 클라우드 서비스 (AWS EC2, S3) . 8.3. 클라우드 서비스 종류 . Private Cloud . 자체적으로 데이터센터 안에 클라우드 환경 구축 . 자산 스스로 보유, 구축 | 기존 IT 인프라 자원 활용 가능 | 소규모로 구축할 때 비용이 높음 | 보안 서비스를 자체적으로 구축해야 함 | . Public Cloud . 비용을 지불하고 서비스 제공 업체가 구축한 Server, Storage, Network 등 IT Infra 사용 . 공용 클라우드는 가입 형태의 서비스 | 대규모 서비스로 구축 시 비용 절감 | 서비스 제공자가 구축한 보안 서비스 안에서 운용 | . Hybrid Cloud . 공용 + 사설 클라우드의 장점만 선택해서 사용 . 필요에 의해 데이터나 컴퓨팅 자원의 위치 조절 | 데이터의 중요도와 비즈니스 핵심 업무 여부에 따라 선택 가능 | . . 9. Microservice . 9.1 Microservice 이해 및 구축 방법론 . 서비스를 비즈니스 경계에 맞게 세분화 하고, 서비스 간 통신은 네트워크 호출을 통해 진행하여 확장 가능하고 회복적이며 유연한 어플리케이션을 구성하는 것 . Microservice 특징 . 기존의 Monolithic 방식은 변화 대응이 어렵고, 새로운 기능 추가 및 업데이트가 어려움 . 서비스마다 필요한 기술 적용 (기술 이기종성) | 장애 격리가 쉽고 신뢰도가 높음 | resource 낭비 줄임 (부분 scale out) | deploy/rollback 시 발생하는 risk 감소 (Blue-Green deploy로 점진적 변경) | . ※ 시스템이 복잡하지 않을 경우, MSA 방식에서 요구되는 요소들이 오히려 생산성을 떨어뜨림 소스코드를 여러 라이브러리로 나누어서 유틸 생성하고 재사용 라이브러리를 통해 Java에 Module 컨셉을 부여해서 여러 버전의 모듈관리 (OSGI/Erlang) . Microservice 구성 전략 . 비즈니스 개념에 따른 모델링 도메인 경계 정의, 기술보단 비즈니스 경계로, 서비스는 두고 DB 스키마부터 단계적 분리 | 공통 자동화 문화 적용 테스트 자동화 (CI1/CD2) 및 Custom Image 환경 구축 | . Blue-Green 배포 . Blue: 이전 버전 Green: 신규 버전 . canaria 배포 . . 배포관련 참고 . . 9.2 Spring Cloud를 활용한 Microservices 개발 . Spring Cloud . Spring Cloud는 Microservce Architecture 환경에서의 개발과 배포, 운영을 쉽게 구성할 수있도록 지원하는 Spring Boot 기반의 Framework Cloud Native한 패턴과 메커니즘을 제공 많은 서비스들이 Java 기반의 Spring Boot로 개발되기에, Java 기반 MSA를 구축하고자 할 시 최적의 선택 . Spring Boot . 기존 Spring 이 가지고 있던 문제들을 해결하여, 보다 빠르게 상용 Spring Application을 개발하고자 시작 Conventioin over Configuration (설정보다 관례) . Library 관리 자동화 (Starter 이용) | 자동 environment configuration (Annotaion 이용) | 내장 Tomcat 및 JAR 모델 지원 ( Web Application이어도 JAR로 packaging runtime 지원) | . Annotaion . 마치 코드의 메타데이터 같은 역할을 하여, Compile, Runtime 과정에서 코드를 어떻게 처리할 것인지 나타냄 Annotation을 통해 Spring Boot가 지향하는 Convention over Configuration을 가능하게 함 . @SpringBootApplication public class TestApplication { public static void main (String[] args) { SpringApplicatoin.run(MyApplication.class, args); } } . MSA architecture . MSA architecture 참고 링크 . MSA component . 1) Edge Server (API Gateway): Zuul/Ribbon, Spring Cloud GW 모든 클라이언트 요청에 대한 end point를 통합하는 서버(프록시 서버처럼 동작) 한 서비스에 한개 이상의 서버가 존재하기 때문에 이 서비스를 사용하는 클라이언트 입장에서는 다수의 end point가 생기게 되며, end point를 변경이 일어났을때, 관리하기가 힘들다. 이런 문제를 해결하기 위해 서비스들의 end point를 하나로 묶을 수 있는 API 게이트웨이가 필요 각 서비스를 직접 호출하지않고 모든 요청이 API 게이트웨이를 통하게 만드는 것 하나의 요청에 여러 서비스를 호출한 후 하나의 결과로 취합 . 요청에 따라 필요한 서비스로 라우팅 모든 서비스의 API 를 노출하는 대신 필요한 API 만을 노출해서 캡슐화 클라이언트 별로 다른 API 를 제공 하나의 요청에 필요한 서비스를 각각 호출해 결과를 모아서 응답 내부에서 사용하는 프로토콜이 다를 경우 외부에는 웹 친화적인 프로토콜(HTTP, WebSocket 등)으로 변환 클라이언트와의 통신을 줄일 수 있고, 클라이언트의 코드도 단순 인증 및 권한, 모니터링, load balancing, caching, logging 등 추가적인 기능 . 하나의 엔트리 포인트를 갖는 것은 장점이자 단점 . API 게이트웨이에서 하는 역할이 많고, 게이트웨이에 장애가 나면 서비스 전체가 사용이 불가능 -&gt; circuit breaker 각 서비스의 API 를 수정하면 API 게이트웨이를 함께 수정해야 합니다. 이는 개발 과정에서 병목(bottleneck)이 되어 개발 과정 지연 API 게이트웨이 또한 개발하고 유지보수해야 할 대상 . 2) Load Balancer: Ribbon . ServerList: 로드 밸런싱 대상 서버 목록 configuration을 통해 static 하게 설정 가능 eureka 등을 기반으로 dynamic하게 설정 가능 | Rule: 요청을 보낼 서버를 선택 Round Robbin - 한 서버씩 돌아가며 전달 Available Filtering - 에러가 많은 서버 제외 weighted Response Time - 서버별 응답 시간에 따라 확률 조절 | Ping: 서버list가 살아있는지 체크 static, dynamic 모두 가능 | . 3) Service Discovery (Service Registry): Eureka, Spring Cloud LB . 클라우드에서 인스턴스는 동적으로 할당되기 때문에 IP주소나 포트 정보가 정해지지 않은 데다가 오토스케일링도 일어나고 중지되고 복구되면서 네트워크 위치가 계속해서 바뀌게 됩니다. . 따라서 클라이언트나 API 게이트웨이가 호출할 서비스를 찾는 매커니즘이 필요하고 이를 서비스 디스커버리(Service Discovery)라고 합니다. 이러한 로직을 구현하는 쪽에 따라서 두 가지 방식으로 나뉩니다. . 클라이언트 사이드 디스커버리 패턴(Client-Side Discovery Pattern) | 서버 사이드 디스커버리 패턴(Server-Side Discovery Pattern) | . API 게이트웨이는 각 서비스를 호출하기 위해 IP 주소와 포트를 알고 있어야 합니다. 기존 환경에서는 이러한 서버의 위치가 고정이라 문제가 없지만, 클라우드 기반에서는 각 서비스가 동적으로 할당된 서버에 배포되면서 해당 서비스의 위치를 파악하는 것이 어려워졌습니다. 이렇게 해당 서비스의 위치를 찾는 기술을 서비스 디스커버리(Service Discovery)라고 합니다. API 게이트웨이는 서버 사이드, 혹은 클라이언트 사이드 기준으로 서비스 디스커버리를 구현할 수 있습니다. . 4) Circuit Breaker: Hystrix, Resilence4j . 회로 차단기 (Circuit Breaker) 이름 그대로 전기선에 유입된 과전류를 감지하는데 이와 마찬가지로 하위 컴포넌트가 과전류에 손상되지 않도록 보호 원격 자원에 대한 모든 호출을 모니터링하고 임계점까지 실패한다면 회로 차단기가 활성화 되어 빠른 실패를 유도, 그리고 문제가 있는 원격 자원을 더 이상 호출하지 못하게 함 | 폴백 (Fallback) 원격 서비스에 대한 호출이 실패할 때, 대체 코드(fallback method)를 실행 | 벌크헤드 (Bulkhead) 서비스에 대한 동시 요청 수를 제한 원격 자원에 대한 호출을 자원 별 스레드 풀로 분리 특정 원격 자원 호출이 느려지더라도 다른 자원들에 대한 호출은 영향이 없음 | . 5) Config Server: Spring Cloud Config Server Spring Cloud Config는 분산 시스템에서 환경설정을 외부로 분리하여 중앙에서 관리할 수 있는 기능을 제공 Application data(환경별 구성 데이터)를 microservice와 분리 따라서 마이크로서비스 인스턴스가 아무리 많더라도 항상 동일한 구성을 유지할 수 있음 Spring Cloud Config에는 고유한 관리 저장소가 있지만 아래의 오픈 소스 프로젝트와도 통합할 수 있음 . Spring Config Server : 공통 환경설정을 가지고 있는 Config Cloud Server 환경설정(name-value pair, YAML 파일)을 위한 HTTP, 리소스 기반 API 속성 값 암호화 및 암호 해독 (대칭 또는 비대칭) @EnableConfigServer 어노테이션을 사용하여 쉽게 Spring Boot 어플케이션에 적용 . | Spring Config Client : 공통 환경설정을 받아 사용하는 Application Server Config Server에 붙어 원격 속성 소스로 Spring 환경 초기화 속성 값 암호화 및 암호 해독 (대칭 또는 비대칭) . | . 6) Message Stream: RabbitMQ, Kafka Spring cloud bus: 동적으로 config 변경을 적용하기 위한 MOM3을 구성한 MQ(Message Queue) Handler . (1) MQ(Message Queue)에 Publisher(=config server)와 Subscriber(마이크로서비스)를 등록 (2) config 변경 정보를 MQ에 전송(Publish Message) (3) 각 마이크로서비스에서 config 동적 반영(Reload Config) . 7) Monitoring: Zipkin . . . 자동화된 빌드 및 테스트가 수행된 후, 개발자가 코드 변경을 repository에 정기적으로 병합하는 개발 방식 &#8617; . | 코드 변경 사항을 repository에서 고객이 사용 가능한 production 환경까지 자동으로 release하는 개발 방식 &#8617; . | 분산 시스템 환경에서 독립된 서비스 간에 비동기 방식으로 메세지를 전송/수신할 수 있게 도와주는 인프라 &#8617; . |",
            "url": "https://nueees.github.io/techblog/kubernetes/2021/09/03/microservice.html",
            "relUrl": "/kubernetes/2021/09/03/microservice.html",
            "date": " • Sep 3, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Kubernetes",
            "content": "출처_Building Cloud private native 전문가 양성과정 교재 . 5.Container Cluster . 5.1.Kubernates 소개 . docker - 단일 시스템에서만 다수의 컨테이너 관리 -&gt; 다수의 시스템과 애프리케이션 설정을 쉽게 설정하고 유지보수할 수 있는 방식인 오케스트레이션(Ochestration)이 필요 kubernates - 구글에서 개발된 컨테이너 오케스트레이션 도구 . 다중 컨테이너 관리를 위한 docker-compose를 설치해야 함. . 쿠버네티스는 클러스터 구성해서 오케스트레이션을 통해 컨테이너를 자동으로 관리, 2개 이상 시스템에서 관리 가능 관리 대상을 object라고 함 - pods(컨테이너 단위)와 controller(pods를 한번에 관리)로 구성(application workload) . 기능 . 컨테이너 플랫폼, 마이크로서비스 플랫폼, 이식성있는 클라우드 플랫폼 제공 . Automatic Binpacking | Storage Orchestration | Secret &amp; Configuration Management | Horizontal Scaling | Service Discovery &amp; Load Balancing | Self Healing | Batch Execution | Automatic Rollbacks &amp; Rollouts | . CI/CD 파이프라인, 애플리케이션 레벨의 서비스, 로깅, 모니터링, 경고 솔루션 등을 제공하지 않음 . . 사전 설치 . choco 패키지 관리도구 설치 | $ Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager ]::SecurityProtocol = [System.Net.ServicePointManager ]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&#39;[https://community.chocolatey.org/install.ps1&#39;](https://community.chocolatey.org/install.ps1&#39;))) | vagrant 설치혹은 vagrant.exe 파일 다운로드 후 설치 가능 | $ choco install vagrant | 하이퍼바이저 설치 virtualbox 설치 (확장팩) | vagrant 명령어로 구성요소 설치 vagrant plugin install vagrant-hostmanager vagrant plugin install vagrant-disksize 확인: vagrant plugin list윈도우의 경우 직접 작성하면 확장자이름이 자동설정될 수 있으니 주의 | $ vagrant box add ubuntu /bionic64 $ vagrant up =&gt; Vagrantfile 이 있는 곳에서 실행 | 가상머신에 쿠버네티스 배포 ssh 설정 | $ ssh-keygen $ ssh-copy-id node1 (노드 모두 입력) localhost 도 설정 | . kubenates 설치 . 1. 패키지 및 git 설치 $ sudo apt update $ sudo apt upgrade -y $ sudo apt install -y python3 python3-pip git $ git clone --single-branch --branch release-2.14 https://github.com/kubernetes-sigs/kubespray.git $ cd kubespray/ $ sudo pip3 install -r requirements.txt` 2. 인벤토리 수정 $ cp -rfp inventory/sample/ inventory/mycluster $ vim inventory/mycluster/inventory.ini [all] node1 ansible_host=192.168.56.21 ip=192.168.56.21 node2 ansible_host=192.168.56.22 ip=192.168.56.22 node3 ansible_host=192.168.56.23 ip=192.168.56.23 controll-plane ansible_host=192.168.56.11 ip=192.168.56.11 [all:vars] ansible_python_interpreter=/usr/bin/python3 [kube-master] controll-plane [etcd] controll-plane [kube-node] node2 node3 node1 [calico-rr] [k8s-cluster:children] kube-master kube-node calico-rr` $ sudo vim /etc/hosts 192.168.56.11 controll-plane.example.com controll-plane 192.168.56.21 node1.example.com node1 192.168.56.22 node2.example.com node2 192.168.56.23 node3.example.com node3 $ vim inventory/mycluster/group_vars/k8s-cluster/addons.yml metrics_server_enabled: true ingress_nginx_enabled: true metallb_enabled: true metallb_ip_range: - &quot;192.168.56.50-192.168.56.99&quot; metallb_protocol: &quot;layer2&quot; $ vim inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml kube_proxy_strict_arp: true 3. 플레이북 실행 $ ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b` 4. kubectl 설치 $ curl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot; $ curl -LO &quot;https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256&quot; $ echo &quot;$(&lt;kubectl.sha256) kubectl&quot; | sha256sum --check $ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl $ kubectl version --client` . . 5.1.Architecture . . kube-api-server를 통해서 대부분의 작업을 함 kube-controller-manager : on-premise 환경에서 각 리소스 관리 cloud-controller-manager : cloud 환경에서 밴더사 자체적으로 관리 . 마스터(master) . cluster를 구성하기 위한 핵심 요소들의 모음 cluster 조정을 위한 control plane 제공 노드에 작업 분배하는 scheduling 작업 cluster evnet 감지하고 대응 운영환경에서는 multi master 환경을 구성 (FT) . 1) API 서버(kube-apiserver) kubernetes API를 노출하는 component kubernetes object 관리, 제어를 위한 front-end . 2) etcd cluster의 meta 정보 정장 key-value 형태로 저장 cluster configuration 정보 보관하고 있으므로 백업 필수 . 3) scheduler cluster 내 생성되는(혹은 배정되지 않은) pod를 감지하고 그것을 구동할 노드를 선택 resourse 상태, HW/SW/Policy 제약, Affinity 등 다양한 기준에 따라 배치 결정 . 4) cube-controller-manager . node controller: 노드 관리, 노드 다운시 대응 | replication controller: 복제 컨트롤러를 사용하는 모든 object관리, 적정 pod 수 유지 | endpoint controller: service와 pod 연결 (물리적 시스템과 컨테이너 내부 어플리케이션 연결) | service account &amp; token controller: 쿠버네티스 namespace, account, token 등 인증 관련된 것 담당 | . 5) cloud-controller-manager . AWS, GCP 등 각 밴더 별 클라우드 서비스 관리 . node controller: 노드 관리, 노드 다운시 대응 | route controller: 클라우드 환경의 네트워크 경로 | service controller: 클라우드 로드밸런서 관리 | volume controller: 클라우드 볼륨 관리 | . 워커 노드(worker node) . container를 실행하고 동작중인 Pods 유지시키고 kubernetes runtime 환경 구성 (minion이라고도 함) . 1) kubelet 각 노드에서 실행되는 agent로 마스터로부터 제공받은 구성 정보, 노드가 수행해야할 작업 동작 . 2) kube-proxy 네트워크 규칙 유지하고 연결에 대한 포워딩 수행하므로써 서비스 추상화 가능하게 함 컨테이너에 연결될 네트워크 구성 관리 . 3) container runtime 실제 컨테이너 동작을 책임지는 구성요소 . Docker | containerd | CRI-O | rktlet | Kubernetes CRI(Container Runtime Interface) | . 추가 요소(add-on) . 구성요소 설명 . 클러스터 DNS | 쿠버네티스 클러스터 내의 여러 오브젝트에 대하여 주소기반으로 오브젝트를 접근할 수 있는 DNS 제공 | . DashBoard | 모니터링 용 웹기반 인터페이스 | . 컨테이너 resource 모니터링 | 컨테이너 리소스 사용량의 시계열 매트릭스 기록 | . 클러스터 logging | 컨테이너 로그를 중앙로그저장소에 저장하고 관리, 로그를 검색/열람하는 인터페이스 제공 | . 쿠버네티스 API . 1) API 버전 규칙 alpha 버전 API : 개발 초기 버전 beta 버전 API : 일정 수준 이상 테스트가 된 버전 stable 버전 : vX 형태의 안정화된 버전 . 2) API 그룹 core group: apiVersion:[version] core 이외 group: apiVersion: [group]/[version] . . 5.3.Menifest . 필드 설명 . apiVersion | object를 생성하기 위한 API 버전을 지정 | . kind | object 종류 | . metadata | name, label, namespace 등 기본정보 | . spec | object 세부 상태 | . Managing Object . 1) Imperative commands(명령형 커맨드) 재사용성이 떨어져 개발 환경에서 테스트 시, 일회성 작업일 경우 사용 . $ kubectl run nginx --image nginx . 2) Imperative object configuration(명령형 오브젝트 구성) yaml, json 포맷으로 파일을 작성하여 kubectl은 작성된 파일을 참고하여 실행 . $ kubectl create -f nginx.yaml . 3) Declarative object configuration(선언형 오브젝트 구성) 특정 디렉토리에 오브젝트 파일을 배치하고 작성된 파일을 참고하여 오브젝트 관리 . $ kubectl diff -f configs/ $ kubectl apply -f configs/ . Running App with Imperative commands 예시 . $ kubectl run mytest-app --image=&lt;ACCOUNT&gt;/myweb --port=8080 --generator=run/v1 # 레플리케이션 컨트롤러(pod) 생성 $ kubectl get pods $ kubectl get replicationcontrollers $ kubectl expose replicationcontroller mytest-app --type=LoadBalancer --name myweb-svc # 서비스 생성 $ kubectl get services $ curl http://192.168.56.11:31289 $ kubectl scale replicationcontroller mytest-app --replicas=3 $ kubectl get pods $ kubectl get replicationcontrollers $ curl http://192.168.56.11:31289 $ kubectl get all $ kubectl delete replicationcontrollers mytest-app $ kubectl delete service myweb-svc . . 6.Application Workload . 6.1.Pods . 도커에서 작업을 수행하기 위해 구동해야 하는 가장 작은 단위는 컨테이너 쿠버네티스 클러스터 내에서 애플리케이션 배포하며 동작하는 단위 1 컨테이너 = 1 애플리케이션 하나의 Pod는 하나의 node에서만 동작 동일한 Pod 의 모든 컨테이너는 동일한 리소스와 로컬 네트워크를 공유하여 머신이 분리되어 있어도 pod 내 컨테이너 간 통신이 가능함 쿠버네티스 클러스터는 여러 개의 노드로 구성되며 각 노드는 컨테이너를 구동할 수 있도록 준비하고 있으나, 하나의 파드에 두개 이상의 컨테이너가 포함된 경우 각 컨테이너를 여러 노드에 분산시켜서 실행할 수는 없음. 1 pod 내 있는 컨테이너는 저장소, 네트워크 IP 등 공유 . Pod 정의: YAML 파일 생성 | cat testapp-pod.yml . . 기본적인 apiVersion, kind, metadata, spec 포함 - 하이픈 기호는 리스트(List)를 의미 . Pod 생성 및 확인 | $ kubectl create -f testapp-pod.yml $ kubectl get pods $ kubectl get pods testapp-pod -o yaml(-o json) $ kubectl discribe pods testapp-pod $ kubectl logs testapp-pod # 로그 확인 $ kubectl port-forward testapp-pod 8080:8080 # 포트포워딩 $ curl http://localhost:8080 | Label(레이블) 및 Selector(셀렉터) Label: 쿠버네티스 클러스터의 모든 오브젝트에 키/값 쌍으로 이루어진 값을 설정하여 리소스 식별, 속성 지정 역할 네임스페이스 내 중복 불가 Label Selector: Label을 식별하고 검색함 | cat testapp-pod-label.yml . . Anotation(어노테이션) 오브젝트의 추가 정보를 기록하는 경우 사용하는 주석 | $ kubectl annotate pods testapp-pod devops-team/developer=&quot;nueees&quot; | Name Space(네임스페이스) Name Space: 쿠버네티스 클러스터 내 오브젝트와 리소스를 용도와 목적에 따라 논리적으로 완전히 분리된 환경default: 기본 네임스페이스 kube-node-lease: 쿠버네티스 노드(마스터/노드)의 가용성 체크를 위한 네임스페이스 kube-public: 모든 사용자 접근가능 kube-system: 클러스터의 리소스가 배치되는 네임스페이스 | $ kubectl get namespaces | Liveness Prove(라이브니스 프로브) 파드 상태가 정상적인지 주기적으로 모니터링 서비스 | HTTP GET Prove: 특정 경로에 HTTP GET 요청, HTTP 응답코드가 2XX/3XX인지 확인 | TCP Socket Prove: 특정 TCP port 연결 시도 | Exec Prove: 컨테이너 내부의 바이너리(명령)를 실행하고 종료 코드 확인 cat testapp-pod-liveness.yml | . $ kubectl create -f testapp-pod-liveness.yml . . 6.2.Controller . 사용자가 의도한 상태로 유지 해 주는 기능 . Deployment . stateless Application 배포 시 사용 Applicaion은 컨테이너 집합인 Pod 단위로 배포 사용자의 기대상태(Desired state)를 유지하도록 하는 controller . Liveness Probe: 응답 체크 | Readlness Probe: 서비스 가능 상태 체크 ReplicaSet에 대한 Update 담당 | . use case: . 신규 ReplicaSet을 생성하여 Pod를 새로운 버전으로 점진적 교체 수행 (Rolling Update) | Application configuration 분리 (Decoupling): Config Map | . ReplicaSet (레플리카셋) . 사용자가 요구하는 복제본 개수만큼 Pod를 복제하고 관리하는 기능 주로 Deployment 의 spec 으로 정의하는 것을 권장함 관리해야 하는 pod을 식별하기 위한 selector, 유지해야 하는 pod의 개수, pod template 포함 . Pod의 다중 레이블 조건 지원 | Pod에 설정된 레이블의 키 조재 여부 조건 선택 가능 | . cat testapp-rs.yml . . $ kubectl create -f testapp-rs.yml $ kubectl get replicasets.apps . cat testapp-rs-exp.yml . . $ kubectl create -f testapp-rs-exp.yml $ kubectl get replicasets.apps -o wide . DaemonSet (데몬셋) . 쿠버네티스 클러스터는 부하 분산, 이중화를 통한 장애 대응 목적 등을 위하여 최소 하나 이상의 노드로 구성되므로 다수의 노드를 사용할 경우 필요에 따라 각 노드별*로 특정 목적을 수행하는 *파드를 한 개씩 배치하여야 하는 경우 발생 . 레플리카셋과 비슷하지만 복제본을 지정하지 않음 . cat testapp-ds.yml . . nodeSelector에 Pod가 배포될 노드 선택 . $ kubectl create -f testapp-ds.yml $ kubectl get replicasets.apps -o wide $ kubectl label nodes kube-node1 node=development # node에 label지정 $ kubectl label nodes kube-node1 --show-label . StatefulSet . Pod이 스케줄 될 때 지속적으로 유지되는 식별자를 가질 수 있도록 관리하는 object use case: 고유한 네트워크 식별자, 지속성을 갖는 스토리지(persistent volumes), 순차적 배포와 스케일링, 순차적인 자동 . . 7.Network - Service . 7.1. Service 생성 . Service: 쿠버네티스 시스템에서 같은 애플리케이션을 실행하도록 구성된 컨트롤러에 의해 생성된 Pod 그룹에 단일 네트워크 진입점 제공 . 서비스에 부여된 IP는 해당 서비스가 종료될 때까지 유지하고 클라이언트는 이 서비스에 부여된 고정 IP 및 PORT를 통해 Pod에 접근 가능 | 클러스터 내 Pod들에게 접근하기 위한 방법으로 사용 | 여러 Pod를 묶어 Healthy한 Pod로 Traffic 라우팅하는 로드 밸런싱 기능 제공 | 클러스터의 Service CIDR 중에서 지정된 IP로 생성 가능 | 서비스 이름은 클러스터내 고유한 DNS로 동작 | . cat testapp-svc.yml . . $ kubectl create -f testapp-svc.yml $ kubectl get services # 진입점인 서비스 $ kubectl get endpoints testapp-svc # 엔드포인트 -&gt; 레플리카셋 컨트롤러의 파드 . 엔드포인트: 최종 목적지인 파드의 주소 및 포트 정보 . $ kubectl run nettool -it --image= &lt;ACCOUNT &gt;/network-multitool # 서비스 접근 테스트 . Session Affinity: 클라이언트가 특정 파드(웹서비스) 요청 시 이전에 처리된 파드로 동일하게 전달하여 처리 . cat testapp-svc-ses-aff.yml . . $ kubectl create -f testapp-svc-ses-aff.yml . Configuring Service Multi-Port . Pod는 하나 이상의 컨테이너로 구성되어 있어서, 각 컨테이너는 서로 다른 포트 사용 가능 . cat testapp-svc-multiport.yml . . $ kubectl create -f testapp-svc-multiport.yml . Configuring Service by named-port . 레플리카셋 컨트롤러의 Pod 템플릿에서 생성될 컨테이너의 포트에 이름을 부여하여 포트 이름 구성 가능 . cat testapp-rs-named-port.yml . . cat testapp-svc-named-port.yml . . $ kubectl create -f testapp-rs-named-port.yml -f testapp-svc-named-port.yml . 7.2. Service 탐색 . kubectl get 명령어로 IP 주소를 수동적으로 확인할 수 있지만 Object끼리 통신을 위한 방식 필요 . 환경변수 방식 | 쿠버네티스 클러스터 내 DNS 사용 방식 | DNS를 이용한 Service 탐색 . kube-systme 네임스페이스에서 쿠버네티스에 등록된 구성요소 확인 가능 그중 k8s-app=kube-dns 레이블 옵션을 통해 DNS 관련 파드 확인 . 1) DNS 관련 리소스 확인 . $ kubectl get all -n kube-system -l k8s-app=kube-dns . 2) 파드 내부 DNS 설정 확인 이름 기반의 주소로 네트워크에 접근하기 위해서 DNS 설정 필요 . $ kubectl exec testapp-rs-m65m4 -- cat /etc/resolv.conf # 위의 coredns 서비스 IP와 다름 . 각 파드의 DNS로 등록되어 있는 위 IP(169.254.0.0/16 형식)는 IPv4 주소형식에서 ‘Link Local Address’이며, 유효한 IP 주소가 아님 3) NoceLocal DNSCache 해당 주소(169.254.25.10)는 coreDNS로 직접적으로 요청이 전달되지 않게 하는 중간 단계 캐시 DNS DaemonSet 형태로 쿠버네티스의 각 노드마다 DNS 캐시 기능을 하는 Pod 배치하여 필요한 경우에만 CoreDNS 호출하는 구조 . Pod &lt;-&gt; NodeLocal DNSCache &lt;-&gt; iptables &lt;-&gt; coreDNS . $ kubectl get daemonsets.apps -l k8s-app=kube-dns -n kube-system # 각 노드별 데몬셋 컨트롤러 확인 $ kubectl get pods -l k8s-app=kube-dns -n kube-system | grep -A 2 Args # node local dns 확인 $ kubectl run nettool -it --image= &lt;ACCOUNT &gt;/network-multitool --generator=run-pod/v1 --rm=true bash # 서비스 접근 테스트 . 주소구성: &lt;리소스(서비스) 이름&gt;... . 7.3. Service 종류 . 위의 내용은 내부 접근이고, 외부 접근 제공하는 서비스 구성 필요 . ClusterIP: 클러스터 내부용 진입점 제공 . NodePort: 쿠버네티스 각 노드(호스트)의 포트를 외부 접근용으로 할당 . LoadBalancer: NodePort의 확장판으로, 외부 로드밸러서로 접근하면 서비스를 통해 파드로 Redirection . ExternalName: 외부에서 접근하기 위한 서비스 유형이 아닌, CNAME 매핑을 통해 특정 FQDN과 통신을 위한 기능 . 외부 접근용 레플리카셋 생성 및 확인 | $ kubectl create -f testapp-rs.yml $ kubectl get replicasets.apps $ kubectl get pods | NodePort 서비스 생성 cat testapp-svc-ext-nodeport.yml | $ kubectl create -f testapp-svc-ext-nodeport.yml # 해당 노드에서 사용할 포트 31111로 지정 $ kubectl get endpoints testapp-svc-ext-np # 서비스의 엔드포인트 확인 (Pod의 8080 포트로 Redirection 됨) $ kubectl get nodes -o wide # 각 노드의 IP 확인 | LoadBalancer 서비스 생성 cat testapp-svc-ext-loadbalancer.yml | $ kubectl create -f testapp-svc-ext-loadbalancer.yml # 해당 노드에서 사용할 포트는 정의하지 않음 $ kubectl get services # LoadBalancer 서비스 확인 | ExternalName 서비스 생성 cat testapp-svc-ext-externalname.yml | $ kubectl run nettool -it --image= &lt;ACCOUNT &gt;/network-multitool --generator=run-pod/v1 --rm=true bash # 서비스 접근 테스트 $ nllookup testapp-svc-extname-gl | $ kubectl create -f testapp-svc-ext-externalname.yml # FQDN은 google이며 이에 대한 CNAME은 testapp-svc-extname-gl $ kubectl get services # ExternalName 서비스 확인 | .",
            "url": "https://nueees.github.io/techblog/kubernetes/2021/09/02/kubernetes.html",
            "relUrl": "/kubernetes/2021/09/02/kubernetes.html",
            "date": " • Sep 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Docker",
            "content": "출처_Building Cloud private native 전문가 양성과정 교재 . 1.Docker Container . Container &amp; Virtualization . 하드웨어 성능이 올라가면서, 유휴자원을 활용하는 가상화(자원을 나눠서 사용) 기술이 대두됨. . 가상화 종류 . Hypervisor Virtualization(Native/Bare Metal): 하드웨어 위에 OS 대신 하이퍼바이저를 설치하고 가상(개별OS)머신 생성 (Hyper-V,XenServer) | Host Virtualization: 하드웨어 위에 OS 설치하고, 하이퍼바이저 설치 후, 가상(개별OS)머신 생성 (VMware, VirtualBox) | Container Virtualization: 하드웨어 위에 OS설치하고, 컨테이너 runtime 관리 S/W 설치 후 가상(App)머신 생성 (Docker) 개별운영체제X, 호스트 OS의 일부를 공유하는 방식 사용, overhead 적음 | 그밖에 Application/Network/Storage Virtualization 있음. | . Docker Architecture . Docker’s strengths . don’t need a guest OS | Transplant : easily ported across different platforms | support On-Premise, Cloud, and DevOps open source | . Docker’s main function . Image 생성/관리 image는 컨테이너 구동을 위한 데이터 생성시 코드로 기술 가능(Infrastructure as Code, IaC) | Image 공유 Registry는 Image의 저장소 (Ubuntu, Debian, CentOS, Fedora의 기본 이미지, 다양한 소프트웨어 탑재 된 이미지 제공) 다운로드, 업로드, 버전 관리 가능 | Container 운영 이미지 상태의 파일 -&gt; 프로세스로 띄움 시작, 중지, 삭제 작업 | . 컨테이너 간 간섭을 방지하기 위해 Isolation 기술 (리눅스 namespace와 cgroup 기능) 사용 . technique for Docker : 리눅스 커널 . 독립된 환경 네임스페이스(namespace) 네임스페이스 별로 독립된 PID, Network, UID, MOUNT, UTS, IPC 사용 | 제어그룹 (cgroup) Process 또는 Thread를 그룹화 하여 관리 - CPU나 Memory를 그룹별로 제한 가능 | 가상 Bridge와 가상 NIC(Network Interface Card) 컨테이너 별 각각의 가상 NIC가 할당(ContainerA - eth0, ContainerB - eth0) 가상 NIC는 docker0(172.17.0.0/16)라는 가상 bridge로 연결되어 컨테이너끼리 또는 호스트(물리NIC)를 통해 외부 네트워크로 연결 (Host - eth0) | 계층 파일 시스템 기본 컨테이너 이미지에 추가 작업 시 COW(Copy on Write)* 방식으로 생성 COW 방식은 부모 프로세스가 자식 프로세스를 생성할 때 전부 복제하지 않고, 쓰기가 발생했을 때 변경된 부분만 복제하는 방법과 유사 도커 이미지관리에 사용되는 FS or Library : Btrfs, AUFS, Device Mapper, OverlayFS | . . 2.Managing Docker Container . Installing Docker Engine . 도커 패키지 리포지토리 연결 및 설치 . # yum install -y yum-utils # 필요 util 설치 # yum-config-manager --add-rep https://download.docker.com/linux/centos/docker-ce.repo # 도커 구성 매니저 설치 # yum repolist # yum -y install docker-ce docker-ce-cli containerd.io # 도커 설치 # yum list docker-ce # systemctl start docker.service # 서비스 시작 # systemctl enable docker.service # systemctl is-active docker.service # docker --help . Docker container image . 도커 이미지 관리 . # docker search --help # docker search centos -s 1000 # repository에서 centos 검색 # docker pull --help # docker pull centos # repository에서 centos 가져오기 # docker images # docker tag --help # docker tag centos:latest centos:ver7 # centos 태그 latest-&gt;ver7로 변경 # docker rmi --help # docker rmi centos # centos image 제거 # docker image prune # 이름없는 dangling 이미지 제거 # docker login # docker tag centos:latest nueees/repo-web:centos # docker push --help # docker push nueees/repo-web:centos . . Docker container management . 도커 컨데이너 관리 . # docker create --help # docker create -it --name c1 centos # centos 이미지로 c1라는 컨테이너 생성 # docker ps --help # docker ps -a # docker inspect --help # docker inspect c1 # # docker start --help # docker start c1 # docker stop --help # docker stop c1 # docker run --help # docker run -it --name c2 centos # docker run -d --name web1 httpd # docker rm -f web1 . run (create+start) exit -&gt; 컨테이너 종료 Ctrl+P+Q -&gt; 컨테이너 종료하지 않고 빠져나옴 . # docker attach --help # docker run -itd --name c1 centos # docker attach c1 # background에 실행중인 c1 컨데이너 접근 # docker exec --help # docker run -d --name web1 httpd # docker exec -it web1 bash # web1 컨테이너 접근해서 bash 실행 # docker top --help # docker top web1 # web1 컨테이너에서 실행중인 process 확인 # docker top web1 aux # docker rename --help # docker rename c1 newc1 # docker pause --help # docker pause web1 # docker unpaues web1 # docker cp --help # docker run -itd --name c1 centos # docker cp dockercp.txt c1:/ # host file -&gt; container c1의 /경로로 copy # docker exec -it c1 cat /dockercp.txt # docker diff --help # docker attach c1 ## rm -f anaconda-post.log # 기존 컨테이너 내 log 삭제 후 # docker diff c1 # docker commit --help # docker commit c1 centos:hello # 기존 c1 container로 new image 생성 # docker images # docker save --help # 여러개 이미지를 archive file로 저장 시 # docker save -o imgarc.tar centos:hello httpd:latest # centos:hello, httpd 두개 image를 archive file로 저장 # docker load --help # docker load -i imgarc.tar # archive file에 저장된 이미지 불러오기 # docker images # 불러온 이미지 확인 # docker export --help # 컨테이너 파일시스템을 archive file로 추출 # docker attach c1 ## echo &quot;This is export test&quot; &gt; export.txt # docker export -o testexport.tar c1 # tar tf testexport.tar | grep export.txt # docker import --help # export로 컨테이너로 추출한 archive file로 이미지로 생성 # docker import testexport.tar export:test . 컨테이너 네트워크 구성 . # ip a s # brctl show # docker run -itd --name c1 centos # brctl show # docker attach c1 ## yum -y install net-tools ## ifconfig ## rount -n # docker0는 172.17.0.0/16 네트워크 사용하고 외부 통신 가능 ## ping -c2 google.co.kr # iptables -L -t nat # 게이트웨이 172.17.0.1이며 마스커레이딩 설정 됨 # docker network --help # docker network ls # 도커 네트워크는 bridge, host, none 세가지 # docker inspect bridge # docker network create --help # docker network create d-net # bridge 유형으로 도커 네트워크 생성 # docker inspect d-net # 네트워크 범위(subnet) 자동으로 172.18.0.0/16으로 설정 # docker network create --subnet 192.168.0.0/24 --gateway 192.168.0.254 custom-net # docker run -it --net custom-net --name a1 alpine # 사용자 정의 custom-net 네트워크 사용하여 컨테이너 생성 # docker run -it --net host --name a2 alpine # host 유형으로 생성시 host 네트워크를 공유 ## ifconfig # docker run -it --net none --name a3 none alpine # none 유형은 네트워크 할당 안함 ## ifconfig . 컨테이너 통신 . # docker run -itd --name a1 alpine # docker run -itd --name a2 --link a1 alpine # docker attach a2 ## ping a1 # ping 감 # docker attach a1 ## ping a2 # ping 가지 않음 # docker exec a1 cat /etc/hosts # docker exec a2 cat /etc/hosts # a1이 등록된 걸 확인 # docker run -itd --name a3 --link a1:alpine1 alpine # 별칭으로 링크 등록 # docker exec a1 ping alpine1 # ping 감 # docker exec a3 cat /etc/hosts # a1과 alpine1으로 등록된 걸 확인 # docker run -d --name web1 httpd # curl localhost # container에서는 80포트가 열려있으나 실제 host의 주소로 접근 불가 # docker run -d -p 80:80 --name web2 httpd # host의 port 80으로 요청오면 container 80으로 전달 (호스트:컨테이너) . 호스트의 특정 포트가 컨테이너 포트와 연결되어 있으면 해당 포트는 다른 컨테이너와 연결될 수 없음. . 컨테이너 볼륨 . # mkdir volume # echo hello &gt; volume/hello.txt # docker run -it -v /boot/volume:/mnt --name v1 centos # host 디렉토리를 container와 공유 (호스트:컨테이너) ## ls /mnt ## cat /mnt/hello.txt ## df -h # /mnt 디렉토리가 host의 /dev/sda1에 연결되어 있음 # mkdir vol1 vol2 vol3 # docker run -it -v /boot/vol1:/vol1 -v /boot/vol1:/vol1 -v /boot/vol1:/vol1 --name v2 centos # 동시에 다수 볼륨도 연결가능 # docker run -it -v /boot/vol1:/vol1 --name c2 centos # 하나 볼륨을 동시에 Read-Write시 crash 발생 # docker run -it -v /boot/vol1:/vol1:ro --name c3 centos # 볼륨 공유시엔 하나 외 나머지는 Read-Only로 ## touch /vol1/xxx # docker run -it -v vol-1:/vol1 --name c1 centos # host에서 디렉토리 생성없이 docker volume 생성 후 연결 (도커볼륨:컨테이너) ## touch /vol1/x # docker run -it -v vol-1:/vol1 --name c2 centos ## ls /vol1 # 파일 x 확인 가능 # docker volume ls # docker 볼륨 확인 # docker inspect vol-1 # 실제로는 host 디렉토리 사용 # docker volume create --help # docker volume create vol2 # 수동으로도 생성 가능 # docker run -it --name m1 mysql # docker inspect m1 # 도커 볼륨을 자동으로 생성하는 이미지가 있음 (&quot;Volumes&quot;:{...) # docker volume ls # 생성된 볼륨 확인 . 컨테이너 환경변수 설정 . # docker run -it -e a=100 --name c1 centos ## echo $a # docker run -it --name db1 mysql # error 출력되며 env 설정 필요 # docker ps -a # docker inspect db1 # (&quot;Entrypoint&quot;:[...) # docker run -d -e MYSQL_ROOT_PASSWORD=1234 --name db2 mysql # 환경변수 주고 실행 . 보통 inpsect 내 “Cmd”:[“mysql”]가 실행되나, Entrypoint가 지정되어 있으면 “Entrypoint”:[“docker-entrypoint.sh”]이 우선 실행됨 . 컨데이너 로그 확인 . # docker logs --help # docker logs db1 # 위 환경변수 오류 error log 확인 가능 # docker logs --tail 2 db2 # docker logs --since &quot;2021-10-12T01:00&quot; db2 . 컨테이너 자원 할당 . # docker run -d --name web1 httpd # options 없이 자원 할당 # docker stats web1 # 최대치 할당 # docker run -d --name web2 --memory=&quot;200m&quot; httpd # docker stats --no-stream web1 # 메모리 limit 200MB # # docker update --help # docker update --memory=500m web2 # up가능하나 down은 불가 # docker run -itd -c 10 --name a1 alpine # 가중치 1로 할당 # docker run -itd -c 20 --name a2 alpine # 가중치 2로 할당 # docker attach a1 ## dd if=/dev/zero of=/dev/null &amp; # docker attach a2 ## dd if=/dev/zero of=/dev/null &amp; # docker stats # cpu 사용률 a2가 a1의 두배 확인 # docker run -itd --name a3 -cpus=&quot;0,2&quot; alpine ## dd if=/dev/zero of=/dev/null &amp; # docker stats # a3의 cpu 사용률 20% 확인 # docker run -itd --name c1 --devie-write-bps /dev/sda:1mb centos # sda 디스크에 write작업시 속도 limit을 1MB/s # docker attach c1 ## dd if=/dev/zero of=/perftest bs=1M count=10 oflag=direct # docker run -itd --name c2 centos # 제한없이 # docker attach c2 ## dd if=/dev/zero of=/perftest bs=1M count=10 oflag=direct . CPU, Memory, Block 제한은 가능하나 NET I/O는 제한 불가 . 워드프레스 컨테이너 실행 example . # docker network create wp-web # 네트워크 생성 # docker run -d --name wp-db --net wp-net -v wp-db-vol:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=1234 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=1234 mysql:5.7 # docker run -d --name wp-web --net wp-net -v wp-web-vol:/var/www/html --link wp-db:mysql -e WORDPRESS_DB_HOST=wp-db:3306 -e WORDPRESS_DB_PASSWORD=1234 -p 80:80 wordpress . http://localhost:8080 또는 http://localhost:8080/wp-admin/install.php로 확인 . . 3.Building Docker Image . Dockerfile . # docker run -it --name c1 centos # yum -y install httpd &gt; /dev/null ## systemctl start httpd ## systemctl enable httpd # httpd 이미지는 CentoOS와 호환되지 않아 설치 시 오류 발생 # docker inspect centos # container boot시 명령어가 지정되어 있음 ( &quot;Cmd&quot;: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;#(nop)&quot;, &quot;CMD [ &quot;/bin/bash &quot;]&quot;...) # dodker inspect httpd # ( &quot;Cmd&quot;: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;#(nop)&quot;, &quot;CMD [ &quot;httpd-foreground &quot;]&quot;...) . 이 경우 직접 컨테이너를 구동하고 수정하는 방식으로는 특정 어플리케이션 지정할 수 없음 Dockerfile을 사용해야 함 . Dockerfile 생성 . # mkdir dftest1 # cd dftest1 # vi Dockerfile FROM httpd:latest MAINTAINER nueees RUN yum -y install httpd COPY index.html /var/www/html CMD /usr/sbin/httpd -D FORGROUND # docker build -t test:1.0 . # 현재 디렉토리(.)로 빌드 # docker images # test:1.0으로 추가된 것 확인 . base image (베이스 이미지) | command (실행 명령) | env (환경 변수) | run (실행 데몬) | 명령어 설명 . FROM | 베이스 이미지 지정 | . MAINTAINER | 작성자 지정 | . RUN | 명령어 실행 | . CMD | 데몬 실행 | . LABEL | 라벨 지정 | . EXPOSE | 포트 내보내기 | . ENV | 환경변수 설정 | . ADD | 파일 추가 | . COPY | 파일 복사 | . VOLUME | 볼륨 마운트 | . ENTRYPOINT | 데몬실행 | . USER | 사용자 설정 | . WORKDIR | 작업 디렉토리 설정 | . ONBUILD | build 후 실행 명령 | . . Docker image build . # docker build -help # cat /root/file/Dockerfile # docker build -t base:1.0 /root/file/ # docker images . docker layer 구조 . # docker build -t base:1.0 /root/file # 실행시 레이어 구조가 하나씩 나옴 . setp 1/5 : FROM centos | step 2/5 : MAINTAINER nueees | step 3/5 RUN yum -y install httpd | step 4/5 COPY index.html /var/www/html | step 5/5 CMD /usr/sbin/httpd -D FORGROUND | . 4.Docker Image Registry . Docker image registry? . Registry - Docker Image Registry . 도커 허브에 공개되어 있는 registry 공식 이미지 . # docker search registry # docker pull registry:2.0 # docker images # docker run -d -p 5000:5000 --name regTest registry:2.0 # docker tag httpd localhost:5000/regTest # docker push localhost:5000/regTest # upload # docker rmi localhost:5000/regTest # docker pull localhost:5000/regTest # download # docker images . Harbor - Docker Image Registry . one of private registries web 기반, 다양한 기능 제공 . docker-compose 설치 . # curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose # chmod +x /usr/local/bin/docker-compose # docker-compose --version . Harbor 설치 파일 download . # wget &quot;https://github.com/goharbor/harbor/releases/download/v2.3.3/harbor-offline-installer-v2.3.3.tgz&quot; # tar xzvf harbor-offline-installer-version.tgz . HTTPS 구성 후 . Configuration File(YML File) 수정 . # vi harbor.yml 5 hostname: docker.nueees.co.kr 13 https: 15 port: 443 17 cerificate: 18 private_key: . Harbor 설치 . # systemctl restart docker # ./install.sh . Harbor 사용 . # docker login -u admin -p Harbor12345 192.168.56.101 # docker tag centos:latest 192.168.56.101/library/docker:centos # docker push 192.168.56.101/library/docker:centos # docker rmi 192.168.56.101/library/docker:centos # docker pull 192.168.56.101/library/docker:centos . 웹에서 Harbor IP 접속하여 대시보드 확인 . .",
            "url": "https://nueees.github.io/techblog/kubernetes/2021/09/01/docker.html",
            "relUrl": "/kubernetes/2021/09/01/docker.html",
            "date": " • Sep 1, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Apache Spark",
            "content": "📎 Tacademy . . 3.1. Spark . Unified Computing Engine and a Set of Libraries for Parallel Data Processing on Computer Clusters 용도: Machine learning, real time analytics, graph processing, etc. . Structured Streaming, Advanced Analytics, Libraries &amp; Ecosystem | High level APIs (Java, Python, …) | Low level APIs (RDDs, Distributed Variables) | Structured APIs (Datasets, DataFrames, SQL) | . 기존 MapReduce에서 disk I/O 부하가 심했던 부분을 memory에서 빠르게 작업 . 1) Low level APIs 직접 map와 reduce를 function으로 구현하여야 함 . 2) High level APIs (functional language) scala와 python으로 간단하게 처리 가능 (추상화되어 있음) . . 3.2. Spark Architecture . . executor 개수와 resource 할당 . Cluster Manager . worker와 executors 사이에 자원을 중계해주는 역할 resource를 효율적으로 분배 . ex) Spark StandAlone(cluster X), (Hadoop)Yarn, Mesos, Kubernetes . Driver Process . SparkContext를 생성하고 RDD를 만들고 operation을 실행하는 프로세스 . Spark-submit을 하면, Spark Driver가 Cluster Manager로부터 Executor 실행을 위한 리소스를 요청 Spark Context는 작업 내용을 task 단위로 분할하여 Executor로 보냄 . Executors . 주어진 작업의 개별 task들을 실행하는 작업 실행하고 결과 return . 1) 애플리케이션을 구성하는 작업들을 실행하여 driver에 그 결과를 return 2) 각 executor 안에 존재하는 block manager라는 서비스를 통해 사용자 프로그램에서 캐시하는 RDD를 저장하기 위한 메모리 저장소를 제공 . Python,R Process &lt;-&gt; JVM(Spark session) -&gt; Executors . End to End: csv file read(narrow) -&gt; DataFrame sort(wide) -&gt; Array . Operations . 1) Transformations: map, filter, groupBy, join lazy operation으로 즉시 실행하는 단계가 아님 . 2) Actions: count, collect, save 실제로 실행후 driver로 결과 return . . 3.3. Processing 방식 . Batch Processing . big &amp; complex | processing massive data | higher latencies | . Real-time Processing (Stream) . relatively simple and generally independent | one at a time processing | sub-second latency | . input data stream -&gt; spark streaming -&gt; batch input data -&gt; spark engine -&gt; processed batch data . spark streaming의 경우 개발하기는 어렵지 않으나, 운영(debugging)이 어려움 . . 3.4. Variables . Accumulator . aggregate multiple values as it progresses . Broadcast . large read-only variable shared across tasks, operations large lookup tables . 디버깅이나 검증용으로 쓰고 스파크 스트리밍에서는 accumulator, broadcast 지양 (GC문제..) . Fault Tolerance 비즈니스 요건에 따라, 실패시 누락이 되면 되는지 안되는지에 따라 고려 (Kafka) . . 3.5. Interface . RDDs (Resilient Distributed Datasets) . low level interface, Data Containers 각기 다른 프로세스 요소들을 추상화한 같은 형태 . 1) Hadoop에서 읽은 RDD path = hdfs://… . 2) Filtered된 RDD func = contains(…) . 3) Mapped된 RDD func = split(…) . Fault되면 Loss된 데이터 이전부터 다시 계산 . data = sc.textFile(...).split(&quot; t&quot;) data.map(lambda x: (x[0], [int(x[1]), 1])) .reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]]) .map(lambda x: [x[0], x[1][0] / x[1][1]]) .collect() . DataFrames . 코드가 직관적이고 schema를 가지는 interface . sqlCtx.table(&quot;people&quot;).groupBy(&quot;name&quot;).agg(&quot;name&quot;, avg(&quot;age&quot;)).collect() . . 3.6. Scheduling . . DAG (Directed Acyclic Graphs) Scheduler .",
            "url": "https://nueees.github.io/techblog/spark/etl/2021/02/03/apache-spark.html",
            "relUrl": "/spark/etl/2021/02/03/apache-spark.html",
            "date": " • Feb 3, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Apache Hadoop",
            "content": "📎 Tacademy . . 2.1. Hadoop . 대용량 데이터를 처리하기 위한 컴퓨터 클러스터에서 동작하는 분산 프로그램 (단일 서버 &lt; 클러스터로 묶은 서버) | 기존 RDBMS는 확장을 하려면 서버를 추가 구매해야하나 (scale-up) | Hadoop은 node(컴퓨터)를 추가함으로써 선형적으로 확장 가능 (scale-out) | . ### 저장 storage: HDFS(Hadoop Distributed File System) ### 연산: MapReduce . 파일을 block(64M,128M) 단위로 분할 | block들은 서로 다른 machine에 분산 저장(Mapreduce 처리) | block들은 여러 machine(기본 3 replication)에 복제되어 data node에 저장 | Master node인 name node는 meta data(저장된 위치, block의 file 정보 등) 관리 | . . 2.2. HDFS Access . 1) shell 2) java api 3) ecosystem - Flume(network source로 데이터 수집) - Sqoop(HDFS와 RDBMS 사이 데이터 전송) - Hue(Web 기반으로 browse, upload, download, file view) . file write &amp; read . name node의 mete data를 통해 접근 . 따라서 name node 데몬 중단되면 cluster 접근 불가 HA로 2개의 name node를 구성하기도 함 (Active/Standby) 1개 name node 구성시 helper node(Secondary name node)가 추가됨 . files blocks . /logs/031512.log | B1,B2,B3 | . /logs/041213.log | B4,B5 | . nodes   . node A | B1, B3, B4 | . node B | B1, B2, B3, B4 | . node C | B3, B5 | . node D | B1, B5, B2 | . node E | B2, B5, B4 | . node 중 idle이 높은 node부터 접근 . . 2.3. 구성요소 . 1) client node node를 통해 정보를 받고 이후 data node와 직접 통신 . 2) master node(job tracker, name node) slave node에 대한 정보와 실행할 task 관리 . 3) slave node(data node, task node) client 요청 시 data 전달, task 수행 . . Data Analytics 관점 . Job tracker: task tracker가 수행할 task 스케줄링, 모니터링 | Task tracker: task를 수행하고 job tracker에게 상황 | . Data Storage 관점 . Name node: Meta data 유지, client로 부터 데이터 요청오면 위치 전달 | Data node: 데이터를 HDFS block 단위로 구성, HA를 위한 replication 3 유지, heartbeat를 통한 파일 위치 전달 | . . 2.4. MapReduce . File . Deer, Bear, River | . Car, Car, River | . Deer, Car, Bear | . Mapping: 파일은 한 줄씩 읽어서 데이터 변경 . 데이터를 key와 value 형태로 pairing하고 list화 .     . map1 | Deer, 1 | .   | Bear, 1 | .   | River, 1 | . map2 | Car, 1 | .   | Car, 1 | .   | River, 1 | . map3 | Deer, 1 | .   | Car, 1 | .   | Bear, 1 | . shuffling . grouping, sorting .   . Bear, 1 | . Bear, 1 | . Car, 1 | . Car, 1 | . Car, 1 | . Deer, 1 | . Deer, 1 | . River, 1 | . River, 1 | . Reducing: map의 결과 데이터를 집계 . aggregating, 후 extract .     . red1 | Bear, 2 | . red2 | Car, 3 | . red3 | Deer, 2 | . red4 | River, 2 | .",
            "url": "https://nueees.github.io/techblog/hadoop/filesystem/2021/02/02/apache-hadoop.html",
            "relUrl": "/hadoop/filesystem/2021/02/02/apache-hadoop.html",
            "date": " • Feb 2, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Apache Kafka",
            "content": "📎 Tacademy . . 1.1. Kafka . a distributed, partitioned, and replicated commit log service . 소스 애플리케이션(mysql, oracle, nosql,…/app/was/…)과 타겟 애플리케이션(hadoop, search engine, monitoring, email…)개수가 많아지고,프로토콜 포맷 파편화가 심해져 유지보수가 어려워짐 . 이러한 복잡함을 해결하기 위해 중간에서 소스 애플리케이션과 타겟 애플리케이션 연결을 느슨하게 함 (scale out) . 프로듀서가 큐에 적재하고 컨슈머가 가져가는 구조 . Kafka Client: kafka와 데이터를 주고 받기 위해 사용하는 (producer, consumer, admin, stream 등) API | . . 1.2. Brocker &amp; Cluster . brocker: Kafka application 서버 단위 | cluster: 3대 이상의 brocker로 구성 . | zookeeper: metadata 저장하여 brockers 관리 | controller: n대 brocker 중 마스터 brocker (allocate partition, health check) | . . 1.3. Topic . 메시지(데이터) 분류 단위 n개의 파티션 할당 가능 각 파티션 마다 고유 offset 가짐 메시지 처리순서는 파티션 별로 관리 . partition: 레코드를 담고 있고 replication 가능 | ISR(In-Sync Replica): replication(leader+follower)의 sync로 된 묶음 | segment: 실제로 메시지가 저장되는 파일 시스템 단위 | . . 1.4. Producer . 메시지(레코드)를 brocker로 송신 API . Role . Topic에 해당하는 메시지를 해당 파티션에 offset과 함께 기록 . Publishing: 특정 topic으로 데이터 송신 | 처리 실패 시 재시도 | key 사용시, 해당 partition으로 데이터 매핑 | . . 1.5. Consumer . 메시지(레코드) brocker에서 수신 API 다수의 consumer를 group화 할 수 있음 . Role . Polling: Topic의 partition으로 부터 메시지(레코드)를 가져감 | Commit: Partition offset 위치를 기록 | Parallel Processing: Consumer 여러개일 경우, Consumer group을 통해 병렬 처리 | . Offset number: . 파티션 내 갖게되는 고유 키 토픽 별, 파티션 별로 고유하게 지정됨 컨슈머가 데이터를 어디까지 읽었는지 확인하는 용도 데이터를 읽은 곳까지 commit . 컨슈머 이슈가 발생해도 __consumer_offset 토픽에 저장된 지점 확인(High availablility) . . 병렬처리를 위해서는 consumer 개수는 partition 개수보다 적거나 같아야 함 . . 1.6. Kafka Streams . 데이터 변환(transformation) API . stateful, stateless와 같은 상태 기반 stream 처리 가능 | stream api와 DSL(Domain Specific Language) 동시 지원 | Exactly-once 처리, HA 특징 | Kafka security(acl, sasl 등) 지원 | stream 처리를 위한 별도 cluster(yarn 등) 불필요 | . . 1.7. Kafka Connect . Kafka client가 아닌 connect를 통해 데이터를 import/export 가능 code 없이 configuration으로 데이터 migration 시 사용 .",
            "url": "https://nueees.github.io/techblog/kafka/message_broker/queue/2021/02/01/apache-kafka.html",
            "relUrl": "/kafka/message_broker/queue/2021/02/01/apache-kafka.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Python Abstract",
            "content": "Python Coding dojang . . 6.1. Abstract Class . from abc import * class MyBookBase(metaclass=ABCMeta): @abstractmethod def display(self): pass @abstractmethod def get_amount(self): print(&quot;How many books sold?&quot;) class MyBook(MyBookBase): cnt = 0 def __init__(self, title, author, price): MyBook.cnt += 1 self.title = title self.author = author self.price = price def display(self): print(&quot;Title: {}&quot;.format(self.title)) print(&quot;Author: {}&quot;.format(self.author)) print(&quot;Price: {}&quot;.format(self.price)) def get_amount(self): super().get_amount() print(self.cnt) new_novel1=MyBook(&#39;Human acts&#39;,&#39;Hangang&#39;,12000) new_novel1.display() &gt; Title: Human acts Author: Hangang Price: 12000 new_novel1.get_amount() &gt; 1 new_novel2=MyBook(&#39;Vegitarian&#39;,&#39;Hangang&#39;,14000) new_novel2.display() &gt; Title: Vegitarian Author: Hangang Price: 14000 new_novel2.get_amount() &gt; 2 .",
            "url": "https://nueees.github.io/techblog/python/2021/01/06/python-abstract.html",
            "relUrl": "/python/2021/01/06/python-abstract.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Python Concurrency",
            "content": "Python Intermediate Programming . . 5.1. Generator Pattern . 작은 메모리 조각 사용 단위 실행 가능한 코루틴(Coroutine) 구현과 연동 . iter function . 파이썬 반복 가능한 타입: for, collections, text file, List, Dict, Set, Tuple, unpacking, *args . next . class WordSplitGenerator: def __init__(self, text): self._text = text.split(&#39; &#39;) def __iter__(self): # print(&#39;Called __iter__&#39;) for word in self._text: yield word # 제네레이터 return def __repr__(self): return &#39;WordSplit(%s)&#39; % (self._text) wg = WordSplitGenerator(&#39;One shot No break&#39;) print(wg) &gt; WordSplit([&#39;One&#39;, &#39;shot&#39;, &#39;No&#39;, &#39;break&#39;]) wt = iter(wg) print(wt) &gt; &lt;generator object WordSplitGenerator.__iter__ at 0x000002142675C200&gt; print(next(wt)) # 내부적으로 iter 호출 &gt; Called __iter__ &gt; &#39;One&#39; print(next(wt)) # iter에서 yield 시점부터 재 호출 &gt; &#39;shot&#39; print(next(wt)) &gt; &#39;No&#39; print(next(wt)) &gt; &#39;break&#39; print(next(wt)) &gt;Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; StopIteration . genetator 생성 . def generator_ex1(): print(&#39;Start&#39;) yield &#39;A Point.&#39; print(&#39;continue&#39;) yield &#39;B Point.&#39; print(&#39;End&#39;) . temp2 = [x * 3 for x in generator_ex1()] print(temp2) &gt; [&#39;A Point.A Point.A Point.&#39;, &#39;B Point.B Point.B Point.&#39;] for i in temp2: print(i) &gt; A Point.A Point.A Point. &gt; B Point.B Point.B Point. . temp3 = (x * 3 for x in generator_ex1()) print(temp3) &gt; &lt;generator object &lt;genexpr&gt; at 0x000002142685DA50&gt; for i in temp3: print(i) &gt; Start &gt; A Point.A Point.A Point. &gt; continue &gt; B Point.B Point.B Point. &gt; End . . 5.2. Generator Functions . import itertools . takewhile . gen2 = itertools.takewhile(lambda n : n &lt; 3, [1,2,3,4,5,5,4,3,2,1]) for v in gen2: print(v) &gt; 1 &gt; 2 . filter랑 비교 . gen1 = filter(lambda n : n &lt; 3, [1,2,3,4,5,5,4,3,2,1]) for v in gen1: print(v) &gt; 1 &gt; 2 &gt; 2 &gt; 1 . filterfalse . gen3 = itertools.filterfalse(lambda n : n &lt; 3, [1,2,3,4,5,5,4,3,2,1]) for v in gen3: print(v) &gt; 3 &gt; 4 &gt; 5 &gt; 5 &gt; 4 &gt; 3 . accumulate . gen4 = itertools.accumulate([x for x in range(1, 6)]) for v in gen4: print(v) &gt; 1 &gt; 3 &gt; 6 &gt; 10 &gt; 15 . chain . gen5 = itertools.chain(&#39;ABCDE&#39;, range(1,11,2)) print(list(gen5)) &gt; [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, 1, 3, 5, 7, 9] gen6 = itertools.chain(enumerate(&#39;ABCDE&#39;)) print(list(gen6)) &gt; [(0, &#39;A&#39;), (1, &#39;B&#39;), (2, &#39;C&#39;), (3, &#39;D&#39;), (4, &#39;E&#39;)] . product . gen7 = itertools.product(&#39;ABCDE&#39;) print(list(gen7)) &gt; [(&#39;A&#39;,), (&#39;B&#39;,), (&#39;C&#39;,), (&#39;D&#39;,), (&#39;E&#39;,)] gen8 = itertools.product(&#39;ABCDE&#39;, repeat=2) # permutation 5P2 경우의 수 전부 출력 print(list(gen8)) &gt; [(&#39;A&#39;, &#39;A&#39;), (&#39;A&#39;, &#39;B&#39;), (&#39;A&#39;, &#39;C&#39;), (&#39;A&#39;, &#39;D&#39;), (&#39;A&#39;, &#39;E&#39;), (&#39;B&#39;, &#39;A&#39;), (&#39;B&#39;, &#39;B&#39;), (&#39;B&#39;, &#39;C&#39;), (&#39;B&#39;, &#39;D&#39;), (&#39;B&#39;, &#39;E&#39;), (&#39;C&#39;, &#39;A&#39;), (&#39;C&#39;, &#39;B&#39;), (&#39;C&#39;, &#39;C&#39;), (&#39;C&#39;, &#39;D&#39;), (&#39;C&#39;, &#39;E&#39;), (&#39;D&#39;, &#39;A&#39;), (&#39;D&#39;, &#39;B&#39;), (&#39;D&#39;, &#39;C&#39;), (&#39;D&#39;, &#39;D&#39;), (&#39;D&#39;, &#39;E&#39;), (&#39;E&#39;, &#39;A&#39;), (&#39;E&#39;, &#39;B&#39;), (&#39;E&#39;, &#39;C&#39;), (&#39;E&#39;, &#39;D&#39;), (&#39;E&#39;, &#39;E&#39;)] . groupby . gen9 = itertools.groupby(&#39;AAABBCCCCDDEEE&#39;) print(list(gen9)) &gt; [(&#39;A&#39;, &lt;itertools._grouper object at 0x0000021426987B20&gt;) , (&#39;B&#39;, &lt;itertools._grouper object at 0x0000021426987700&gt;) , (&#39;C&#39;, &lt;itertools._grouper object at 0x0000021426987BB0&gt;) , (&#39;D&#39;, &lt;itertools._grouper object at 0x00000214269879A0&gt;) , (&#39;E&#39;, &lt;itertools._grouper object at 0x0000021426987A90&gt;)] gen9 = itertools.groupby(&#39;AAABBCCCCDDEEE&#39;) # 위에 iter 다 돌아서 다시 생성해서 돌려줌 for chr, group in gen9: print(chr, &#39; : &#39;, list(group)) &gt; A : [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;] &gt; B : [&#39;B&#39;, &#39;B&#39;] &gt; C : [&#39;C&#39;, &#39;C&#39;, &#39;C&#39;, &#39;C&#39;] &gt; D : [&#39;D&#39;, &#39;D&#39;] &gt; E : [&#39;E&#39;, &#39;E&#39;, &#39;E&#39;] . . 5.3. Coroutine . 동시성 프로그래밍을 위해 routine 실행 중 중지 coroutine: multi thread 코드는 thread간 deadlock, context switching, resource 소비 등을 신경써야 하며 bug 발생률이 높아짐 single thread로 동작하도록 만든게 Coroutine . yield . main &lt;-&gt; sub 왼쪽(y)에 있으면 main-&gt;sub로 받는 것 오른쪽(x)에 있으면 sub-&gt;main으로 주는 것 ex) y = yield x . from inspect import getgeneratorstate def coroutine2(x): # sub cnt = 1 print(&#39;&gt;&gt;&gt; coroutine started : {}, {}&#39;.format(x,cnt)) y = yield x # main으로 x(10)전달 # 다음 실행시 main에서 y(20)받음 cnt += 1 print(&#39;&gt;&gt;&gt; coroutine y received : {}, {}&#39;.format(y,cnt)) z = yield x - y # main으로 x-y(10-20) # 다음 실행시 main에서 z(30)받음 cnt += 1 print(&#39;&gt;&gt;&gt; coroutine z received : {}, {}&#39;.format(z,cnt)) # main cr3 = coroutine2(10) print(getgeneratorstate(cr3)) &gt; GEN_CREATED print(next(cr3)) # x 반환하므로 &gt; 10 print(getgeneratorstate(cr3)) &gt; GEN_SUSPENDED print(cr3.send(20)) # x-y 반환하므로 &gt; -10 print(cr3.send(30)) # error &gt; coroutine received : 30, 3 &gt; StopIteration error . . 5.4. Futures . map . import os import time from concurrent import futures WORK_LIST = [100000, 1000000, 10000000, 10000000] # concurrency control def sum_generator(n): return sum(n for n in range(1, n+1)) def main(): # Worker Count worker = min(10, len(WORK_LIST)) # 시작 시간 start_tm = time.time() # 결과 건수 # ProcessPoolExecutor with futures.ThreadPoolExecutor() as excutor: # map -&gt; 작업 순서 유지, 즉시 실행 result = excutor.map(sum_generator, WORK_LIST) # 종료 시간 end_tm = time.time() - start_tm # 출력 포멧 msg = &#39; n Result -&gt; {} Time : {:.2f}s&#39; # 최종 결과 출력 print(msg.format(list(result), end_tm)) if __name__ == &#39;__main__&#39;: main() &gt; Result -&gt; [5000050000, 500000500000, 50000005000000, 50000005000000] Time : 4.53s . wait, as_completed . def sum_generator(n): return sum(n for n in range(1, n+1)) # wait # as_completed def main(): # Worker Count worker = min(10, len(WORK_LIST)) # 시작 시간 start_tm = time.time() # Futures futures_list = [] # 결과 건수 # ProcessPoolExecutor with ThreadPoolExecutor() as excutor: for work in WORK_LIST: # future 반환 future = excutor.submit(sum_generator, work) # 스케쥴링 futures_list.append(future) # 스케쥴링 확인 print(&#39;Scheduled for {} : {}&#39;.format(work, future)) # print() # wait 결과 출력 # result = wait(futures_list, timeout=7) # # 성공 # print(&#39;Completed Tasks : &#39; + str(result.done)) # # 실패 # print(&#39;Pending ones after waiting for 7seconds : &#39; + str(result.not_done)) # # 결과 값 출력 # print([future.result() for future in result.done]) # as_completed 결과 출력 for future in as_completed(futures_list): result = future.result() done = future.done() cancelled = future.cancelled # future 결과 확인 print(&#39;Future Result : {}, Done : {}&#39;.format(result, done)) print(&#39;Future Cancelled : {}&#39;.format(cancelled)) # 종료 시간 end_tm = time.time() - start_tm # 출력 포멧 msg = &#39; n Time : {:.2f}s&#39; # 최종 결과 출력 print(msg.format(end_tm)) if __name__ == &#39;__main__&#39;: main() &gt; Scheduled for 10000 : &lt;Future at 0x20268c4a280 state=finished returned int&gt; Scheduled for 100000 : &lt;Future at 0x20268c4a400 state=pending&gt; Scheduled for 1000000 : &lt;Future at 0x20268d7c1c0 state=running&gt; Scheduled for 10000000 : &lt;Future at 0x20268d7c190 state=pending&gt; Future Result : 5000050000, Done : True Future Cancelled : &lt;bound method Future.cancelled of &lt;Future at 0x20268c4a400 state=finished returned int&gt;&gt; Future Result : 50005000, Done : True Future Cancelled : &lt;bound method Future.cancelled of &lt;Future at 0x20268c4a280 state=finished returned int&gt;&gt; Future Result : 500000500000, Done : True Future Cancelled : &lt;bound method Future.cancelled of &lt;Future at 0x20268d7c1c0 state=finished returned int&gt;&gt; Future Result : 50000005000000, Done : True Future Cancelled : &lt;bound method Future.cancelled of &lt;Future at 0x20268d7c190 state=finished returned int&gt;&gt; Time : 2.10s . . 5.5. asyncio . async . import asyncio import timeit from urllib.request import urlopen from concurrent.futures import ThreadPoolExecutor import threading # 실행 시작 시간 start = timeit.default_timer() # 서비스 방향이 비슷한 사이트로 실습 권장(예 : 게시판성 커뮤니티) urls = [&#39;http://daum.net&#39;, &#39;https://naver.com&#39;, &#39;http://mlbpark.donga.com/&#39;, &#39;https://tistory.com&#39;, &#39;https://wemakeprice.com/&#39;] async def fetch(url, executor): # 쓰레드명 출력 print(&#39;Thread Name :&#39;, threading.current_thread().getName(), &#39;Start&#39;, url) # 실행 res = await loop.run_in_executor(executor, urlopen, url) print(&#39;Thread Name :&#39;, threading.current_thread().getName(), &#39;Done&#39;, url) # 결과 반환 return res.read()[0:5] async def main(): # 쓰레드 풀 생성 executor = ThreadPoolExecutor(max_workers=10) # future 객체 모아서 gather에서 실행 futures = [ asyncio.ensure_future(fetch(url, executor)) for url in urls ] # 결과 취합 rst = await asyncio.gather(*futures) print() print(&#39;Result : &#39;, rst) if __name__ == &#39;__main__&#39;: # 루프 초기화 loop = asyncio.get_event_loop() # 작업 완료 까지 대기 loop.run_until_complete(main()) # 수행 시간 계산 duration = timeit.default_timer() - start # 총 실행 시간 print(&#39;Total Running Time : &#39;, duration) &gt; Thread Name : MainThread Start http://daum.net Thread Name : MainThread Start https://naver.com Thread Name : MainThread Start http://mlbpark.donga.com/ Thread Name : MainThread Start https://tistory.com Thread Name : MainThread Start https://wemakeprice.com/ Thread Name : MainThread Done http://mlbpark.donga.com/ Thread Name : MainThread Done https://naver.com Thread Name : MainThread Done http://daum.net Thread Name : MainThread Done https://tistory.com Thread Name : MainThread Done https://wemakeprice.com/ Result : [b&#39;&lt;!DOC&#39;, b&#39; n&lt;!do&#39;, b&#39;&lt;!DOC&#39;, b&#39; n t&lt;!d&#39;, b&#39; x1f x8b x08 x00 x00&#39;] Total Running Time : 6.3161305 . await . import asyncio import timeit from urllib.request import urlopen from concurrent.futures import ThreadPoolExecutor import threading from bs4 import BeautifulSoup import sys import io sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding = &#39;utf-8&#39;) sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding = &#39;utf-8&#39;) # 실행 시작 시간 start = timeit.default_timer() # 서비스 방향이 비슷한 사이트로 실습 권장(예 : 게시판성 커뮤니티) urls = [&#39;http://daum.net&#39;, &#39;https://naver.com&#39;, &#39;http://mlbpark.donga.com/&#39;, &#39;https://tistory.com&#39;, &#39;https://www.inflearn.com/&#39;] async def fetch(url, executor): # 쓰레드명 출력 print(&#39;Thread Name :&#39;, threading.current_thread().getName(), &#39;Start&#39;, url) # 실행 res = await loop.run_in_executor(executor, urlopen, url) soup = BeautifulSoup(res.read(), &#39;html.parser&#39;) # 전체 페이지 소스 확인 # print(soup.prettify()) # 이 부분에서 BeautifulSoup Selector(선택자)를 활용해서 다양한 정보 가져오기 가능 # 현 예제에서는 페이지 타이틀 정보 수집 result_data = soup.title print(&#39;Thread Name :&#39;, threading.current_thread().getName(), &#39;Done&#39;, url) # 결과 반환 return result_data async def main(): # 쓰레드 풀 생성 executor = ThreadPoolExecutor(max_workers=10) # future 객체 모아서 gather에서 실행 futures = [ asyncio.ensure_future(fetch(url, executor)) for url in urls ] # 결과 취합 rst = await asyncio.gather(*futures) print() print(&#39;Result : &#39;, rst) if __name__ == &#39;__main__&#39;: # 루프 초기화 loop = asyncio.get_event_loop() # 작업 완료 까지 대기 loop.run_until_complete(main()) # 수행 시간 계산 duration = timeit.default_timer() - start # 총 실행 시간 print(&#39;Total Running Time : &#39;, duration) &gt; Thread Name : MainThread Start http://daum.net Thread Name : MainThread Start https://naver.com Thread Name : MainThread Start http://mlbpark.donga.com/ Thread Name : MainThread Start https://tistory.com Thread Name : MainThread Start https://www.inflearn.com/ Thread Name : MainThread Done http://mlbpark.donga.com/ Thread Name : MainThread Done https://www.inflearn.com/ Thread Name : MainThread Done https://naver.com Thread Name : MainThread Done http://daum.net Thread Name : MainThread Done https://tistory.com Result : [&lt;title&gt;Daum&lt;/title&gt;, &lt;title&gt;NAVER&lt;/title&gt;, &lt;title&gt;↗ 파크에 오면 즐겁다 MLBPARK&lt;/title&gt;, &lt;title&gt;TISTORY&lt;/title&gt;, &lt;title&gt;인프런 - 프로가 되는 온라인 클래스 | 온라인 강의 플랫폼&lt;/title&gt;] Total Running Time : 6.4905200999999995 . .",
            "url": "https://nueees.github.io/techblog/python/2021/01/05/python-concurrency.html",
            "relUrl": "/python/2021/01/05/python-concurrency.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Python First Class Function",
            "content": "Python Intermediate Programming 스쿨오브웹 . . 4.1. First Class Function . an instance of the Object type 이미 정의된 여러 함수를 간단히 재활용 가능 . Python Function 특징 . 1.런타임 초기화 2.변수 할당 3.함수 인수 전달 4.함수 결과 반환 . def factorial(n): &#39;&#39;&#39;Factorial Function -&gt; n : int&#39;&#39;&#39; if n == 1: # n &lt; 2 return 1 return n * factorial(n-1) class A: pass print(factorial(5)) &gt; 120 print(factorial.__name__) # 이름 &gt; factorial print(factorial.__code__) # 코드 위치와 객체 정보 &gt; &lt;code object factorial at 0x00000209FFE98EA0, file &quot;&lt;stdin&gt;&quot;, line 1&gt; print(type(factorial), type(A)) # factorial 함수는 객체 타입이 function이고, A 함수는 객체 타입이 type으로 나옴 &gt; &lt;class &#39;function&#39;&gt; &lt;class &#39;type&#39;&gt; . 변수 할당 . var_func = factorial print(factorial, var_func) &gt; &lt;function factorial at 0x00000209FFDBAC10&gt; &lt;function factorial at 0x00000209FFDBAC10&gt; print(var_func(5)) &gt; 120 print(list(map(var_func, range(1,6)))) &gt; [1, 2, 6, 24, 120] . 함수 인수 전달 및 함수 결과 반환 . 고위 함수(Higher-order function): map, filter, reduce… . print(list(map(var_func, filter(lambda x: x % 2, range(1,6))))) # map함수 첫 인자로 계산할 함수 var_func 전달 &gt; [1, 6, 120] from functools import reduce from operator import add print(reduce(add, range(1,11))) # reduce함수 첫 인자로 계산할 함수 add 전달, 누적 &gt; 55 . Callable 확인 . print(set(sorted(dir(factorial))) - set(sorted(dir(A)))) # function 객체의 variables와 method 중 type 객체꺼 빼서 보면 __call__이 있음 factorial.__call(5) 계산 가능 &gt; {&#39;__globals__&#39;, &#39;__name__&#39;, &#39;__annotations__&#39;, &#39;__call__&#39;, &#39;__code__&#39;, &#39;__kwdefaults__&#39;, &#39;__qualname__&#39;, &#39;__get__&#39;, &#39;__defaults__&#39;, &#39;__closure__&#39;} print(callable(str), callable(A), callable(var_func), callable(3.14)) &gt; True True True False from inspect import signature sg = signature(var_func) print(sg) &gt; (n) print(sg.parameters) &gt; OrderedDict([(&#39;n&#39;, &lt;Parameter &quot;n&quot;&gt;)]) . callable: 함수 생성 후 해당 함수가 method 형태로 호출이 가능한지 확인 할 수 있음 signature: 어떤 파라메터를 받고 좀 더 상세하게 확인 가능한 lib . partial . 인수를 고정해서 함수 사용 시 편리 . from operator import add from functools import partial print(add(10,10)) &gt; 20 five = partial(add, 5) print(five(10)) # 5+10 &gt; 15 # 추가 six = partial(five, 6) print(six()) # 5+6 &gt; 11 print([five(i) for i in range(1,11)]) &gt; [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] print(list(map(five, range(1,11)))) &gt; [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] . . 4.2. Closure . 동시성 제어 자신의 영역 밖에서 호출된 함수의 변수값과 레퍼런스를 복사하고 저장한 뒤, 이 후에도 값들에 액세스할 수 있게 도와줌 다른 함수의 지역변수를 그 함수가 종료된 이후에도 상태 기억 프리변수(free variable): 클로저가 만들어지는 당시의 값과 레퍼런스에 맵핑하여 주는 역할 . ex) 결과 누적하는 함수들 (sum, reduce…) . Closure 특징 . 메모리 공간에 여러 자원이 접근 시 교착상태(Dead Lock) -&gt; 동시성(Concurrency)제어 필요 | 하나의 Thread 안에서 메모리를 공유하지 않고 “메시지 전달” | “메시지 전달”로 처리하기 위한 공유하되 변경되지 않는 Immutable, Read Only 적극적으로 사용 (함수형 프로그래밍) | closure는 Immutable 자료구조, STM(stands for Software Transactional Memory) -&gt; multi thread programming | Python에서는 Coroutine이란 개념을 이용해서 single thread로 병행성 제어 | Class vs Closure 비교 . class Averager(): # class def __init__(self): self._series = [] def __call__(self, v): self._series.append(v) print(&#39;inner &gt;&gt;&gt; {} / {}&#39;.format(self._series, len(self._series))) return sum(self._series) / len(self._series) averager_cls = Averager() # init print(averager_cls(15)) # call &gt; inner &gt;&gt;&gt; [15] / 1 print(averager_cls(35)) &gt; inner &gt;&gt;&gt; [15, 35] / 2 print(averager_cls(40)) &gt; inner &gt;&gt;&gt; [15, 35, 40] / 3 . def closure_ex1(): # closure series = [] # Free variable def averager(v): series.append(v) print(&#39;inner &gt;&gt;&gt; {} / {}&#39;.format(series, len(series))) return sum(series) / len(series) return averager # 함수 자체 리턴 avg_closure1 = closure_ex1() # create closure print(avg_closure1) # closure 확인 &gt; &lt;function closure_ex1.&lt;locals&gt;.averager at 0x000002142617EE50&gt; print(avg_closure1(15)) # call &gt; inner &gt;&gt;&gt; [15] / 1 print(avg_closure1(35)) &gt; inner &gt;&gt;&gt; [15, 35] / 2 print(avg_closure1(40)) &gt; inner &gt;&gt;&gt; [15, 35, 40] / 3 . 함수 내부 확인 . print(dir(avg_closure1)) &gt; [&#39;__annotations__&#39;, &#39;__call__&#39;, &#39;__class__&#39;, &#39;__closure__&#39;, &#39;__code__&#39;, &#39;__defaults__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__get__&#39;, &#39;__getattribute__&#39;, &#39;__globals__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__kwdefaults__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__name__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__qualname__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;] print(dir(avg_closure1.__code__)) &gt; [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;co_argcount&#39;, &#39;co_cellvars&#39;, &#39;co_code&#39;, &#39;co_consts&#39;, &#39;co_filename&#39;, &#39;co_firstlineno&#39;, &#39;co_flags&#39;, &#39;co_freevars&#39;, &#39;co_kwonlyargcount&#39;, &#39;co_lnotab&#39;, &#39;co_name&#39;, &#39;co_names&#39;, &#39;co_nlocals&#39;, &#39;co_posonlyargcount&#39;, &#39;co_stacksize&#39;, &#39;co_varnames&#39;, &#39;replace&#39;] print(avg_closure1.__code__.co_freevars) # 위 리스트에서 co_freevars 확인 &gt; (&#39;series&#39;,) print(dir(avg_closure1.__closure__[0])) &gt; [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;cell_contents&#39;] print(avg_closure1.__closure__[0].cell_contents) # 위 리스트에서 실제값 확인을 위해 cell_contents 확인 &gt; [15, 35, 40] . closure 실행 순서 확인 . def outer_func(): # 1 message = &#39;Hi&#39; # 3 def inner_func(): # 4 print(message) # 6 return inner_func() # 5 outer_func() # 2 &gt; Hi . 5에서 괄호를 지워면 Hi가 출력되지 않음 . def outer_func(): # 1 message = &#39;Hi&#39; # 3 def inner_func(): # 4 print(message) # 6 return inner_func # 5 outer_func() # 2 &gt; &lt;function outer_func.&lt;locals&gt;.inner_func at 0x00000214261B5280&gt; . nonlocal 변수 선언 . def outer_func(): cnt = 0 def inner_func(): nonlocal cnt cnt = cnt + 1 # inner에서 cnt를 nonlocal선언 안하고 쓰면 여기서 error남 print(cnt) return inner_func func1 = outer_func() func1() &gt; 1 func1() &gt; 2 func1() &gt; 3 . . 4.3. Decorator . 기존의 코드에 여러가지 기능을 추가하는 구문 함수를 다른 함수의 인자로 전달한다는 점이 특징 . Decorator 특징 . 장점 . 중복 제거, 코드 간결, 공통 함수 작성 | 로깅, 프레임워크, 유효성 체크 -&gt; 공통 기능 | 조합해서 사용 용이 | 단점 . 가독성 감소 | 특정 기능에 한정된 함수는 -&gt; 단일 함수로 작성하는 것이 유리 | 디버깅 불편 | Decorator 사용 vs 미사용 . def outer_function(msg): # 1 def inner_function(): # 5 print(msg) # 7 return inner_function # 6 hi_func = outer_function(&#39;Hi&#39;) # 2 bye_func = outer_function(&#39;Bye&#39;) hi_func() # 3 &gt; Hi bye_func() &gt; Bye . def decorator_function(original_function): def wrapper_function(*msg): #1 print(&#39;{} before calling function.&#39;.format(original_function.__name__)) return original_function(*msg) #2 return wrapper_function @decorator_function def display(): print(&#39;calling display function.&#39;) @decorator_function def display_info(msg): print(&#39;calling display_info({}) function.&#39;.format(msg)) display() &gt; display before calling function. &gt; calling display function. display_info(&#39;Hi&#39;) &gt; display_info before calling function. &gt; calling display_info(Hi) function. display_info(&#39;Bye&#39;) &gt; display_info before calling function. &gt; calling display_info(Bye) function. . Decorator 로깅 예제 . logging for debugging . import time def perf_clock(func): def perf_clocked(*args): st = time.perf_counter() result = func(*args) et = time.perf_counter() - st name = func.__name__ arg_str = &#39;, &#39;.join(repr(arg) for arg in args) print(&#39;[%0.5fs] %s(%s) -&gt; %r&#39; % (et, name, arg_str, result)) return result return perf_clocked @perf_clock def time_func(seconds): time.sleep(seconds) @perf_clock def sum_func(*numbers): return sum(numbers) . # no decorator none_deco1 = perf_clock(time_func) none_deco2 = perf_clock(sum_func) print(none_deco1) &gt; &lt;function perf_clock.&lt;locals&gt;.perf_clocked at 0x0000021426669280&gt; print(none_deco1.__code__.co_freevars) &gt; (&#39;func&#39;,) none_deco1(1.5) # time_func &gt; [1.50070s] time_func(1.5) -&gt; None none_deco2(100, 150, 250, 300, 350) # sum_func &gt; [0.00000s] sum_func(100, 150, 250, 300, 350) -&gt; 1150 . # decorator time_func(1.5) &gt; [1.50811s] time_func(1.5) -&gt; None sum_func(100, 150, 250, 300, 350) &gt; [0.00001s] sum_func(100, 150, 250, 300, 350) -&gt; 1150 . .",
            "url": "https://nueees.github.io/techblog/python/2021/01/04/python-firstclassfunction.html",
            "relUrl": "/python/2021/01/04/python-firstclassfunction.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Python Sequence",
            "content": "Python Intermediate Programming . . 3.0. Sequence type . 컨테이너 vs 플랫 . Container: 서로다른 자료형: list, tuple, collections.deque Flat: 한 개의 자료형 str, bytes, bytearray, array.array, memoryview . 가변 vs 불변 . Mutable: list, bytearray, array.array, memoryview, deque Immutable: tuple, str, bytes . 얕은 복사 vs 깊은 복사 . 리스트 생성 시 for문으로 재귀적으로 생성과 * 연산으로 생성 차이 . marks1 = [[&#39;A&#39;] * 5 for n in range(5)] marks2 = [[&#39;A&#39;] * 5] * 5 print(marks1) &gt; [[&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;]] print(marks2) &gt; [[&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;]] # 값 변경 marks1[0][1] = &#39;X&#39; marks2[0][1] = &#39;X&#39; print(marks1) &gt; [[&#39;A&#39;, &#39;X&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;]] print(marks2) &gt; [[&#39;A&#39;, &#39;X&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;X&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;X&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;X&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], [&#39;A&#39;, &#39;X&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;]] # id값 차이 확인 print([id(i) for i in marks1]) &gt; [1817473083648, 1817473082944, 1817473083136, 1817473083584, 1817473083712] print([id(i) for i in marks2]) &gt; [1817473087040, 1817473087040, 1817473087040, 1817473087040, 1817473087040] . shallow copy: 객체 껍데기만 복사하고 내용은 동일한 객체 참조 mutable한 list의 경우 수정되고, immutable한 tuple의 경우 별도의 객체에 저장됨 . import copy a = [1, [1, 2, 3]] b = copy.copy(a) # shallow copy print(b) &gt; [1, [1, 2, 3]] b[0] = 100 # immutable tuple 수정 print(b) &gt; [100, [1, 2, 3]] print(a) # immutable tuple의 경우 복사본만 수정 &gt; [1, [1, 2, 3]] c = copy.copy(a) # shallow copy c[1].append(4) # mutable list 수정 print(c) &gt; [1, [1, 2, 3, 4]] print(a) # mutable list의 경우 둘 다 값이 변경됨 &gt; [1, [1, 2, 3, 4]] . deep copy: 객체 복사 후 내용도 재귀적으로 복사 mutable, immutable 상관없이 별도의 객체에 저장됨 . a = [1, [1, 2, 3]] b = copy.deepcopy(a) # deep copy print(b) &gt; [1, [1, 2, 3]] b[0] = 100 b[1].append(4) print(b) &gt; [100, [1, 2, 3, 4]] print(a) &gt; [1, [1, 2, 3]] . 3.1. Generator . 한 번에 한 개의 항목을 생성(메모리 유지X) . Generator 생성 . chars = &#39;abc&#39; tuple_g = (ord(s) for s in chars) print(tuple_g) &gt; &lt;generator object &lt;genexpr&gt; at 0x000001A729CF75F0&gt; type(tuple_g) &gt; &lt;class &#39;generator&#39;&gt; print(next(tuple_g)) &gt; 97 print(next(tuple_g)) &gt; 98 print(next(tuple_g)) &gt; 99 print(next(tuple_g)) &gt; StopIteration Error . . 3.2. Tuple Advanced . unpacking . x, y, *rest = range(10) print(x, y, rest) &gt; 0 1 [2, 3, 4, 5, 6, 7, 8, 9] . . 3.3. HashTable . 적은 리소스로 많은 데이터를 효율적으로 관리 중복 허용 안되는 데이터 타입들 (Dict, Set) . Set vs List . t1 = (10, 20, (30, 40, 50)) t2 = (10, 20, [30, 40, 50]) print( hash(t1)) &gt; 465510690262297113 print( hash(t2)) # hash error남 . setdefault 미사용 (dict) . source = ((&#39;k1&#39;, &#39;val1&#39;), (&#39;k1&#39;, &#39;val2&#39;), (&#39;k2&#39;, &#39;val3&#39;), (&#39;k2&#39;, &#39;val4&#39;), (&#39;k2&#39;, &#39;val5&#39;)) new_dict1 = {} for k, v in source: if k in new_dict1: new_dict1[k].append(v) else: new_dict1[k] = [v] print(new_dict1) &gt; {&#39;k1&#39;: [&#39;val1&#39;, &#39;val2&#39;], &#39;k2&#39;: [&#39;val3&#39;, &#39;val4&#39;, &#39;val5&#39;]} . setdefault 사용 . new_dict2 = {} for k, v in source: new_dict2.setdefault(k, []).append(v) print(new_dict2) &gt; {&#39;k1&#39;: [&#39;val1&#39;, &#39;val2&#39;], &#39;k2&#39;: [&#39;val3&#39;, &#39;val4&#39;, &#39;val5&#39;]} . . 3.4. Dict &amp; Set Advanced . # immutable Dict from types import MappingProxyType d = {&#39;key1&#39;: &#39;value1&#39;} # Read Only d_frozen = MappingProxyType(d) print(d, id(d)) &gt; {&#39;key1&#39;: &#39;value1&#39;} 1817473569408 print(d_frozen, id(d_frozen)) &gt; {&#39;key1&#39;: &#39;value1&#39;} 1817472793280 print(d is d_frozen, d == d_frozen) &gt; False True # 수정 불가 d_frozen[&#39;key1&#39;] = &#39;value2&#39; # mappingproxy 객체는 assign지원하지 않는다는 error d[&#39;key2&#39;] = &#39;value2&#39; print(d) &gt; {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;} s1 = {&#39;Apple&#39;, &#39;Orange&#39;, &#39;Apple&#39;, &#39;Orange&#39;, &#39;Kiwi&#39;} s2 = set([&#39;Apple&#39;, &#39;Orange&#39;, &#39;Apple&#39;, &#39;Orange&#39;, &#39;Kiwi&#39;]) s3 = {3} s4 = set() # Not {} s5 = frozenset({&#39;Apple&#39;, &#39;Orange&#39;, &#39;Apple&#39;, &#39;Orange&#39;, &#39;Kiwi&#39;}) # 추가 s1.add(&#39;Melon&#39;) # 추가 불가 s5.add(&#39;Melon&#39;) # frozenset은 add 속성 존재하지 않는다는 error print(s1, type(s1)) &gt; {&#39;Orange&#39;, &#39;Apple&#39;, &#39;Kiwi&#39;, &#39;Melon&#39;} &lt;class &#39;set&#39;&gt; print(s2, type(s2)) &gt; {&#39;Orange&#39;, &#39;Apple&#39;, &#39;Kiwi&#39;} &lt;class &#39;set&#39;&gt; print(s3, type(s3)) &gt; {3} &lt;class &#39;set&#39;&gt; print(s4, type(s4)) &gt; set() &lt;class &#39;set&#39;&gt; print(s5, type(s5)) &gt; frozenset({&#39;Orange&#39;, &#39;Apple&#39;, &#39;Kiwi&#39;}) &lt;class &#39;frozenset&#39;&gt; . .",
            "url": "https://nueees.github.io/techblog/python/2021/01/03/python-sequence.html",
            "relUrl": "/python/2021/01/03/python-sequence.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Python Magic Method",
            "content": "Python Intermediate Programming docs.python.org . . 2.1. Built-In Functions . Basic Data Types . basic numeric, string, boolean . print(int) print(float) print(str) print(bool) &gt; &lt;class &#39;str&#39;&gt; # 모두 클래스로 만들어져 있음 print(dir(str)) # 모든 속성 메소드 확인 &gt; [&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__float__&#39;, &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;, &#39;__pos__&#39;, &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;, &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;as_integer_ratio&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;, &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;] . n = 10 print(n.__doc__) # 위처럼 숫자 할당 시 처리 방법 &gt; int([x]) -&gt; integer int(x, base=10) -&gt; integer Convert a number or string to an integer, or return 0 if no arguments are given. If x is a number, return x.__int__(). For floating point numbers, this truncates towards zero. If x is not a number or if base is given, then x must be a string, bytes, or bytearray instance representing an integer literal in the given base. The literal can be preceded by &#39;+&#39; or &#39;-&#39; and be surrounded by whitespace. The base defaults to 10. Valid bases are 0 and 2-36. Base 0 means to interpret the base from the string as an integer literal. &gt;&gt;&gt; int(&#39;0b100&#39;, base=0) 4 print(n + 100, n.__add__(100)) &gt; 110 110 print(n * 100, n.__mul__(100)) &gt; 1000 1000 print(n.__bool__(), bool(n)) &gt; True True . 2.2. Magic Methods . class Vector(object): def __init__(self, *args): &#39;&#39;&#39;Create a vector, example : v = Vector(5,10)&#39;&#39;&#39; if len(args) == 0: self._x, self._y = 0, 0 else: self._x, self._y = args def __repr__(self): &#39;&#39;&#39;Returns the vector infomations&#39;&#39;&#39; return &#39;Vector(%r, %r)&#39; % (self._x, self._y) def __add__(self, other): &#39;&#39;&#39;Returns the vector addition of self and other&#39;&#39;&#39; return Vector(self._x + other._x, self._y + other._y) def __mul__(self, y): return Vector(self._x * y, self._y * y) def __bool__(self): return bool(max(self._x, self._y)) # create instance v1 = Vector(5,7) v2 = Vector(23, 35) v3 = Vector() . ge/le/add/sub/mul/div/str . print(Vector.__init__.__doc__) &gt; Create a vector, example : v = Vector(5,10) print(Vector.__repr__.__doc__) &gt; Returns the vector infomations print(Vector.__add__.__doc__) &gt; Returns the vector addition of self and other print(v1, v2, v3) &gt; Vector(5, 7) Vector(23, 35) Vector(0, 0) print(v1 + v2) &gt; Vector(28, 42) print(v1 * 3) &gt; Vector(15, 21) print(bool(v1), bool(v2), bool(v3) &gt; True True False . . 2.3. Named Tuple . Tuple 사용 . from math import sqrt pt1 = (1.0, 5.0) pt2 = (2.5, 1.5) l_leng1 = sqrt((pt2[0] - pt1[0]) ** 2 + (pt2[1] - pt1[1]) ** 2) print(l_leng1) . 인덱스 에러 가능성 있음 . Named Tuple 사용 . from collections import namedtuple Point = namedtuple(&#39;Point&#39;, &#39;x y&#39;) pt3 = Point(1.0, 5.0) pt4 = Point(2.5, 1.5) l_leng2 = sqrt((pt4.x - pt3.x) ** 2 + (pt4.y - pt3.y) ** 2) print(l_leng2) . Named Tuple 선언 . Point = namedtuple(&#39;Point&#39;, &#39;x y&#39;) Point1 = namedtuple(&#39;Point&#39;, [&#39;x&#39;, &#39;y&#39;]) Point2 = namedtuple(&#39;Point&#39;, &#39;x, y&#39;) Point3 = namedtuple(&#39;Point&#39;, &#39;x y x class&#39;, rename=True) # 자동으로 중복되는 필드네임 _2, _3으로 바뀜 Point4 = namedtuple(&#39;Point&#39;, &#39;x y x&#39;, rename=False) # 필드네임 중복 에러남 default . Unpacking . p1 = Point1(45, y=20) print(p1) &gt; Point(x=45, y=20) x, y = p1 print(x, y) &gt; 45 20 . _make . temp = [10,20,30,40] p2 = Point3._make(temp) print(p2) &gt; Point(x=10, y=20, _2=30, _3=40) . instance 생성 . _fields . print(p1._fields, p2._fields) &gt; (&#39;x&#39;, &#39;y&#39;) (&#39;x&#39;, &#39;y&#39;, &#39;_2&#39;, &#39;_3&#39;) . field name 확인 . _asdict . print(p1._asdict(), p2._asdict()) &gt; {&#39;x&#39;: 45, &#39;y&#39;: 20} {&#39;x&#39;: 10, &#39;y&#39;: 20, &#39;_2&#39;: 30, &#39;_3&#39;: 40} . field name의 value 확인 (OrderedDict으로 return) . 2.4. Named Tuple 예제 . Classes = namedtuple(&#39;Classes&#39;, [&#39;rank&#39;, &#39;number&#39;]) # List Comprehension으로 students2 = [Classes(rank, number) for rank in &#39;A B C D&#39;.split() for number in [str(n) for n in range(1,21)]] for s in student2: print(s) . .",
            "url": "https://nueees.github.io/techblog/python/2021/01/02/python-magicmethod.html",
            "relUrl": "/python/2021/01/02/python-magicmethod.html",
            "date": " • Jan 2, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Python Class & Method",
            "content": "Python Intermediate Programming . . 1.1. OOP (Object Oriented Programming) . 객체 지향 프로그래밍: 코드 재사용 . Using List . car_company_list = [&#39;Ferrari&#39;, &#39;Bmw&#39;, &#39;Audi&#39;] car_detail_list = [ {&#39;color&#39; : &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000}, {&#39;color&#39; : &#39;Black&#39;, &#39;horsepower&#39;: 270, &#39;price&#39;: 5000}, {&#39;color&#39; : &#39;Silver&#39;, &#39;horsepower&#39;: 300, &#39;price&#39;: 6000} ] car_company_list[1] del car_detail_list[1] . 관리가 불편하고 인덱스로 접근하기 때문에 오류 가능성이 높음 . Using Dictionary . cars_dicts = [ {&#39;car_company&#39;: &#39;Ferrari&#39;, &#39;car_detail&#39;: {&#39;color&#39; : &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000}}, {&#39;car_company&#39;: &#39;Bmw&#39;, &#39;car_detail&#39;: {&#39;color&#39; : &#39;Black&#39;, &#39;horsepower&#39;: 270, &#39;price&#39;: 5000}}, {&#39;car_company&#39;: &#39;Audi&#39;, &#39;car_detail&#39;: {&#39;color&#39; : &#39;Silver&#39;, &#39;horsepower&#39;: 300, &#39;price&#39;: 6000}} ] del cars_dicts[1] . 재사용성이 낮고, 중첩시 overwrite, 키 조회 예외 처리 따로 해야함 . Using Class . class Car(): def __init__(self, company, details): self._company = company self._details = details def __str__(self): return &#39;str : {} - {}&#39;.format(self._company, self._details) def __repr__(self): return &#39;repr : {} - {}&#39;.format(self._company, self._details) car1 = Car(&#39;Ferrari&#39;, {&#39;color&#39; : &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000}) car2 = Car(&#39;Bmw&#39;, {&#39;color&#39; : &#39;Black&#39;, &#39;horsepower&#39;: 270, &#39;price&#39;: 5000}) car3 = Car(&#39;Audi&#39;, {&#39;color&#39; : &#39;Silver&#39;, &#39;horsepower&#39;: 300, &#39;price&#39;: 6000}) print(car1.__dict__) &gt; {&#39;_company&#39;: &#39;Ferrari&#39;, &#39;_details&#39;: {&#39;color&#39;: &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000}} . 재사용성 높고, 메소드 활용 가능 . Class variables . class Car(): &quot;&quot;&quot; Car Class Author : Cho Date : 2021.01.01 &quot;&quot;&quot; # 클래스 변수 car_count = 0 def __init__(self, company, details): self._company = company self._details = details Car.car_count += 1 def __str__(self): return &#39;str : {} - {}&#39;.format(self._company, self._details) def __repr__(self): return &#39;repr : {} - {}&#39;.format(self._company, self._details) def detail_info(self): print(&#39;Current Id : {}&#39;.format(id(self))) print(&#39;Car Detail Info : {} {}&#39;.format(self._company, self._details.get(&#39;price&#39;))) def __del__(self): Car.car_count -= 1 . self 값 . car1 = Car(&#39;Ferrari&#39;, {&#39;color&#39; : &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000}) car2 = Car(&#39;Bmw&#39;, {&#39;color&#39; : &#39;Black&#39;, &#39;horsepower&#39;: 270, &#39;price&#39;: 5000}) car3 = Car(&#39;Audi&#39;, {&#39;color&#39; : &#39;Silver&#39;, &#39;horsepower&#39;: 300, &#39;price&#39;: 6000}) print(car1) &gt; str : Ferrari - {&#39;color&#39;: &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000} . id 값 . print(id(car1)) &gt; 1817466634144 . dir 값 . print(dir(car1)) &gt; [&#39;__class__&#39;, &#39;__del__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_company&#39;, &#39;_details&#39;, &#39;car_count&#39;, &#39;detail_info&#39;] . dict 값 . print(car1.__dict__) &gt; {&#39;_company&#39;: &#39;Ferrari&#39;, &#39;_details&#39;: {&#39;color&#39;: &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000}} . doc 값 . print(Car.__doc__) &gt; Car Class Author : Kim Date : 2019.11.08 . user defined method 값 . car1.detail_info() &gt; Current Id : 1817466634144 Car Detail Info : Ferrari 8000 Car.detail_info() # self missing error 발생 Car.detail_info(car1) &gt; Current Id : 1817466634144 Car Detail Info : Ferrari 8000 . class 비교 . print(car1.__class__, car2.__class__) &gt; &lt;class &#39;__main__.Car&#39;&gt; &lt;class &#39;__main__.Car&#39;&gt; print(id(car1.__class__) == id(car2.__class__)) &gt; True . Instance variables . print(car1._company) # 직접접근 방법, 권장 X &gt; Ferrari . Class variables . print(car1.car_count) &gt; 3 print(Car.car_count) &gt; 3 . class 공유 확인 . print(Car.__dict__) &gt; {&#39;__module__&#39;: &#39;__main__&#39; , &#39;__doc__&#39;: &#39; n Car Class n Author : Kim n Date : 2019.11.08 n&#39; , &#39;car_count&#39;: 2 , &#39;__init__&#39;: &lt;function Car.__init__ at 0x000001A729721CA0&gt; , &#39;__str__&#39;: &lt;function Car.__str__ at 0x000001A7297C0550&gt; , &#39;__repr__&#39;: &lt;function Car.__repr__ at 0x000001A7297C05E0&gt; , &#39;detail_info&#39;: &lt;function Car.detail_info at 0x000001A7297C0670&gt; , &#39;__del__&#39;: &lt;function Car.__del__ at 0x000001A7297C0700&gt; , &#39;__dict__&#39;: &lt;attribute &#39;__dict__&#39; of &#39;Car&#39; objects&gt; , &#39;__weakref__&#39;: &lt;attribute &#39;__weakref__&#39; of &#39;Car&#39; objects&gt;} print(car1.__dict__) &gt; {&#39;_company&#39;: &#39;Ferrari&#39;, &#39;_details&#39;: {&#39;color&#39;: &#39;White&#39;, &#39;horsepower&#39;: 400, &#39;price&#39;: 8000}} . instance namespace 없으면 class(상위)에서 검색 . print(car1.car_count) &gt; 2 print(Car.car_count) &gt; 2 . . 1.2. Method . class Car(object): &#39;&#39;&#39; Car Class Author : Cho Date : 2021.01.01 Description : Class, Static, Instance Method &#39;&#39;&#39; # Class Variable price_per_raise = 1.0 def __init__(self, company, details): self._company = company # 인스턴스 변수 앞에 언더바 붙임 self._details = details # Instance Method # self : 객체의 고유한 속성 값 사용 def detail_info(self): # 첫변수 self로 받음 print(&#39;Current Id : {}&#39;.format(id(self))) print(&#39;Car Detail Info : {} {}&#39;.format(self._company, self._details.get(&#39;price&#39;))) # Instance Method def get_price(self): return &#39;Before Car Price -&gt; company : {}, price : {}&#39;.format(self._company, self._details.get(&#39;price&#39;)) # Instance Method def get_price_culc(self): return &#39;After Car Price -&gt; company : {}, price : {}&#39;.format(self._company, self._details.get(&#39;price&#39;) * Car.price_per_raise) # Class Method @classmethod def raise_price(cls, per): # 첫변수 무조건 cls로 받음 if per &lt;= 1: print(&#39;Please Enter 1 or More&#39;) return cls.price_per_raise = per return &#39;Succeed! price increased.&#39; # Static Method @staticmethod def is_bmw(inst): # 전달 변수 없어도 되나 차량확인시 필요 변수 inst 전달 if inst._company == &#39;Bmw&#39;: return &#39;OK! This car is {}.&#39;.format(inst._company) return &#39;Sorry. This car is not Bmw.&#39; . car1 = Car(&#39;Bmw&#39;, {&#39;color&#39; : &#39;Black&#39;, &#39;horsepower&#39;: 270, &#39;price&#39;: 5000}) car2 = Car(&#39;Audi&#39;, {&#39;color&#39; : &#39;Silver&#39;, &#39;horsepower&#39;: 300, &#39;price&#39;: 6000}) . Class Method . 데코레이터 @classmethod 사용 첫 변수로 cls 받아서 사용 . Instance Method . 객체의 고유한 속성 값 self 변수 받아서 사용 . Static Method . 데코레이터 @staticmethod 사용 class나 instance method처럼 cls나 self를 받지 않고 구현 .",
            "url": "https://nueees.github.io/techblog/python/2021/01/01/python-class-method.html",
            "relUrl": "/python/2021/01/01/python-class-method.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Breadth First Search(BFS)",
            "content": "Python Algorithm Practice . . 7.1. 최대점수 구하기 (DFS) . 이번 정보올림피아드대회에서 좋은 성적을 내기 위하여 현수는 선생님이 주신 N개의 문제를 풀려고 합니다. 각 문제는 그것을 풀었을 때 얻는 점수와 푸는데 걸리는 시간이 주어지게 됩니다. 제한시간 M안에 N개의 문제 중 최대점수를 얻을 수 있도록 해야 합니다. (해당문제는 해당시간이 걸리면 푸는 걸로 간주한다, 한 유형당 한개만 풀 수 있습니다.) . def DFS(L, sum, time): global res if time&gt;m: return if L==n: if sum&gt;res: res=sum else: DFS(L+1, sum+pv[L], time+pt[L]) DFS(L+1, sum, time) if __name__==&quot;__main__&quot;: n, m=map(int, input().split()) pv=list() pt=list() for i in range(n): a, b=map(int, input().split()) pv.append(a) pt.append(b) res=-2147000000 DFS(0, 0, 0) print(res) . . 7.2. . . 7.3. . . 7.4. . . 7.5. . . 7.6. . . 7.7. . . 7.8. . . 7.9. . . 7.10. . . 7.11. . . 7.12. . . 7.13. . . 7.14. . . 7.15. . . 7.16. . . 7.17. .",
            "url": "https://nueees.github.io/techblog/algorithm/python/2020/12/07/bfs.html",
            "relUrl": "/algorithm/python/2020/12/07/bfs.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Depth First Search(DFS)",
            "content": "Python Algorithm Practice . . 6.1. Recursive program for binary number (DFS) . 10진수 N이 입력되면 2진수로 변환하여 출력하는 프로그램을 작성하세요. 단 재귀함수를 이용해서 출력해야 합니다. . num = int(input()) def to_binary(num): if num == 0: return else: to_binary(num//2) print(num%2, end=&#39;&#39;) to_binary(num) . . 6.2. Subset (DFS) . 자연수 N이 주어지면 1부터 N까지의 원소를 갖는 집합의 부분집합을 모두 출력하는 프로그램을 작성하세요. . def tree(root): if root == n+1: for i in range(1,n+1): if a[i] == 1: print(i, end=&#39;&#39;) print() else: a[root] = 1 tree(root+1) a[root] = 0 tree(root+1) n = int(input()) a = [0]*(n+1) tree(1) . . 6.3. Subset sum problem (DFS) . N개의 원소로 구성된 자연수 집합이 주어지면, 이 집합을 두 개의 부분집합으로 나누었을 때 두 부분집합의 원소의 합이 서로 같은 경우가 존재하면 “YES”를 출력하고, 그렇지 않으면 “NO”를 출력하는 프로그램을 작성하세요. 둘로 나뉘는 두 부분집합은 서로소 집합이며, 두 부분집합을 합하면 입력으로 주어진 원래의 집합이 되어 합니다. 예를 들어 {1, 3, 5, 6, 7, 10}이 입력되면 {1, 3, 5, 7} = {6, 10} 으로 두 부분집합의 합이 16으로 같은 경우가 존재하는 것을 알 수 있다. . def tree(L, sum): if sum &gt; total//2: return if L == n: if sum == (total-sum): print(&quot;YES&quot;) sys.exit(0) else: tree(L+1, sum+a[L]) tree(L+1, sum) if __name__==&quot;__main__&quot;: n = int(input()) a = list(map(int, input().split())) total = sum(a) tree(0, 0) print(&quot;NO&quot;) . . 6.4. Getting on truck (DFS) . 트럭은 C(kg) 넘게 태울수가 없다. C(kg)을 넘지 않으면서 사람을 가장 무겁게 태우고 싶다. N명의 사람 수와 각 사람 수의 무게 W가 주어지면, 트럭에 태울 수 있는 가장 무거운 무게를 구하는 프로그램을 작성하세요. . def tree(L, sum): global result if sum &gt; c: return if L == n: if sum &gt; result: result = sum else: tree(L+1, sum+a[L]) tree(L+1, sum) if __name__==&quot;__main__&quot;: c, n = map(int, input().split()) a = [0]*n result = -1 for i in range(n): a[i] = int(input()) tree(0,0) print(result) . . 6.5. Duplicated permutation (DFS) . 1부터 N까지 번호가 적힌 구슬이 있습니다. 이 중 중복을 허락하여 M번을 뽑아 일렬로 나열하는 방법을 모두 출력합니다. . def tree(L): global cnt if L == m: print(res) cnt += 1 return else: for i in range(1, n+1): res[L] = i tree(L+1) if __name__==&quot;__main__&quot;: n, m = map(int, input().split()) res = [0]*m cnt = 0 tree(0) print(cnt) . . 6.6. Give change (DFS) . 다음과 같이 여러 단위의 동전들이 주어져 있을때 거스름돈을 가장 적은 수의 동전으로 교환해주려면 어떻게 주면 되는가? 각 단위의 동전은 무한정 쓸 수 있다. . def tree(L, sum): # L동전개수 global res if L &gt; res: # 최소 동전개수 이상이면 return return if sum &gt; m: return if sum == m: if L &lt; res: res = L else: for i in range(n): tree(L+1, sum+a[i]) if __name__ == &quot;__main__&quot;: n = int(input()) a = list(map(int, input().split())) m = int(input()) res = 2**1000 a.sort(reverse=True) tree(0,0) print(res) . . 6.7. Get permutation (DFS) . 1부터 N까지 번호가 적힌 구슬이 있습니다. 이 중 M개를 뽑아 일렬로 나열하는 방법을 모두 출력합니다. . def tree(L): global cnt if L == m: for i in range(m): print(res[i], end=&#39; &#39;) print() cnt += 1 else: for i in range(1,n+1): if ch[i] == 0: ch[i] = 1 res[L] = i tree(L+1) ch[i] = 0 if __name__==&quot;__main__&quot;: n, m = map(int, input().split()) res = [0]*m ch = [0]*(n+1) cnt = 0 tree(0) print(cnt) . . 6.8. Pascal’s triangle (DFS) . 가장 윗줄에 1부터 N까지의 숫자가 한 개씩 적혀 있다. 그리고 둘째 줄부터 차례대로 파스칼의 삼각형처럼 위의 두개를 더한 값이 저장되게 된다. 예를 들어 N이 4 이고 가장 윗 줄에 3 1 2 4 가 있다고 했을 때, 다음과 같은 삼각형이 그려진다. 3　1　2　4 　4　3　6 　　7　9 　　16 . def tree(L, sum): if L == 4 and sum == f: for i in p: print(i, end=&#39; &#39;) sys.exit(0) else: for i in range(1, n+1): if ch[i] == 0: ch[i] = 1 p[L] = i tree(L+1, sum+(p(L)*b[L])) ch[i] = 0 if __name__ == &quot;__main__&quot;: n, f = map(int, input().split()) p = [0]*n b = [1]*n ch = [0]*(n+1) for i in range(1, n): b[i] = b[i-1] * (n-i)//i print(b[i]) tree(0, 0) # package 사용 n, f = map(int, input().split()) b = [1]*n cnt = 0 for i in range(1, n): b[i] = b[i-1]*(n-i)//i a = list(range(1, n+1)) for tmp in it.permutations(a): sum = 0 for L, x in enumerate(tmp): sum += (x*b[L]) if sum == f: for x in tmp: print(x, end=&#39; &#39;) break . . 6.9. Get combination (DFS) . 1부터 N까지 번호가 적힌 구슬이 있습니다. 이 중 M개를 뽑는 방법의 수를 출력하는 프로그램을 작성하세요. . def tree(L, s): global cnt if L == m: for i in range(m): print(res[i], end=&#39; &#39;) print() cnt += 1 else: for i in range(s, n+1): res[L] = tree(L+1, i+1) n, m = map(int, input().split()) res = [0]*(n+1) cnt = 0 tree(0, 1) print(cnt) . . 6.10. Guess combination (DFS) . N개의 정수가 주어지면 그 숫자들 중 K개를 뽑는 조합의 합이 임의의 정수 M의 배수인 개수는 몇 개가 있는지 출력하는 프로그램을 작성하세요. 예를 들면 5개의 숫자 2 4 5 8 12가 주어지고, 3개를 뽑은 조합의 합이 6의 배수인 조합을 찾으면 4+8+12 2+4+12로 2가지가 있습니다. . def tree(L, s, sum): global cnt if L == k: if sum%m == 0: cnt += 1 else: for i in range(s, n): tree(L+1, i+1, sum+a[i]) if __name__==&quot;__main__&quot;: n, k = map(int, input().split()) a = list(map(int, input().split())) m = int(input()) cnt = 0 tree(0, 0, 0) print(cnt) # package 사용 n, k = map(int, input().split()) a = list(map(int, input().split())) m = int(input()) cnt = 0 for x in it.combinations(a, k): if sum(x)%m == 0: cnt += 1 print(cnt) . . 6.11. Adjacent matrix (DFS) . 아래 그림과 같은 그래프 정보를 인접행렬로 표현해보세요. . n = int(input()) m = int(input()) g = [[0]*(n+1) for _ in range(n+1)] for i in range(m): a, b = map(int, input().split()) g[a][b]=1 g[b][a]=1 for i in range(1, n+1): for j in range(1, n+1): print(g[i][j], end=&#39; &#39;) print() . . 6.12. Breadth First Search (BFS) . 방향그래프가 주어지면 1번 정점에서 N번 정점으로 가는 모든 경로의 가지 수를 출력하는 프로그램을 작성하세요. 방문한 노드는 중복해서 방문하지 않습니다. 위 그래프에서 1번 정점에서 5번 정점으로 가는 가지 수는 총 6가지입니다. 1 2 3 4 5 1 2 5 1 3 4 2 5 1 3 4 5 1 4 2 5 1 4 5 . def tree(vt): global cnt if vt == n: cnt += 1 # for x in path: # print(x, end=&#39; &#39;) else: for i in range(1, n+1): if g[vt][i]==1 and ch[i]==0: ch[i] = 1 # path.append(i) tree(i) # path.pop() ch[i] = 0 if __name__==&quot;__main__&quot;: n, m = map(int, input().split()) g = [[0]*(n+1) for _ in range((n+1))] ch = [0]*(n+1) # dup check for i in range(m): a, b = map(int, input().split()) g[a][b] = 1 cnt = 0 # path = [] # path.append(1) ch[1] = 1 tree(1) print(cnt) . .",
            "url": "https://nueees.github.io/techblog/algorithm/python/2020/12/06/dfs.html",
            "relUrl": "/algorithm/python/2020/12/06/dfs.html",
            "date": " • Dec 6, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Stack, Queue, Hash, Heap algorithm",
            "content": "Python Algorithm Practice . . 5.1. Biggest number (stack) . 선생님은 현수에게 숫자 하나를 주고, 해당 숫자의 자릿수들 중 m개의 숫자를 제거하여 가장 큰 수를 만들라고 했습니다. 여러분이 현수를 도와주세요.(단 숫자의 순서는 유지해야 합니다) 만약 5276823 이 주어지고 3개의 자릿수를 제거한다면 7823이 가장 큰 숫자가 됩니다. . num, m = map(int, input().split()) num = list(map(int,str(num))) stack = [] for i in num: while stack and m &gt; 0 and stack[-1] &lt; i: stack.pop() m -= 1 stack.append(i) if m != 0: stack = stack[:m] res = &#39;&#39;.join(map(str, stack)) print(res) . . 5.2. Iron bar (stack) . 여러 개의 쇠막대기를 레이저로 절단하려고 한다. 효율적인 작업을 위해서 쇠막대기를 아래에서 위로 겹쳐 놓고, 레이저를 위에서 수직으로 발사하여 쇠막대기들을 자른다. 쇠막대기와 레이저의 배치는 다음 조건을 만족한다. • 쇠막대기는 자신보다 긴 쇠막대기 위에만 놓일 수 있다. • 쇠막대기를 다른 쇠막대기 위에 놓는 경우 완전히 포함되도록 놓되, 끝점은 겹치지 않도록 놓는다. • 각 쇠막대기를 자르는 레이저는 적어도 하나 존재한다. • 레이저는 어떤 쇠막대기의 양 끝점과도 겹치지 않는다. 아래 그림은 위 조건을 만족하는 예를 보여준다. 수평으로 그려진 굵은 실선은 쇠막대기이고, 점은 레이저의 위치, 수직으로 그려진 점선 화살표는 레이저의 발사 방향이다. 이러한 레이저와 쇠막대기의 배치는 다음과 같이 괄호를 이용하여 왼쪽부터 순서대로 표현할 수 있다. . 레이저는 여는 괄호와 닫는 괄호의 인접한 쌍 ‘( )’ 으로 표현된다. 또한, 모든 ‘( )’는 반드시 레이저를 표현한다. | 쇠막대기의 왼쪽 끝은 여는 괄호 ‘(’ 로, 오른쪽 끝은 닫힌 괄호 ‘)’ 로 표현된다. 위 예의 괄호 표현은 그림 위에 주어져 있다. 쇠막대기는 레이저에 의해 몇 개의 조각으로 잘려지는데, 위 예에서 가장 위에 있는 두 개의 쇠막대기는 각각 3개와 2개의 조각으로 잘려지고, 이와 같은 방식으로 주어진 쇠막대기들은 총 17개의 조각으로 잘려진다. 쇠막대기와 레이저의 배치를 나타내는 괄호 표현이 주어졌을 때, 잘려진 쇠막대기 조각의 총 개수를 구하는 프로그램을 작성하시오. | bar = input() stack = [] cnt = 0 prev = &#39;)&#39; for i in range(len(bar)): # print(bar[i]) if bar[i] == &#39;(&#39;: stack.append(bar[i]) else: # &#39;)&#39; stack.pop() if bar[i-1] == &#39;(&#39;: cnt += len(stack) else: # &#39;)&#39; cnt += 1 print(cnt) . . 5.3. Evaluation of Postfix Expression (stack) . 중위표기식이 입력되면 후위표기식으로 변환하는 프로그램을 작성하세요. 중위표기식은 우리가 흔히 쓰은 표현식입니다. 즉 3+5 와 같이 연산자가 피연산자 사이에 있으면 중위표기식입니다. 후위표기식은 35+ 와 같이 연산자가 피연산자 뒤에 있는 표기식입니다. 예를 들어 중위표기식이 3+5*2 를 후위표기식으로 표현하면 352*+ 로 표현됩니다. 만약 다음과 같이 연산 최우선인 괄호가 표현된 식이라면 (3+5)*2 이면 35+2* 로 바꾸어야 합니다. . exp = input() stack = [] res = &#39;&#39; for x in exp: if x.isdecimal(): res += x else: if x == &#39;(&#39;: stack.append(x) elif x == &#39;)&#39;: while stack and (stack[-1] != &#39;(&#39;): res += stack.pop() stack.pop() elif x == &#39;*&#39; or x == &#39;/&#39;: while stack and (stack[-1] == &#39;*&#39; or stack[-1] == &#39;/&#39;): res += stack.pop() stack.append(x) elif x == &#39;+&#39; or x == &#39;-&#39;: while stack and (stack[-1] != &#39;(&#39;): res += stack.pop() stack.append(x) while stack: res += stack.pop() print(res) . . 5.4. Evaluation of Postfix (stack) . 후위연산식이 주어지면 연산한 결과를 출력하는 프로그램을 작성하세요. 만약 3*(5+2)-9 을 후위연산식으로 표현하면 352+*9- 로 표현되며 그 결과는 21입니다. . exp = input() stack = [] for x in exp: if x.isdecimal(): stack.append(int(x)) else: n2 = stack.pop() n1 = stack.pop() if x == &#39;*&#39;: stack.append(n1*n2) if x == &#39;/&#39;: stack.append(n1/n2) if x == &#39;+&#39;: stack.append(n1+n2) if x == &#39;-&#39;: stack.append(n1-n2) print(stack.pop()) . . 5.5. Last survivor (queue) . 왕은 왕자들을 나이 순으로 1번부터 N번까지 차례로 번호를 매긴다. 그리고 1번 왕자부터 N번 왕자까지 순서대로 시계 방향으로 돌아가며 동그랗게 앉게 한다. 그리고 1번 왕자부터 시계방향으로 돌아가며 1부터 시작하여 번호를 외치게 한다. 한 왕자가 K(특정숫자)를 외치면 그 왕자는 밖으로 나오게 된다. 그리고 다음 왕자부터 다시 1부터 시작하여 번호를 외친다. 이렇게 해서 마지막까지 남은 왕자가 살아남는다. 예를 들어 총 8명의 왕자가 있고, 3을 외친 왕자가 제외된다고 하자. 처음에는 3번 왕자가 3을 외쳐 제외된다. 이어 6, 1, 5, 2, 8, 4번 왕자가 차례대로 제외되고 마지막까지 남게 된 7번 왕자가 살아 남습니다. N과 K가 주어질 때 살아남는 왕자의 번호를 출력하는 프로그램을 작성하시오. . from collections import deque n, k = map(int, input().split()) dequeue = list(range(1,n+1)) dq = deque(dequeue) while dq: for _ in range(k-1): tmp = dq.popleft() dq.append(tmp) res = dq.popleft() print(res) . . 5.6. Prioritizing in emergency room (queue) . 메디컬 병원 응급실에는 의사가 한 명밖에 없습니다. 응급실은 환자가 도착한 순서대로 진료를 합니다. 하지만 위험도가 높은 환자는 빨리 응급조치를 의사가 해야 합니다. 이런 문제를 보완하기 위해 응급실은 다음과 같은 방법으로 환자의 진료순서를 정합니다. • 환자가 접수한 순서대로의 목록에서 제일 앞에 있는 환자목록을 꺼냅니다. • 대기 목록에서 꺼낸 환자 보다 위험도가 높은 환자가 존재하면 대기목록 제일 뒤로 다시 넣습니다. 그렇지 않으면 진료를 받습니다. 즉 대기목록에 자기 보다 위험도가 높은 환자가 없을 때 자신이 진료를 받는 구조입니다. 현재 N명의 환자가 대기목록에 있습니다. N명의 대기목록 순서의 환자 위험도가 주어지면, 대기목록상의 M번째 환자는 몇 번째로 진료를 받는지 출력하는 프로그램을 작성하세요. 대기목록상의 M번째는 대기목록의 제일 처음 환자를 0번째로 간주하여 표현한 것입니다. . from collections import deque n, p = map(int, input().split()) list(map(int, input().split())) a = [(pos, val) for pos, val in enumerate(list(map(int, input().split())))] dq = deque(a) cnt = 0 while dq: tmp = dq.popleft() if any(tmp[1] &lt; x[1] for x in dq): dq.append(tmp) else: cnt += 1 if tmp[0] == p: break print(cnt) . . 5.7. Making a curriculum (queue) . 현수는 1년 과정의 수업계획을 짜야 합니다. 수업중에는 필수과목이 있습니다. 이 필수과목은 반드시 이수해야 하며, 그 순서도 정해져 있습니다. 만약 총 과목이 A, B, C, D, E, F, G가 있고, 여기서 필수과목이 CBA로 주어지면 필수과목은 C, B, A과목이며 이 순서대로 꼭 수업계획을 짜야 합니다. 여기서 순서란 B과목은 C과목을 이수한 후에 들어야 하고, A과목은 C와 B를 이수한 후에 들어야 한다는 것입니다. 현수가 C, B, D, A, G, E로 수업계획을 짜면 제대로 된 설계이지만 C, G, E, A, D, B 순서로 짰다면 잘 못 설계된 수업계획이 됩니다. 수업계획은 그 순서대로 앞에 수업이 이수되면 다음 수업을 시작하다는 것으로 해석합니다. 수업계획서상의 각 과목은 무조건 이수된다고 가정합니다. 필수과목순서가 주어지면 현수가 짠 N개의 수업설계가 잘된 것이면 “YES”, 잘못된 것이면 “NO”를 출력하는 프로그램을 작성하세요. . from collections import deque req = input() dq = deque(req) n = int(input()) for i in range(n): plan = input() for x in plan: if x in dq: if x != dq.popleft(): print(f&#39;{i}번째 NO&#39;) break else: if len(dq) == 0: print(f&#39;{i}번째 NO&#39;) else: print(f&#39;{i}번째 YES&#39;) . . 5.8. Finding words (hash) . 시를 쓰기 전에 시에 쓰일 단어를 미리 노트에 적어둡니다. 이번에는 N개의 단어를 노트에 적었는데 시에 쓰지 않은 단어가 하나 있다고 합니다. 그 단어를 찾아 주세요. . n = int(input()) dic = {} for i in range(n): word = input() dic[word] = 0 for i in range(n-1): w = input() dic[w] += 1 for k, v in dic.items(): if v == 0: print(k) . . 5.9. Anagram . 두 문자열이 알파벳의 나열 순서를 다르지만 그 구성이 일치하면 두 단어는 Anagram이라고 합니다. 예를 들면 AbaAeCe 와 baeeACA 는 알파벳을 나열 순서는 다르지만 그 구성을 살펴보면 A(2), a(1), b(1), C(1), e(2)로 알파벳과 그 개수가 모두 일치합니다. 즉 어느 한 단어를 재 배열하면 상대편 단어가 될 수 있는 것을 아나그램이라 합니다. 길이가 같은 두 개의 단어가 주어지면 두 단어가 아나그램인지 판별하는 프로그램을 작성하세요. 아나그램 판별시 대소문자가 구분됩니다. . str1 = input() str2 = input() dic = {} dic.get(&#39;C&#39;,0) for i in str1: dic[i] = dic.get(i,0)+1 for i in str2: dic[i] -= 1 if any(x != 0 for x in dic.values()): print(&quot;NO&quot;) else: print(&quot;YES&quot;) . . 5.10. Min-heap . 최소힙은 완전이진트리로 구현된 자료구조입니다. 그 구성은 부모 노드값이 왼쪽자식과 오른쪽 자식노드의 값보다 작게 트리를 구성하는 것입니다. 그렇게 하면 트리의 루트(root)노드는 입력된 값들 중 가장 작은 값이 저장되어 있습니다. 예를 들어 5 3 2 1 4 6 7순으로 입력되면 최소힙 트리는 아래와 같이 구성됩니다. 최소힙 자료를 이용하여 다음과 같은 연산을 하는 프로그램을 작성하세요. 1) 자연수가 입력되면 최소힙에 입력한다. 2) 숫자 0 이 입력되면 최소힙에서 최솟값을 꺼내어 출력한다. (출력할 자료가 없으면 -1를 출력한다.) 3) -1이 입력되면 프로그램 종료한다. . import heapq as hq a = [] while True: n = int(input()) if n == -1: break if n == 0: if len(a) == 0: print(-1) else: print(hq.heappop(a)) else: hq.heappush(a, n) . . 5.11. Max-heap . 최대힙은 완전이진트리로 구현된 자료구조입니다. 그 구성은 부모 노드값이 왼쪽자식과 오른쪽 자식노드의 값보다 크게 트리를 구성하는 것입니다. 그렇게 하면 트리의 루트(root)노드는 입력된 값들 중 가장 큰 값이 저장되어 있습니다. 예를 들어 5 3 2 1 4 6 7순으로 입력되면 최대힙 트리는 아래와 같이 구성됩니다. . 최대힙 자료를 이용하여 다음과 같은 연산을 하는 프로그램을 작성하세요. 1) 자연수가 입력되면 최대힙에 입력한다. 2) 숫자 0 이 입력되면 최대힙에서 최댓값을 꺼내어 출력한다.(출력할 자료가 없으면 -1를 출력한다.) 3) -1이 입력되면 프로그램 종료한다. . import heapq as hq a = [] while True: n = int(input()) if n == -1: break if n == 0: if len(a) == 0: print(-1) else: print(-hq.heappop(a)) else: hq.heappush(a, -n) . .",
            "url": "https://nueees.github.io/techblog/algorithm/python/2020/12/05/stack-queue.html",
            "relUrl": "/algorithm/python/2020/12/05/stack-queue.html",
            "date": " • Dec 5, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Decision & Greedy algorithm",
            "content": "Python Algorithm Practice . . 4.1. Binary search . 임의의 N개의 숫자가 입력으로 주어집니다. N개의 수를 오름차순으로 정렬한 다음 N개의 수 중 한 개의 수인 M이 주어지면 이분검색으로 M이 정렬된 상태에서 몇 번째에 있는지 구하는 프로그램을 작성하세요. 단 중복값은 존재하지 않습니다. . n, m = map(int, input().split()) a = list(map(int, input().split())) a.sort() lt = 0 rt = n-1 while lt &lt;= rt: mid=(lt+rt)//2 if a[mid] == m: print(mid+1) break elif a[mid]&gt;m: rt = mid-1 else: lt = mid+1 . . 4.2. Cutting LAN cable . 한 학원은 자체적으로 K개의 랜선을 가지고 있다. 그러나 K개의 랜선은 길이가 제각각이다. 선생님은 랜선을 모두 N개의 같은 길이의 랜선으로 만들고 싶었기 때문에 K개의 랜선을 잘라서 만들어야 한다. 예를 들어 300cm 짜리 랜선에서 140cm 짜리 랜선을 두 개 잘라내면 20cm 은 버려야 한다. (이미 자른 랜선은 붙일 수 없다.) 편의를 위해 랜선을 자를때 손실되는 길이는 없다고 가정하며, 기존의 K개의 랜선으로 N개의 랜선을 만들 수 없는 경우는 없다고 가정하자. 그리고 자를 때는 항상 센티미터 단위로 정수 길이만큼 자른다고 가정하자. N개보다 많이 만드는 것도 N개를 만드는 것에 포함된다. 이때 만들 수 있는 최대 랜선의 길이를 구하는 프로그램을 작성하세요. . k, n = map(int,input().split()) line = [] for i in range(k): line.append(int(input())) line.sort() def count(len): cnt = 0 for x in line: cnt += (x//len) return cnt largest = line[k-1] lt = 1 rt = largest while lt &lt;= rt: mid = (lt+rt)//2 if count(mid) &gt;= n: res = mid lt = mid+1 else: rt = mid-1 print(res) . . 4.3. DVD recording . 가수의 라이브 동영상을 DVD로 만들어 판매하려 한다. DVD에는 총 N개의 곡이 들어가는데, DVD에 녹화할 때에는 라이브에서의 순서가 그대로 유지되어야 한다. 즉, 1번 노래와 5번 노래를 같은 DVD에 녹화하기 위해서는 1번과 5번 사이의 모든 노래도 같은 DVD에 녹화해야 한다. 또한 한 노래를 쪼개서 두 개의 DVD에 녹화하면 안된다. DVD 생산 최소화를 위해 M개가 한 세트인 DVD에 모든 동영상을 녹화하기로 하였다. 이 때 DVD의 크기(녹화 가능한 길이)를 최소로 하려고 한다. 그리고 M개의 DVD는 모두 같은 크기여야 제조원가가 적게 들기 때문에 꼭 같은 크기로 해야 한다. . 3개의 DVD용량이 17분짜리이면 (1, 2, 3, 4, 5) (6, 7), (8, 9) 이렇게 3개의 DVD로 녹음을 할 수 있다. 17분 용량보다 작은 용량으로는 3개의 DVD에 모든 영상을 녹화할 수 없다. . n, m = map(int, input().split()) music = list(map(int, input().split())) max_music = max(music) def count(capacity): cnt = 1 sum = 0 for x in music: if sum+x &gt; capacity: cnt += 1 sum = x else: sum += x return cnt lt=1 rt=sum(music) while lt &lt;= rt: mid = (lt+rt)//2 if mid &gt; max_music and count(mid) &lt;= m: res = mid rt = mid-1 else: lt = mid+1 print(res) . . 4.4. Deciding Stables size . N개의 마구간이 수직선상에 있습니다. 각 마구간은 x1, x2, x3, ……, xN의 좌표를 가지며, 마구간 간에 좌표가 중복되는 일은 없습니다. 각 마구간에는 한 마리의 말만 넣을 수 있고, 가장 가까운 두 말의 거리가 최대가 되게 말을 마구간에 배치하고 싶습니다. C마리의 말을 N개의 마구간에 배치했을 때 가장 가까운 두 말의 거리가 최대가 되는 그 최대값을 출력하는 프로그램을 작성하세요. . n, c = map(int,input().split()) stall = [] for _ in range(n): stall.append(int(input())) def count(len): cnt = 1 ep = stall[0] for i in range(1, n): if stall[i] - ep &gt;= len: cnt += 1 ep = stall[i] return cnt stall.sort() lt = 1 rt = stall[n-1] while lt &lt;= rt: mid = (lt+rt)//2 if count(mid) &gt;= c: res = mid lt = mid+1 else: rt = mid-1 print(res) . . 4.5. Allocating meeting room . 한 개의 회의실이 있는데 이를 사용하고자 하는 n개의 회의들에 대하여 회의실 사용표를 만들려고 한다. 각 회의에 대해 시작시간과 끝나는 시간이 주어져 있고, 각 회의가 겹치지 않게 하면서 회의실을 사용할 수 있는 최대수의 회의를 찾아라. 단, 회의는 한번 시작하면 중간에 중단될 수 없으며 한 회의가 끝나는 것과 동시에 다음 회의가 시작될 수 있다. . n = int(input()) meeting = [] for i in range(n): s, e = map(int,input().split()) meeting.append((s,e)) # meeting.sort() meeting.sort(key= lambda x : (x[1], x[0])) # order by end time, start time 순 last_time = 0 cnt = 0 for s, e in meeting: if last_time &lt;= s: # 다음 시작시간이 이전 끝나는 시간 보다 같거나 크면 회의실에서 회의 가능 last_time = e cnt += 1 print(cnt) . . 4.6. Selecting Korean wrestling . 현수는 씨름 감독입니다. 현수는 씨름 선수를 선발공고를 냈고, N명의 지원자가 지원을 했습니다. 현수는 각 지원자의 키와 몸무게 정보를 알고 있습니다. 현수는 씨름 선수 선발 원칙을 다음과 같이 정했습니다. “다른 모든 지원자와 일대일 비교하여 키와 몸무게 중 적어도 하나는 크거나, 무거운 지원자만 뽑기로 했습니다.” 만약 A라는 지원자보다 키도 크고 몸무게도 무거운 지원자가 존재한다면 A지원자는 탈락입니다. . n = int(input()) candidate = [] for i in range(n): h, w = map(int, input().split()) candidate.append((h,w)) candidate.sort(key=lambda x: (-x[0])) cnt = 0 largest = 0 for h, w in candidate: if w &gt; largest: largest = w cnt += 1 print(cnt) . . 4.7. Arranging warehouse storage . 창고에 상자가 가로방향으로 일렬로 쌓여 있습니다. |||||||| |:-:|:-:|:-:|:-:|:-:|:-:|:-:| | | | | | | | | | | |9| | | | | | | |8| | |8| | | | |7|7| |7| | |6| |6|6| |6|6| |5| |5|5| |5|5| |4| |4|4| |4|4| |3|3|3|3| |3|3| |2|2|2|2|2|2|2| |1|1|1|1|1|1|1| . 만약 가로의 길이가 7이라면 1열은 높이가 6으로 6개의 상자가 쌓여 있고, 2열은 3개의 상자, 3열은 9개의 상자가 쌓여 있으며 높이는 9라고 읽는다. 창고 높이 조정은 가장 높은 곳에 상자를 가장 낮은 곳으로 이동하는 것을 말한다. 가장 높은 곳이나 가장 낮은 곳이 여러곳이면 그 중 아무거나 선택하면 된다. 위에 그림을 1회 높이 조정을 하면 다음과 같아진다. |||||||| |:-:|:-:|:-:|:-:|:-:|:-:|:-:| | | | | | | | | | | |X| | | | | | | |8| | |8| | | | |7|7| |7| | |6| |6|6| |6|6| |5| |5|5| |5|5| |4| |4|4| |4|4| |3|3|3|3|3|3|3| |2|2|2|2|2|2|2| |1|1|1|1|1|1|1| . 창고의 가로 길이(l)와 각 열의 상자 높이(a)가 주어집니다. m회의 높이 조정을 한 후 가장 높은 곳과 가장 낮은 곳의 차이를 출력하는 프로그램을 작성하세요. . l = int(input()) a = list(map(int,input().split())) m = int(input()) a.sort() for _ in range(m): a[0] += 1 a[l-1] -= 1 a.sort() print(a[l-1]-a[0]) . . 4.8. Calculating a number of minimum lifeboat in sinking Titanic . 침몰 중인 유람선에는 N명의 승객이 타고 있습니다. 구명보트를 타고 탈출해야 하는데 타이타닉에 있는 구명보트는 2명 이하로만 탈 수 있으며, 보트 한 개에 탈 수 있는 총 무게도 M kg 이하로 제한되어 있습니다. N명의 승객 몸무게가 주어졌을 때 승객 모두가 탈출하기 위한 구명보트의 최소 개수를 출력하는 프로그램을 작성하세요. . n, limit = map(int, input().split()) a = list(map(int,input().split())) a.sort() cnt = 0 while a: if len(p)==1: cnt += 1 break if a[0] + a[-1] &gt; limit: a.pop() cnt += 1 else: a.pop(0) a.pop() cnt += 1 print(cnt) . . 4.9. Incresing subsequence . 1부터 N까지의 모든 자연수로 구성된 길이 N의 수열이 주어집니다. 이 수열의 왼쪽 맨 끝 숫자 또는 오른쪽 맨 끝 숫자 중 하나를 가져와 나열하여 가장 긴 증가수열을 만듭니다. 이때 수열에서 가져온 숫자(왼쪽 맨 끝 또는 오른쪽 맨 끝)는 그 수열에서 제거됩니다. 예를 들어 2 4 5 1 3 이 주어지면 만들 수 있는 가장 긴 증가수열의 길이는 4입니다. 맨 처음 왼쪽 끝에서 2를 가져오고, 그 다음 오른쪽 끝에서 3을 가져오고, 왼쪽 끝에서 4, 왼쪽 끝에서 5를 가져와 2 3 4 5 증가수열을 만들 수 있습니다. . n = int(input()) a = list(map(int,input().split())) res = &quot;&quot; tmp = [] last = 0 l, r = 0, n-1 while l &lt;= r: if a[l] &gt; last: tmp.append((a[l], &quot;L&quot;)) if a[r] &gt; last: tmp.append((a[r], &quot;R&quot;)) tmp.sort() if len(tmp)==0: break else: res += tmp[0][1] last = tmp[0][0] if res[-1] == &#39;L&#39;: l += 1 else: r -= 1 tmp.clear() print(len(res)) print(res) . . 4.10. Reverse sequence . 1부터 n까지의 수를 한 번씩만 사용하여 이루어진 수열이 있을 때, 1부터 n까지 각각의 수 앞에 놓여 있는 자신보다 큰 수들의 개수를 수열로 표현한 것을 역수열이라 한다. 예를 들어 다음과 같은 수열의 경우 4 8 6 2 5 1 3 7 1앞에 놓인 1보다 큰 수는 4, 8, 6, 2, 5. 이렇게 5개이고, 2앞에 놓인 2보다 큰 수는 4, 8, 6. 이렇게 3개, 3앞에 놓인 3보다 큰 수는 4, 8, 6, 5 이렇게 4개… 따라서 4 8 6 2 5 1 3 7의 역수열은 5 3 4 0 2 1 1 0 이 된다. n과 1부터 n까지의 수를 사용하여 이루어진 수열의 역수열이 주어졌을 때, 원래의 수열을 출력하는 프로그램을 작성하세요. . n = int(input()) a = list(map(int,input().split())) ori = [0]*n for i in range(n): cnt = 0 for j in range(n): if ori[j] == 0 and cnt == a[i]: ori[j] = i+1 break if ori[j] == 0: cnt += 1 print(ori) . .",
            "url": "https://nueees.github.io/techblog/algorithm/python/2020/12/04/decision-greedy.html",
            "relUrl": "/algorithm/python/2020/12/04/decision-greedy.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Search & Simulation algorithm",
            "content": "Python Algorithm Practice . . 3.1. Sequence of characters . N개의 문자열 데이터를 입력받아 앞에서 읽을 때나 뒤에서 읽을 때나 같은 경우(회문 문자열)이면 YES를 출력하고 회문 문자열이 아니면 NO를 출력하는 프로그램을 작성한다. 단 회문을 검사할 때 대소문자를 구분하지 않습니다. . n = int(input()) for i in range(n): s = input() s = s.lower() size = len(s) for j in range(size//2): if s[j] != s[-1-j]: print(f&quot;#{i+1} NO&quot;) break else: print(f&quot;#{i+1} YES&quot;) # s[::-1]으로도 구현 가능함. . . 3.2. Extract only number . 문자와 숫자가 섞여있는 문자열이 주어지면 그 중 숫자만 추출하여 그 순서대로 자연수를 만듭니다. 만들어진 자연수와 그 자연수의 약수 개수를 출력합니다. 만약 “t0e0a1c2h0er”에서 숫자만 추출하면 0, 0, 1, 2, 0이고 이것을 자연수를 만들면 120이 됩니다. 즉 첫자리 0은 자연수화 할 때 무시합니다. 출력은 120를 출력하고, 다음 줄에 120의 약수의 개수를 출력하면 됩니다. 추출하여 만들어지는 자연수는 100,000,000을 넘지 않습니다. . s = input() res = 0 for i in s: if i.isdigit(): res = res*10 + int(i) cnt = 0 for i in range(1,res+1): if res%i==0: cnt += 1 print(cnt) . . 3.3. Reverse cards . 1부터 20까지 숫자가 하나씩 쓰인 20장의 카드가 아래 그림과 같이 오름차순으로 한 줄로 놓여있다. 각 카드의 위치는 카드 위에 적힌 숫자와 같이 1부터 20까지로 나타낸다. . |1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20| |:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:|:–:| ||||||||||||||||||||| 이제 여러분은 다음과 같은 규칙으로 카드의 위치를 바꾼다: 구간 [a, b] (단, 1 ≤ a ≤ b ≤ 20)가 주어지면 위치 a부터 위치 b까지의 카드를 현재의 역순으로 놓는다. 예를 들어, 현재 카드가 놓인 순서가 위의 그림과 같고 구간이 [5, 10]으로 주어진다면, 위치 5부터 위치 10까지의 카드 5, 6, 7, 8, 9, 10을 역순으로 하여 10, 9, 8, 7, 6, 5로 놓는다. 이제 전체 카드가 놓인 순서는 아래 그림과 같다. . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 . 1 | 2 | 3 | 4 | 10 | 9 | 8 | 7 | 6 | 5 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | . 이 상태에서 구간 [9, 13]이 다시 주어진다면, 위치 9부터 위치 13까지의 카드 6, 5, 11, 12, 13을 역순으로 하여 13, 12, 11, 5, 6으로 놓는다. 이제 전체 카드가 놓인 순서는 아래 그림과 같다. . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 . 1 | 2 | 3 | 4 | 10 | 9 | 8 | 7 | 6 | 5 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | . 1 | 2 | 3 | 4 | 10 | 9 | 8 | 7 | 13 | 12 | 11 | 5 | 6 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | . 오름차순으로 한 줄로 놓여있는 20장의 카드에 대해 10개의 구간이 주어지면, 주어진 구간의 순서대로 위의 규칙에 따라 순서를 뒤집는 작업을 연속해서 처리한 뒤 마지막 카드들의 배치를 구하는 프로그램을 작성하세요. . a = list(range(0,21)) for _ in range(10): s, e = map(int, input().split()) for i in range((e-s+1)//2): a[s+i], a[e-i] = a[e-i], a[s+i] a.pop(0) print(a) ## 또는 s,e 받고 난 후 역배열 바꿔서 해도 됨 아래 참조 tmp = a[s-1:e] a[s-1:e] = tmp[::-1] . . 3.4. Merge lists . 오름차순으로 정렬이 된 두 리스트가 주어지면 두 리스트를 오름차순으로 합쳐 출력하는 프로그램을 작성하세요. . # sort로 바로 구할 수도 있지만 포인터 연습으로 구현 n1 = int(input()) l1 = list(map(int, input().split())) n2 = int(input()) l2 = list(map(int, input().split())) p1=p2=0 res = [] while p1&lt;n1 and p2&lt;n2: if l1[p1] &lt;= l2[p2]: res.append(l1[p1]) p1 += 1 else: res.append(l2[p2]) p2 += 1 if p1&lt;n1: res = res + l1[p1:] if p2&lt;n2: res = res + l2[p2:] for i in res: print(i, end=&#39; &#39;) . . 3.5. Sum of Numbers . N개의 수로 된 수열 $A[1], A[2], …, A[N]$ 이 있다. 이 수열의 i번째 수부터 j번째 수까지의 합 $A[i]+A[i+1]+…+A[j-1]+A[j]$가 M이 되는 경우의 수를 구하는 프로그램을 작성하시오. . n, m = map(int, input().split()) a = list(map(int,input().split())) cnt = 0 for i in range(n): for j in range(i+1,n): if m==sum(a[i:j+1]): cnt += 1 print(cnt) ## 다른 방법 lt = 0 rt = 1 tot = a[0] cnt = 0 while True: if tot &lt; m: if rt &lt; n: tot += a[rt] else: break elif tot == m: cnt += 1 tot -= a[lt] lt += 1 else: tot -= a[lt] lt += 1 . . 3.6. Sum of Matrix . 5*5 격자판에 아래와 같이 숫자가 적혀있습니다. .           . 10 | 13 | 10 | 12 | 15 | . 12 | 39 | 30 | 28 | 11 | . 11 | 25 | 50 | 53 | 15 | . 19 | 27 | 29 | 37 | 27 | . 19 | 13 | 30 | 13 | 19 | . N*N의 격자판이 주어지면 각 행의 합, 각 열의 합, 두 대각선의 합 중 가 장 큰 합을 출력합니다. . n = int(input()) mat = [list(map(int, input().split())) for _ in range(n)] max = 0 mrow = 0 mcol = 0 for i in range(n): mrow = mcol = 0 for j in range(n): mrow += mat[i][j] mcol += mat[j][i] if mrow &gt; max: max = mrow if mcol &gt; max: max = mcol rdiag = 0 ldiag = 0 for i in range(n): # diagonal rdiag += mat[i][i] # right diagonal ldiag += mat[i][n-i-1] # left diagonal if rdiag &gt; max: max = rdiag if ldiag &gt; max: max = ldiag print(max) . . 3.7. Diamond . 현수의 농장은 N*N 격자판으로 이루어져 있으며, 각 격자안에는 한 그루의 사과나무가 심어져있다. N의 크기는 항상 홀수이다. 가을이 되어 사과를 수확해야 하는데 현수는 격자판안의 사과를 수확할 때 다이아몬드 모양의 격자판만 수확하고 나머지 격자안의 사과는 새들을 위해서 남겨놓는다. 만약 N이 5이면 아래 그림과 같이 진한 부분의 사과를 수확한다. .           . 10 | 13 | 10 | 12 | 15 | . 12 | 39 | 30 | 28 | 11 | . 11 | 25 | 50 | 53 | 15 | . 19 | 27 | 29 | 37 | 27 | . 19 | 13 | 30 | 13 | 19 | . 현수과 수확하는 사과의 총 개수를 출력하세요. . n = int(input()) mat = [list(map(int, input().split())) for _ in range(n)] res = 0 s = e = n//2 for i in range(n): for j in range(s, e+1): res += mat[i][j] if i &lt; n//2: s -= 1 e += 1 else: s += 1 e -= 1 print(res) . . 3.8. Sandglass . 현수는 곳감을 만들기 위해 감을 깍아 마당에 말리고 있습니다. 현수의 마당은 N*N 격자판으로 이루어져 있으며, 현수는 각 격자단위로 말리는 감의 수를 정합니다. 그런데 해의 위치에 따라 특정위치의 감은 잘 마르지 않습니다. 그래서 현수는 격자의 행을 기준으로 왼쪽, 또는 오른쪽으로 회전시켜 위치를 변경해 모든 감이 잘 마르게 합니다. 만약 회전명령 정보가 2 0 3이면 2번째 행을 왼쪽으로 3만큼 아래 그림처럼 회전시키는 명령입니다. .           . 10 | 13 | 10 | 12 | 15 | . 12 | 39 | 30 | 28 | 11 | . 11 | 25 | 50 | 53 | 15 | . 19 | 27 | 29 | 37 | 27 | . 19 | 13 | 30 | 13 | 19 | .           . 10 | 13 | 10 | 12 | 15 | . 23 | 11 | 12 | 39 | 30 | . 11 | 25 | 50 | 53 | 15 | . 19 | 27 | 29 | 37 | 27 | . 19 | 13 | 30 | 13 | 19 | . 첫 번째 수는 행번호, 두 번째 수는 방향인데 0이면 왼쪽, 1이면 오른쪽이고, 세 번째 수는 회전하는 격자의 수입니다. M개의 회전명령을 실행하고 난 후 아래와 같이 마당의 모래시계 모양의 영역에는 감 이 총 몇 개가 있는지 출력하는 프로그램을 작성하세요. .           . 10 | 13 | 10 | 12 | 15 | . 23 | 11 | 12 | 39 | 30 | . 11 | 25 | 50 | 53 | 15 | . 19 | 27 | 29 | 37 | 27 | . 19 | 13 | 30 | 13 | 19 | . n = int(input()) mat = [list(map(int,input().split())) for _ in range(n)] m = int(input()) for _ in range(m): row, tw, k = map(int,input().split()) if tw == 0: # toward left mat[row-1] = mat[row-1][k:]+mat[row-1][:k] else: # toword right mat[row-1] = mat[row-1][-k:]+mat[row-1][:-k] s = 0 e = n res = 0 for i in range(n): res += sum(mat[i][s:e]) # print(mat[i][s:e]) if i &lt; n//2: s += 1 e -= 1 else: s -= 1 e += 1 print(res) . . 3.9. Hill . 지도 정보가 N*N 격자판에 주어집니다. 각 격자에는 그 지역의 높이가 쓰여있습니다. 각 격자판의 숫자 중 자신의 상하좌우 숫자보다 큰 숫자는 봉우리 지역입니다. 봉우리 지역이 몇 개 있는 지 알아내는 프로그램을 작성하세요. 격자의 가장자리는 0으로 초기화 되었다고 가정한다. 만약 N=5 이고, 격자판의 숫자가 다음과 같다면 봉우리의 개수는 10개입니다. .               . 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 0 | 5 | 3 | 7 | 2 | 3 | 0 | . 0 | 3 | 7 | 1 | 6 | 1 | 0 | . 0 | 7 | 2 | 5 | 3 | 4 | 0 | . 0 | 4 | 3 | 6 | 4 | 1 | 0 | . 0 | 8 | 7 | 3 | 5 | 2 | 0 | . 0 | 0 | 0 | 0 | 0 | 0 | 0 | . n = int(input()) mat = [list(map(int,input().split())) for _ in range(n)] mat.insert(0, [0]*n) # upper padding mat.append([0]*n) # lower padding for i in mat: i.insert(0, 0) # left padding i.append(0) # right padding dx = [-1, 0, 1, 0] # direction x dy = [0, 1, 0, -1] # direction y cnt = 0 for i in range(1, n+1): for j in range(1, n+1): if all(mat[i][j] &gt; mat[i+dx[k]][j+dy[k]] for k in range(4)): cnt += 1 print(cnt) . . 3.10. Sudoku . 스도쿠는 매우 간단한 숫자 퍼즐이다. 9×9 크기의 보드가 있을 때, 각 행과 각 열, 그리고 9개의 3×3 크기의 보드에 1부터 9까지의 숫자가 중복 없이 나타나도록 보드를 채우면 된다. 예를 들어 다음을 보자. .                   . 1 | 4 | 3 | 6 | 2 | 8 | 5 | 7 | 9 | . 5 | 7 | 2 | 1 | 3 | 9 | 4 | 6 | 8 | . 9 | 8 | 6 | 7 | 5 | 4 | 2 | 3 | 1 | . 3 | 9 | 1 | 5 | 4 | 2 | 7 | 8 | 6 | . 4 | 6 | 8 | 9 | 1 | 7 | 3 | 5 | 2 | . 7 | 2 | 5 | 8 | 6 | 3 | 9 | 1 | 4 | . 2 | 3 | 7 | 4 | 8 | 1 | 6 | 9 | 5 | . 6 | 1 | 9 | 2 | 7 | 5 | 8 | 4 | 3 | . 8 | 5 | 4 | 3 | 9 | 6 | 1 | 2 | 7 | . 위 그림은 스도쿠를 정확하게 푼 경우이다. 각 행에 1부터 9까지의 숫자가 중복 없이 나오고, 각 열에 1부터 9까지의 숫자가 중복 없이 나오고, 각 3×3짜리 사각형(9개이며, 예시로 위에서 굵은 숫자)에 1부터 9까지의 숫자가 중복 없이 나오기 때문이다. 완성된 9×9 크기의 수도쿠가 주어지면 정확하게 풀었으면 “YES”, 잘 못 풀었으면 “NO”를 출력하는 프로그램을 작성하세요. . mat = [list(map(int,input().split())) for _ in range(9)] def check(mat): for i in range(9): chk1 = [0]*10 # row check chk2 = [0]*10 # col check for j in range(9): chk1[mat[i][j]] = 1 chk2[mat[j][i]] = 1 if sum(chk1) != 9 or sum(chk2) != 9: return False for i in range(3): # outer for j in range(3): chk3 = [0]*10 # 3x3 group check for i2 in range(3): # inner for j2 in range(3): chk3[mat[i*3+i2][j*3+j2]] = 1 if sum(chk3) != 9: return False return True if check(mat): print(&quot;YES&quot;) else: print(&quot;NO&quot;) . . 3.11. Mirrored String in Matrix . 1부터 9까지의 자연수로 채워진 7*7 격자판이 주어지면 격자판 위에서 가로방향 또는 세로방향으로 길이 5자리 회문수가 몇 개 있는지 구하는 프로그램을 작성하세요. 회문수란 121과 같이 앞에서부터 읽으나 뒤에서부터 읽으나 같은 수를 말합니다. . . 빨간색처럼 구부러진 경우(87178)는 회문수로 간주하지 않습니다. . sys.stdin=open(&quot;input.txt&quot;, &quot;r&quot;) mat = [list(map(int, input().split())) for _ in range(7)] cnt=0 for i in range(3): for j in range(7): tmp = mat[j][i:i+5] if tmp == tmp[::-1]: cnt+=1 for k in range(2): if mat[i+k][j] != mat[i+5-k-1][j]: break else: cnt+=1 print(cnt) . .",
            "url": "https://nueees.github.io/techblog/algorithm/python/2020/12/03/search.html",
            "relUrl": "/algorithm/python/2020/12/03/search.html",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Kth number",
            "content": "Python Algorithm Practice . . 2.1. Nth Prime Number . 두 개의 자연수 N과 K가 주어졌을 때, N의 약수들 중 K번째로 작은 수를 출력하는 프로그램을 작성하세요. . n, k = map(int, input().split()) print(n,k) cnt = 0 for i in range(1,n+1): if n%i==0: cnt+=1 if cnt==k: print(i) break else: print(-1) . . 2.2. Nth Number . N개의 숫자로 이루어진 숫자열이 주어지면 해당 숫자열중에서 s번째부터 e번째까지의 수를 오름차순 정렬했을 때 k번째로 나타나는 숫자를 출력하는 프로그램을 작성하세요. . T = int(input()) for i in range(T): n, s, e, k = map(int, input().split()) a = list(map(int, input().split())) # print(n,s,e,k) cnt = 0 a = a[s-1:e] a.sort() print(a[k-1]) . . 2.3. Nth Biggest Number . 1부터 100사이의 자연수가 적힌 N장의 카드를 가지고 있습니다. 같은 숫자의 카드가 여러장 있을 수 있습니다. 이 중 3장을 뽑아 각 카드에 적힌 수를 합한 값을 기록하려고 합니다. 3장을 뽑을 수 있는 모든 경우를 기록합니다. 기록한 값 중 K번째로 큰 수를 출력하는 프로그램을 작성하세요. . n, k = map(int, input().split()) a = list(map(int, input().split())) myset = set() # to remove duplication for i in range(n): for j in range(i+1, n): for l in range(j+1): myset.add(a[i]+a[j]+a[l]) res = list(myset) res.sort(reverse=True) print(res[k-1]) . . 2.4. Closest Representative value . N명의 학생의 수학점수가 주어집니다. N명의 학생들의 평균(소수 첫째자리 반올림)을 구하고, N명의 학생 중 평균에 가장 가까운 학생은 몇 번째 학생인지 출력하는 프로그램을 작성하세요. 평균과 가장 가까운 점수가 여러 개일 경우 먼저 점수가 높은 학생의 번호를 답으로 하고, 높은 점수를 가진 학생이 여러 명일 경우 그 중 학생번호가 빠른 학생의 번호를 답으로 합니다. . n = int(input()) a = list(map(int, input().split())) avg = round(sum(a)/n) min = 2**10000 for idx, x in enumerate(a): tmp = abs(x-avg) if tmp &lt; min: min = tmp score = x res = idx+1 elif tmp == min: if x &gt; score: score = x res = idx+1 print(avg, res) . . 2.5. Regular Polyhedron . 두 개의 정 N면체와 정 M면체의 두 개의 주사위를 던져서 나올 수 있는 눈의 합 중 가장 확률이 높은 숫자를 출력하는 프로그램을 작성하세요. 정답이 여러 개일 경우 오름차순으로 출력합니다. . n, m = map(int, input().split()) cnt = [0]*(n+m+1) cnt for i in range(1, n+1): for j in range(1, m+1): cnt[i+j] += 1 cnt max = -1 for i in range(1,len(cnt)): if cnt[i]&gt;max: max=cnt[i] for i in range(1,len(cnt)): if cnt[i]==max: print(i, end=&#39; &#39;) . . 2.6. Sum the Digits of a Number . N개의 자연수가 입력되면 각 자연수의 자릿수의 합을 구하고, 그 합이 최대인 자연수를 출력하는 프로그램을 작성하세요. . n = int(input()) a = list(map(int, input().split())) def digit_sum(x): sum = 0 while x&gt;0: sum += x%10 x = x//10 return sum max = 0 for x in a: tot = digit_sum(x) if tot &gt; max: max = tot res = x print(res) . . 2.7. Prime Number . 자연수 N이 입력되면 1부터 N까지의 소수의 개수를 출력하는 프로그램을 작성하세요. 예를 들어 20이 입력되면 1부터 20까지의 소수는 2, 3, 5, 7, 11, 13, 17, 19로 총 8개입니다. (제한시간은 1초) . n = int(input()) plist = [0]*(n+1) cnt = 0 for i in range(2,n+1): if plist[i] == 0: cnt += 1 for j in range(i,n+1,i): plist[j] = 1 print(cnt) . . 2.8. Prime with digits reversed . N개의 자연수가 입력되면 각 자연수를 뒤집은 후 그 뒤집은 수가 소수이면 그 수를 출력하는 프로그램을 작성하세요. 예를 들어 32를 뒤집으면 23이고, 23은 소수이다. 그러면 23을 출력한다. 단 910를 뒤집으면 19로 숫자화 해야 한다. 첫 자리부터의 연속된 0은 무시한다. . n = int(input()) a = list(map(int, input().split())) def reverse(x): res = 0 while x&gt;0: t = x%10 res = res*10+t x = x//10 return res def isPrime(x): if x==1: return False for i in range(2,x//2+1): if x%i==0: return False else: return True for x in a: tmp = reverse(x) if isPrime(tmp): print(tmp, end=&#39; &#39;) . . 2.9. Dice game . 1에서부터 6까지의 눈을 가진 3개의 주사위를 던져서 다음과 같은 규칙에 따라 상금을 받는 게임이 있다. 규칙(1) 같은 눈이 3개가 나오면 10,000원+(같은 눈)*1,000원의 상금을 받게 된다. 규칙(2) 같은 눈이 2개만 나오는 경우에는 1,000원+(같은 눈)*100원의 상금을 받게 된다. 규칙(3) 모두 다른 눈이 나오는 경우에는 (그 중 가장 큰 눈)*100원의 상금을 받게 된다. 예를 들어, 3개의 눈 3, 3, 6이 주어지면 상금은 1,000+3*100으로 계산되어 1,300원을 받게 된다. 또 3개의 눈이 2, 2, 2로 주어지면 10,000+2*1,000 으로 계산되어 12,000원을 받게 된다. 3개의 눈이 6, 2, 5로 주어지면 그 중 가장 큰 값이 6이므로 6*100으로 계산되어 600원을 상금으로 받게 된다. N 명이 주사위 게임에 참여하였을 때, 가장 많은 상금을 받은 사람의 상금을 출력하는 프로그램을 작성하세요. . n = int(input()) sum = 0 for i in range(n): tmp = input().split() tmp.sort(reverse=True) a,b,c = map(int, tmp) if a==b and a==c: sum += 10000 + (a * 1000) elif a==b: sum += 1000 + (a * 100) elif b==c: sum += 1000 + (b * 100) else: sum += (a * 100) print(sum) . . 2.10. Calculate a score . OX 문제는 맞거나 틀린 두 경우의 답을 가지는 문제를 말한다. 여러 개의 OX 문제로 만들어진 시험에서 연속적으로 답을 맞히는 경우에는 가산점을 주기 위해서 다음과 같이 점수 계산을 하기로 하였다. 1번 문제가 맞는 경우에는 1점으로 계산한다. 앞의 문제에 대해서는 답을 틀리다가 답이 맞는 처음 문제는 1점으로 계산한다. 또한, 연속으로 문제의 답이 맞는 경우에서 두 번째 문제는 2점, 세 번째 문제는 3점, …, K번째 문제는 K점으로 계산한다. 틀린 문제는 0점으로 계산한다. 예를 들어, 아래와 같이 10 개의 OX 문제에서 답이 맞은 문제의 경우에는 1로 표시하고, 틀린 경우에는 0으로 표시하였을 때, 점수 계산은 아래 표와 같이 계산되어, 총 점수는 1+1+2+3+1+2=10 점이다. . 채점 1 0 1 1 1 0 0 1 1 0 . 점수 | 1 | 0 | 1 | 2 | 3 | 0 | 0 | 1 | 2 | 0 | . 시험문제의 채점 결과가 주어졌을 때, 총 점수를 계산하는 프로그램을 작성하세요. . n = int(input()) a = list(map(int, input().split())) sum = 0 cnt = 0 for x in a: if x == 1: cnt += 1 sum += cnt else: cnt = 0 print(sum) . .",
            "url": "https://nueees.github.io/techblog/algorithm/python/2020/12/02/kth-number.html",
            "relUrl": "/algorithm/python/2020/12/02/kth-number.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Basic Sort & Binary Search",
            "content": "Python Algorithm Practice . . import random arr = random.sample(range(1,11),10) size = len(arr) k = 1 # search 1st . 1.1. Selection sort . for i in range(0,size-1): min_idx = i for j in range(i+1, size): # print(i,j) if (arr[j] &lt; arr[min_idx]): min_idx = j arr[min_idx], arr[i] = arr[i], arr[min_idx] print(arr[k]) . . 1.2. Bubble sort . for end in range(size-1, 0, -1): for i in range(end): # print(end, i) if arr[i] &gt; arr[i+1]: arr[i], arr[i+1] = arr[i+1], arr[i] print(arr[k]) . . 1.3. Insertion sort . for end in range(size): for i in range(end, 0, -1): # print(end, i) if arr[i-1] &gt; arr[i]: arr[i-1], arr[i] = arr[i], arr[i-1] print(arr[k]) . . 1.4. Quick sort . def quick_sort(arr): if len(arr) &lt;= 1: return arr pivot = arr[0] lesser_arr, equal_arr, greater_arr = [], [], [] for i in arr: if i &lt; pivot: lesser_arr.append(i) elif i &gt; pivot: greater_arr.append(i) else: equal_arr.append(i) print(lesser_arr, equal_arr, greater_arr) return quick_sort(lesser_arr) + equal_arr + quick_sort(greater_arr) quick_sort(arr) . . 1.5. Merge sort . def merge_sort(arr): if len(arr) &lt; 2: return arr mid = len(arr) // 2 low_arr = merge_sort(arr[:mid]) high_arr = merge_sort(arr[mid:]) merged_arr = [] l = h = 0 while l &lt; len(low_arr) and h &lt; len(high_arr): if low_arr[l] &lt; high_arr[h]: merged_arr.append(low_arr[l]) l +=1 else: merged_arr.append(high_arr[h]) h += 1 merged_arr += low_arr[l:] merged_arr += high_arr[h:] print(merged_arr) return merged_arr merge_sort(arr) . . 1.6. Heap sort . # subtree rooted at index i. def heapify(arr, n, i): largest = i # Initialize largest as root_i l = 2*i + 1 # left = 2*i + 1 r = 2*i + 2 # right = 2*i + 2 if l &lt; n and arr[i] &lt; arr[l]: largest = l if r &lt; n and arr[largest] &lt; arr[r]: largest = r if largest != i: arr[i], arr[largest] = arr[largest], arr[i] heapify(arr, n, largest) heap_size = len(arr) # 1) Build a maxheap. # Since last parent will be at ((heap_size//2)-1) we can start at that location. for root_i in range((heap_size//2)-1, -1, -1): heapify(arr, heap_size, root_i) # 2) One by one extract elements for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # 가장 큰 값을 마지막으로 이동 후 build a maxheap 반복 heapify(arr, i, 0) print(arr) . . 1.7. Depth First Search(DFS) for binary search . . Preorder Traversal . root -&gt; left -&gt; right 전위순회 출력 : 1 2 4 5 3 6 7 . def traversal(num): if num &gt; 7: return else: print(num, end=&#39; &#39;) traversal(num*2) traversal(num*2+1) traversal(1) . Inorder Traversal . left -&gt; root -&gt; right 중위순회 출력 : 4 2 5 1 6 3 7 . def traversal(num): if num &gt; 7: return else: traversal(num*2) print(num, end=&#39; &#39;) traversal(num*2+1) traversal(1) . Preorder Traversal . left -&gt; right -&gt; root 후위순회 출력 : 4 5 2 6 7 3 1 . def traversal(num): if num &gt; 7: return else: traversal(num*2) print(num, end=&#39; &#39;) traversal(num*2+1) traversal(1) . .",
            "url": "https://nueees.github.io/techblog/algorithm/python/2020/12/01/sort-search.html",
            "relUrl": "/algorithm/python/2020/12/01/sort-search.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "SQL1",
            "content": "HackerRank . . 1.1. Sub-String . input: . name, occupation Ashley Professor Samantha Actor Julia Doctor Britney Professor Maria Professor Meera Professor Priya Doctor Priyanka Professor Jennifer Actor Ketty Actor Belvet Professor Naomi Professor Jane Singer Jenny Singer Kristeen Singer Christeen Singer Eve Actor Aamina Doctor . output: . Samantha(D) Julia(A) Maria(A) Meera(S) . select name||&#39;(&#39;||substr(occupation,1,1)||&#39;)&#39; from OCCUPATIONS ; . . 1.2. Pivot . input: . name, occupation Ashley Professor Samantha Actor Julia Doctor Britney Professor Maria Professor Meera Professor Priya Doctor Priyanka Professor Jennifer Actor Ketty Actor Belvet Professor Naomi Professor Jane Singer Jenny Singer Kristeen Singer Christeen Singer Eve Actor Aamina Doctor . output: . doc,pro,sing,act Jenny, Ashley, Meera, Jane Samantha, Christeen, Priya, Julia NULL, Ketty, NULL, Maria . with wocc as ( select case when occupation = &#39;Doctor&#39; then name else Null end as doc ,case when occupation = &#39;Professor&#39; then name else Null end as pro ,case when occupation = &#39;Singer&#39; then name else Null end as sing ,case when occupation = &#39;Actor&#39; then name else Null end as act from OCCUPATIONS ) select * from ( select d.doc, p.pro, s.sing, a.act from (select row_number() over (order by sub.doc) col, sub.doc from wocc sub) d ,(select row_number() over (order by sub.pro) col, sub.pro from wocc sub) p ,(select row_number() over (order by sub.sing) col, sub.sing from wocc sub) s ,(select row_number() over (order by sub.act) col, sub.act from wocc sub) a where d.col = p.col and p.col = s.col and s.col = a.col ) where rownum &lt; (select cnt from ( select occupation, count(*) as cnt from OCCUPATIONS group by occupation order by 2 desc ) where rownum = 1)+1 ; . . 1.3. Hierarchical Table . input: . child, parent 1,2 3,2 6,8 9,8 2,5 8,5 5,null . input: . 1 Leaf 2 Inner 3 Leaf 5 Root 6 Leaf 8 Inner 9 Leaf . select N , case when CONNECT_BY_ISLEAF = 1 then &#39;Leaf&#39; when CONNECT_BY_ISLEAF = 0 and P is null then &#39;Root&#39; else &#39;Inner&#39; end from BST T1 start with P is null connect by prior N = P order by N ; . . 1.4. Join Table . input: . COMPANY c_code, founder C1, Monika C2, Samantha . LEAD_MANAGER l_m_code, c_code LM1, C1 LM2, C2 . SENIOR_MANAGER s_m_code, l_m_code, c_code SM1, LM1, C1 SM2, LM1, C1 SM3, LM2, C2 . MANAGER m_code, s_m_code, l_m_code, c_code M1, SM1, LM1, C1 M2, SM3, LM2, C2 M3, SM3, LM2, C2 . EMPLOYEE s_m_code, l_m_code, c_code E1, M1, SM1, LM1, C1 E2, M1, SM1, LM1, C1 E3, M2, SM3, LM2, C2 E4, M3, SM3, LM2, C2 . output: . C1 Monika 1 2 1 2 C2 Samantha 1 1 2 2 . select e.company_code, founder , count(distinct e.lead_manager_code) , count(distinct e.senior_manager_code) , count(distinct e.manager_code) , count(distinct e.employee_code ) from (select distinct company_code, founder from Company) C left outer join (select distinct lead_manager_code, company_code from Lead_Manager) LM on C.company_code = LM.company_code left outer join (select distinct senior_manager_code, lead_manager_code, company_code from Senior_Manager) SM on LM.lead_manager_code = SM.lead_manager_code left outer join (select distinct manager_code, senior_manager_code, lead_manager_code, company_code from Manager) M on SM.senior_manager_code = M.senior_manager_code left outer join (select distinct employee_code, manager_code, senior_manager_code, lead_manager_code, company_code from Employee) E on M.manager_code = E.manager_code group by e.company_code, founder order by 1 ; . . 1.5. Max, Min values . input: . DEF, ABC, PQRS, WXY . output: . ABC 3 PQRS 4 . with wstation as (select city , length(city) as lencity from station order by 2, 1 ) , w2station as (select city, lencity from (select row_number() over (partition by lencity order by city) as rnum , city , lencity from wstation ) where rnum &lt; 2 ) select * from w2station where (lencity = (select min(lencity) from wstation) or lencity = (select max(lencity) from wstation) ) ; . . 1.6. Join table2 . input: . STUDENTS ID, Name, Marks 1, Julia, 88 2, Samantha, 68 3, Maria, 99 4, Scarlet, 78 5, Ashley, 63 6, Jane, 81 . GRADES Grade, Min_Mark, Max_Mark 1, 0, 9 2, 10, 19 3, 20, 29 4, 30, 39 5, 40, 49 6, 50, 59 7, 60, 69 8, 70, 79 9, 80, 89 10, 90, 100 . output: . lower than 8, name is null . Maria 10 99 Jane 9 81 Julia 9 88 Scarlet 8 78 NULL 7 63 NULL 7 68 . select case when g.grade &gt; 7 then s.name else NULL end as name , g.grade , s.marks from STUDENTS s inner join GRADES g on s.marks between g.min_mark and g.max_mark order by g.grade desc, name ; . . 1.7. Group by &amp; Having . input: . HACKERS h_id, name 5580, Rose 8439, Angela 27205, Frank 52243, Patrick 52348, Lisa 57645, Kimberly 77726, Bonnie 83082, Michael 86870, Todd 90411, Joe . DIFFICULTY d_level, score 1, 20 2, 30 3, 40 4, 60 5, 80 6, 100 7, 120 . CHALLENGE c_id, h_id, d_level 4810, 77726, 4 21089, 27205, 1 36566, 5580, 7 66730, 52243, 6 71055, 52243, 2 . SUBMISSION s_id, h_id, c_id, score 68628, 77726, 36566, 30 65300, 77726, 21089, 10 40326, 52243, 36566, 77 8941, 27205, 4810, 4 83554, 77726, 66730, 30 … 97397, 90411, 66730, 100 84162, 83082, 4810, 40 97431, 90411, 71055, 30 . output: . 90411 Joe . with wsub as ( select h.hacker_id, h.name, count(distinct s.challenge_id) as cnt --, s.score from Submissions s left outer join Hackers h on s.hacker_id = h.hacker_id left outer join Challenges c on s.challenge_id = c.challenge_id left outer join Difficulty d on c.difficulty_level = d.difficulty_level where s.score = d.score and s.score is not null group by h.hacker_id, h.name having count(distinct s.challenge_id) &gt; 1 order by cnt desc, h.hacker_id ) select hacker_id, name from wsub; . . 1.8. grouping and ranking . input: . WANDS id, code, coins_needed, power 1, 4, 3588, 8 2, 3, 9365, 3 3, 3, 7187, 10 4, 3, 734, 8 5, 1, 6020, 8 6, 2, 6773, 3 7, 3, 9873, 10 8, 3, 7721, 8 9, 1, 1647, 8 10, 4, 504, 3 11, 2, 7587, 10 12, 5, 9897, 8 13, 3, 4651, 8 14, 2, 5408, 3 15, 2, 6018, 10 16, 4, 7710, 8 17, 2, 8798, 8 18, 2, 3312, 3 19, 4, 7651, 10 20, 5, 5689, 8 . WANDS_PROPERTY code, age, is_evil 1, 45, 0 2, 40, 0 3, 4, 1 4, 20, 0 5, 17, 0 . output: . 9 45 1647 10 12 17 9897 10 1 20 3688 8 15 40 6018 7 19 20 7651 6 11 40 7587 5 10 20 504 5 18 40 3312 3 20 17 5689 3 5 45 6020 2 14 40 5408 1 . with wsub as ( select rank() over (partition by w.power, wp.age order by w.coins_needed) as rank , w.power , wp.age , w.coins_needed , w.id from WANDS w inner join WANDS_PROPERTY wp on w.code = wp.code and wp.is_evil = 0 order by w.power desc, wp.age desc, w.coins_needed ) select id, age, coins_needed, power from wsub where rank = 1 ; . . 1.9. max인 공동 1위들은 전체 출력, 나머지는 공동 랭킹들은 제외 . select c.hacker_id, h.name, count(c.challenge_id) as cnt from hackers h inner join challenges c on h.hacker_id = c.hacker_id group by c.hacker_id, h.name having count(c.challenge_id) = (select max(tmp1.cnt) from (select count(hacker_id) as cnt from challenges group by hacker_id order by hacker_id) tmp1) or count(c.challenge_id) in ( select cnt1 from( select hacker_id, count(challenge_id) as cnt1 from challenges group by hacker_id) group by cnt1 having count(cnt1) = 1) order by count(c.challenge_id) desc, c.hacker_id ; .",
            "url": "https://nueees.github.io/techblog/sql/2020/11/01/SQL1.html",
            "relUrl": "/sql/2020/11/01/SQL1.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "통계분석3 (QDA, MDS, CA)",
            "content": ". 06_통계분석_3 . 판별분석 QDA(Quadratic Discriminant Analysis) . 선형판별분석(LDA), 이차판별 분석(QDA)은 확률론적 생성모형 . 가능도 y의 클래스값에 따른 x의 분포에 대한 정보를 먼저 알아낸 후, 베이즈 정리를 사용하여 주어진 x에 대한 y의 확률분포를 찾음 . | 주로 PCA와 같이 차원을 독립변수들을 축소할 때 사용 . | 클래스를 최대한 분리할 수 있는 축(boundary)를 찾아 분류 | 분산 대비 평균의 차이를 극대화 하는 boundary를 찾아 분류 . | 여기서는 이차 판별분석으로 “분류”하는 예시 | 모든 클래스k에 대하여 동일한 covariance matrix를 가정했던 LDA와 달리 QDA는 k클래스 마다 각각의 covariance matrix를 가지게 함 | k의 클래스 별 공분산 구조가 확연히 다를때 사용 | 설명변수가 많아질 수록 추정하는 모수도 많아지므로 샘플이 많이 필요 (+속도 저하) | 샘플이 적어서 분산을 줄이는 것이 중요할 경우 LDA를, 샘플이 많아서 분산에 대한 우려가 적을때, 혹은 공분산에 대한 가정이 비현실적으로 판단될 때에는 QDA를 사용 | . LDA (Linear Discriminant Analysis) . 기본 가정 각 클래스 동일한 공분산 구조를 갖고있음. | 각 클래스마다 정규분포를 따른다. | . | . from sklearn.discriminant_analysis import LinearDiscriminantAnalysis X = np.array([[-1, 1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y = np.array([1, 1, 1, 2, 2, 2]) clf = LinearDiscriminantAnalysis() clf.fit(X, y) print(clf.predict([[-0.8, -1]])) . [2] . QDA (Quadratic Discriminant Analysis) . 기본 가정 각 클래스는 정규분포를 가진다. (공분산 가정 X) | . | . from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis quad_clf = QuadraticDiscriminantAnalysis() quad_clf.fit(X, y) print(clf.predict([[-0.8, -1]])) . [2] . from sklearn.metrics import confusion_matrix y_pred = clf.predict(X) confusion_matrix(y, y_pred) y_pred2 = quad_clf.predict(X) confusion_matrix(y, y_pred2) . QuadraticDiscriminantAnalysis() . from sklearn.datasets import make_moons, make_circles, make_classification from matplotlib.colors import ListedColormap from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt %matplotlib inline h=0.2 names = [&quot;LDA&quot;, &quot;QDA&quot;] classifiers = [ LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis()] X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y) datasets = [make_moons(noise=0.3, random_state=0), make_circles(noise=0.2, factor=0.5, random_state=1), linearly_separable ] figure = plt.figure(figsize=(27, 9)) i = 1 # iterate over datasets for ds in datasets: # preprocess dataset, split into training and test part X, y = ds X = StandardScaler().fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4) x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # just plot the dataset first cm = plt.cm.RdBu cm_bright = ListedColormap([&#39;#FF0000&#39;, &#39;#0000FF&#39;]) ax = plt.subplot(len(datasets), len(classifiers) + 1, i) # Plot the training points ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright) # and testing points ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6) ax.set_xlim(xx.min(), xx.max()) ax.set_ylim(yy.min(), yy.max()) ax.set_xticks(()) ax.set_yticks(()) i += 1 # iterate over classifiers for name, clf in zip(names, classifiers): ax = plt.subplot(len(datasets), len(classifiers) + 1, i) clf.fit(X_train, y_train) score = clf.score(X_test, y_test) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, m_max]x[y_min, y_max]. if hasattr(clf, &quot;decision_function&quot;): Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) else: Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1] # Put the result into a color plot Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8) # Plot also the training points ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright) # and testing points ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6) ax.set_xlim(xx.min(), xx.max()) ax.set_ylim(yy.min(), yy.max()) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(name) ax.text(xx.max() - .3, yy.min() + .3, (&#39;%.2f&#39; % score).lstrip(&#39;0&#39;), size=15, horizontalalignment=&#39;right&#39;) i += 1 figure.subplots_adjust(left=.02, right=.98) plt.show() . . MultiDimensional Scaling (MDS) 다차원척도법 . 여러 차원 축소 기법 중 하나 . 종류 . 1) 계량적: PCoA (principle coordinates analysis) Classical multidimensional scaling으로, PCA (principle component analysis)와 매우 비슷하나, PCA: Euclidean 거리 사용하고 선형 관계 있으면 사용 (대부분 geological data) PCoA: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 있으면 사용 (biogeographic data) . 2) 비계량적: Non-MultiDimensional Scaling (NMDS) NMDS: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 없으면 사용 (어떤 지역 species 개체 수 많은 지) . 계량적(구간척도, 비율척도) . mds 객체 생설할 때 dissimilarity로 euclidean할지 precomputed 미리 계산된 걸로 할 지 정하고 mds.fit_transform 옵션에서 계산된 manhattan_distances 넣어주면 됨 . from sklearn.manifold import MDS from matplotlib import pyplot as plt import sklearn.datasets as dt import seaborn as sns import numpy as np from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances from matplotlib.offsetbox import OffsetImage, AnnotationBbox from sklearn.datasets import load_digits . X2 = np.array([[0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 0], [0, 1, 1]]) mds2 = MDS(random_state=0) X2_transform = mds2.fit_transform(X2) print(X2_transform) stress2 = mds2.stress_ print(stress2) . [[ 0.72521687 0.52943352] [ 0.61640884 -0.48411805] [-0.9113603 -0.47905115] [-0.2190564 0.71505714] [-0.21120901 -0.28132146]] 0.18216844548575456 . stress는 잘 피팅되었는지 검증용으로, 계산된 거리가 dissimilarity 차이를 보여주는데 . stress가 통상 0.2 이상이면 차원 높여야 함 . # plt.scatter([1,2,3,4], [10,30,20,40], s=[30,60,90,120], c=range(4), cmap=&#39;jet&#39;) # plt.colorbar() . colors = [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;, &#39;c&#39;, &#39;m&#39;] size = [64, 64, 64, 64, 64] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(121, projection=&#39;3d&#39;) plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors) plt.title(&#39;Original Points&#39;) ax = fig.add_subplot(122) plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors) plt.title(&#39;Embedding in 2D&#39;) fig.subplots_adjust(wspace=.4, hspace=0.5) plt.show() . . dist_manhattan = manhattan_distances(X2) mds3 = MDS(dissimilarity=&#39;precomputed&#39;, random_state=0) # Get the embeddings X2_transform_L1 = mds3.fit_transform(dist_manhattan) print(X2_transform_L1) print(mds3.stress_) . [[ 0.9847767 0.84738596] [ 0.81047787 -0.37601578] [-1.104849 -1.06040621] [-0.29311254 0.87364759] [-0.39729303 -0.28461157]] 0.4047164940033806 . fig = plt.figure(2, (15,6)) ax = fig.add_subplot(131, projection=&#39;3d&#39;) plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors) plt.title(&#39;Original Points&#39;) ax = fig.add_subplot(132) plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors) plt.title(&#39;Embedding in 2D&#39;) fig.subplots_adjust(wspace=.4, hspace=0.5) ax = fig.add_subplot(133) plt.scatter(X2_transform_L1[:,0], X2_transform_L1[:,1], s=size, c=colors) plt.title(&#39;Embedding in 2D L1&#39;) fig.subplots_adjust(wspace=.4, hspace=0.5) plt.show() . . # print(load_digits.__doc__) X, y = load_digits(return_X_y=True) X = X[:100] print(X.shape) mds = MDS(n_components=2) X_transformed = mds.fit_transform(X[:100]) print(X_transformed.shape) Y = y[:100] print(Y.size) # print(X_transformed[:5,0]) # print(X_transformed[:5,1]) print(mds.stress_) . (100, 64) (100, 2) 100 1133807.722583498 . colormap = np.array([&#39;b&#39;, &#39;g&#39;, &#39;r&#39;, &#39;c&#39;, &#39;m&#39;, &#39;y&#39;, &#39;k&#39;, &#39;w&#39;, &#39;w&#39;, &#39;w&#39;]) # colormap[Y] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(122) plt.scatter(X_transformed[:,0], X_transformed[:,1], c=colormap[Y]) plt.title(&#39;Embedding in 2D&#39;) plt.show() . . nmds = MDS(n_components=2, metric=False) nX_transformed2 = nmds.fit_transform(X) # print(nX_transformed2) nX_transformed2 *= np.sqrt((X ** 2).sum()) / np.sqrt((nX_transformed ** 2).sum()) # print(nX_transformed2) Y = y[:100] # print(Y.size) # print(nX_transformed[:5,0]) # print(nX_transformed[:5,1]) # print(nmds.stress_) colormap = np.array([&#39;b&#39;, &#39;g&#39;, &#39;r&#39;, &#39;c&#39;, &#39;m&#39;, &#39;y&#39;, &#39;k&#39;, &#39;w&#39;, &#39;w&#39;, &#39;w&#39;]) # colormap[Y] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(122) plt.scatter(nX_transformed2[:,0], nX_transformed2[:,1], c=colormap[Y]) plt.title(&#39;Embedding in non mds 2D&#39;) plt.show() . . non-metric MDS: 다차원척도법 비계량적(순서척도) . 1) 차이에 대해 수치화(quantified) 한 값을 얻기 힘들때, 순서만 알 수 있을 때 사용 예) 검정색-진회색-연회색-흰색… 중 가장 밝은 색, 빈도 수가 많은 데이터 . 2) 유클리디안 외 user-selected 거리 메트릭을 사용하고 싶을 때 (Jaccard,…) . Metric = False 옵션 주면 됨. . 3) 차원이 미리 결정되어야 하고, local minima(지역 최소값) 수렴 가능성이 있고, 시간 오래 걸리는 게 단점 . from sklearn.preprocessing import MinMaxScaler from mpl_toolkits import mplot3d df = pd.read_csv(&#39;../data/yeast-transcriptomics/SC_expression.csv&#39;) df = df.iloc[:,1:] # print(df.corr()) # print(df.T) df1 = df.T.values sc = MinMaxScaler() scaled = sc.fit_transform(df1) # print(scaled) mds = MDS(n_components=2) mds_scaled = mds.fit_transform(scaled) nmds = MDS(n_components=2, metric=False) nmds_scaled = nmds.fit_transform(scaled) plt.subplot(121) sns.scatterplot(x=mds_scaled[:,0],y=mds_scaled[:,1]) plt.legend(loc=&#39;best&#39;) plt.title(&#39;MDS&#39;) plt.subplot(122) sns.scatterplot(x=nmds_scaled[:,0],y=nmds_scaled[:,1]) plt.legend(loc=&#39;best&#39;) plt.title(&#39;nMDS&#39;) . No handles with labels found to put in legend. No handles with labels found to put in legend. Text(0.5, 1.0, &#39;MDS&#39;) . . CA (Correspondence Analysis) - 대응분석 . 차원 축소 기법 . 출처 . 1. Feature Extraction . supervised/unsupervised는 Y값의 분산 활용 유무로 나뉨 . Unsupervised: PCA, AutoEncoder . PCA는 X라는 독립변수들의 간의 선형결합으로 추출된 새로운 변수 추출 . Supervised: PLS(부분 최소제곱법) . PLS는 X라는 독립변수들의 선형결합과 Y라는 종속변수 이 2개 간의 공분산을 최대화 하는 새로운 변수 추출함 PCA의 일반적인 선형결합으로 추출된 새로운 변수가 설명하지 못하는 부분에 반복적으로 최소제곱법을 적용함 . 2. Feature Selection . Unsupervised: PCA loading . Supervised: Information Gain, Stepwise, Losso regression, Genetic algorithm . 대응분석 . PLS 군에 속하는 차원축소 방법 다변량 범주형 자료 대상으로 탐색적 분석 시 사용 분할표에서 행과 열의 범주들간의 대응 관계를 탐구하기 위해 2차원 공간상 관계로 시각화 . python은 big data, ML용으로 많이 사용되므로 상대적으로 데이터 양이 한정적인 대응분석은 잘 사용하지 않는 듯함 . Homogeneity (동질성): 각 행에 대해 열의 분포가 동일한가? Independence (독립성): 두 범주형 변수(X,Y)는 서로 독립인가? 이원분할표 -&gt; 단순대응분석 (독립성, 동질성 검정 -&gt; chi-squared test) 다원분할표 -&gt; 다중대응분석 (chi-squared test 신뢰도가 떨어지므로 ) . PCA vs CA . PCA: 많은 종의 생물에서 패턴 찾을 때 . CA: 생물 종간의 상대적인 패턴을 찾을 때 . 예) A = {50,20,10} B = {5,2,1} PCA의 경우 A와 B를 완전 다르다고 판단 (유클리디안 거리 측정법 사용) CA의 경우 A와 B가 비슷하다고 판단 . chi-squared test vs CA . chi-squared test: 두 범주형 변수간의 연관성을 찾을 떄, . CA: 두 변수가 가지고 있는 범주들 사이의 관계를 찾을 때, 두 개 이상의 범주 군 사이의 상관성을 분석하는 기법 . 예) X = df[[‘bill_length_mm’,’bill_depth_mm’]] Y = df[[‘flipper_length_mm’,’body_mass_g’]] chi-squared test로는 X와 Y의 독립성 검정을 실시하고, 두 group간에는 상당히 연관 관계가 있다고 판단까지 CA로는 X와 Y의 group내에서 어떤식으로 연관관계가 있는지 판단 . example코드출처 . import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np from sklearn.cross_decomposition import CCA #Canonical Correlation Analysis . link2data = &quot;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv&quot; df = pd.read_csv(link2data) df =df.dropna() print(df.head()) # pre-processing X = df[[&#39;bill_length_mm&#39;,&#39;bill_depth_mm&#39;]] # print(X.head()) X_mc = (X-X.mean())/(X.std()) print(X_mc.head()) Y = df[[&#39;flipper_length_mm&#39;,&#39;body_mass_g&#39;]] # print(Y.head()) Y_mc = (Y-Y.mean())/(Y.std()) print(Y_mc.head()) . species island bill_length_mm bill_depth_mm flipper_length_mm 0 Adelie Torgersen 39.1 18.7 181.0 1 Adelie Torgersen 39.5 17.4 186.0 2 Adelie Torgersen 40.3 18.0 195.0 4 Adelie Torgersen 36.7 19.3 193.0 5 Adelie Torgersen 39.3 20.6 190.0 body_mass_g sex 0 3750.0 MALE 1 3800.0 FEMALE 2 3250.0 FEMALE 4 3450.0 FEMALE 5 3650.0 MALE bill_length_mm bill_depth_mm 0 -0.894695 0.779559 1 -0.821552 0.119404 2 -0.675264 0.424091 4 -1.333559 1.084246 5 -0.858123 1.744400 flipper_length_mm body_mass_g 0 -1.424608 -0.567621 1 -1.067867 -0.505525 2 -0.425733 -1.188572 4 -0.568429 -0.940192 5 -0.782474 -0.691811 . ca = CCA() ca.fit(X_mc, Y_mc) X_c, Y_c = ca.transform(X_mc, Y_mc) print(X_c[:5]) print(Y_c[:5]) . [[-1.18625232 -0.01036701] [-0.70957262 -0.4560358 ] [-0.79073194 -0.13080943] [-1.7186634 -0.07362316] [-1.77229457 0.73624799]] [[-1.40879506 0.68286617] [-1.05385671 0.42987851] [-0.3935502 -0.83961988] [-0.5428878 -0.45857086] [-0.76354771 -0.01420367]] . cc_res = pd.DataFrame({&quot;CCX_1&quot;:X_c[:, 0], &quot;CCY_1&quot;:Y_c[:, 0], &quot;CCX_2&quot;:X_c[:, 1], &quot;CCY_2&quot;:Y_c[:, 1], &quot;Species&quot;:df.species.tolist(), &quot;Island&quot;:df.island.tolist(), &quot;sex&quot;:df.sex.tolist()}) print(cc_res.head()) print(np.corrcoef(X_c[:, 0], Y_c[:, 0])) # 첫번째 변수끼리의 공분산 print(np.corrcoef(X_c[:, 1], Y_c[:, 1])) # 두번째 변수끼리의 공분산 . CCX_1 CCY_1 CCX_2 CCY_2 Species Island sex 0 -1.186252 -1.408795 -0.010367 0.682866 Adelie Torgersen MALE 1 -0.709573 -1.053857 -0.456036 0.429879 Adelie Torgersen FEMALE 2 -0.790732 -0.393550 -0.130809 -0.839620 Adelie Torgersen FEMALE 3 -1.718663 -0.542888 -0.073623 -0.458571 Adelie Torgersen FEMALE 4 -1.772295 -0.763548 0.736248 -0.014204 Adelie Torgersen MALE [[1. 0.78763151] [0.78763151 1. ]] [[1. 0.08638695] [0.08638695 1. ]] . fig = plt.figure(figsize=(15,5)) sns.set_context(&quot;talk&quot;, font_scale=1.2) fig.add_subplot(121) sns.scatterplot(x=&quot;CCX_1&quot;, y=&quot;CCY_1&quot;, hue=&quot;Species&quot;, data=cc_res) plt.title(&#39;Comp. 1, corr = %.2f&#39; % np.corrcoef(X_c[:, 0], Y_c[:, 0])[0, 1]) fig.add_subplot(122) sns.scatterplot(x=&quot;CCX_2&quot;, y=&quot;CCY_2&quot;, hue=&quot;Species&quot;, data=cc_res) plt.title(&#39;Comp. 2, corr = %.2f&#39; % np.corrcoef(X_c[:, 1], Y_c[:, 1])[0, 1]) # fig.add_subplot(122) # sns.scatterplot(x=&quot;CCX_2&quot;, # y=&quot;CCY_2&quot;, # hue=&quot;sex&quot;, data=cc_res) # plt.title(&#39;Second Pair of Canonical Covariate, corr = %.2f&#39; % # np.corrcoef(X_c[:, 1], Y_c[:, 1])[0, 1]) . Text(0.5, 1.0, &#39;Comp. 2, corr = 0.09&#39;) . . ccX_df = pd.DataFrame({&quot;CCX_1&quot;:X_c[:, 0], &quot;CCX_2&quot;:X_c[:, 1], &quot;Species&quot;:df.species.astype(&#39;category&#39;).cat.codes, &quot;Island&quot;:df.island.astype(&#39;category&#39;).cat.codes, &quot;sex&quot;:df.sex.astype(&#39;category&#39;).cat.codes, &quot;bill_length&quot;:X_mc.bill_length_mm, &quot;bill_depth&quot;:X_mc.bill_depth_mm}) corr_X_df= ccX_df.corr(method=&#39;pearson&#39;) print(corr_X_df.head()) plt.figure(figsize=(10,8)) X_df_lt = corr_X_df.where(np.tril(np.ones(corr_X_df.shape)).astype(np.bool)) sns.heatmap(X_df_lt,cmap=&quot;coolwarm&quot;,annot=True,fmt=&#39;.1g&#39;) plt.tight_layout() plt.savefig(&quot;Heatmap_Canonical_Correlates_from_X_and_data.jpg&quot;, format=&#39;jpeg&#39;, dpi=100) . CCX_1 CCX_2 Species Island sex CCX_1 1.000000e+00 -1.217716e-16 0.935057 -0.561781 0.025383 CCX_2 -1.217716e-16 1.000000e+00 -0.078719 0.228933 0.576790 Species 9.350575e-01 -7.871884e-02 1.000000 -0.622428 0.010964 Island -5.617810e-01 2.289327e-01 -0.622428 1.000000 -0.012435 sex 2.538332e-02 5.767897e-01 0.010964 -0.012435 1.000000 bill_length bill_depth CCX_1 0.828437 -0.734650 CCX_2 0.560082 0.678447 Species 0.730548 -0.740346 Island -0.337179 0.568031 sex 0.344078 0.372673 C: Users ADMINI~1 AppData Local Temp/ipykernel_24036/698194835.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations X_df_lt = corr_X_df.where(np.tril(np.ones(corr_X_df.shape)).astype(np.bool)) . . 시계열 분석 - fbprophet . prophet은 페이스북에서 개발한 시계열 예측 패키지다. ARIMA와 같은 확률론적이고 이론적인 모형이 아니라 몇가지 경험적 규칙(heuristic rule)을 사용하는 단순 회귀모형이지만 단기적 예측에서는 큰 문제 없이 사용할 수 있다. | . import pandas as pd url = &quot;https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv&quot; df = pd.read_csv(url) df.tail() . ds y . 2900 2016-01-16 | 7.817223 | . 2901 2016-01-17 | 9.273878 | . 2902 2016-01-18 | 10.333775 | . 2903 2016-01-19 | 9.125871 | . 2904 2016-01-20 | 8.891374 | . from fbprophet import Prophet m = Prophet() m.fit(df) . INFO:numexpr.utils:NumExpr defaulting to 4 threads. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. &lt;fbprophet.forecaster.Prophet at 0x1ec2c8e2308&gt; . future = m.make_future_dataframe(periods=365) future.tail() . ds . 3265 2017-01-15 | . 3266 2017-01-16 | . 3267 2017-01-17 | . 3268 2017-01-18 | . 3269 2017-01-19 | . yhat이 예측값 | . forecast = m.predict(future) forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail() . ds yhat yhat_lower yhat_upper . 3265 2017-01-15 | 8.203217 | 7.465164 | 8.969418 | . 3266 2017-01-16 | 8.528203 | 7.758541 | 9.264207 | . 3267 2017-01-17 | 8.315601 | 7.668485 | 9.087909 | . 3268 2017-01-18 | 8.148207 | 7.397069 | 8.896107 | . 3269 2017-01-19 | 8.160103 | 7.498117 | 8.846597 | . fig1 = m.plot(forecast) . . fig2 = m.plot_components(forecast) . . . . . 연관성 분석 = 장바구니분석 . import pandas as pd from mlxtend.preprocessing import TransactionEncoder from mlxtend.frequent_patterns import apriori, association_rules . 구매한 물건이 담긴 데이터 | . dataset = [[&#39;Milk&#39;, &#39;Onion&#39;, &#39;Nutmeg&#39;, &#39;Eggs&#39;, &#39;Yogurt&#39;], [&#39;Onion&#39;, &#39;Nutmeg&#39;, &#39;Eggs&#39;, &#39;Yogurt&#39;], [&#39;Milk&#39;, &#39;Apple&#39;, &#39;Eggs&#39;], [&#39;Milk&#39;, &#39;Unicorn&#39;, &#39;Corn&#39;, &#39;Yogurt&#39;], [&#39;Corn&#39;, &#39;Onion&#39;, &#39;Onion&#39;, &#39;Ice cream&#39;, &#39;Eggs&#39;]] . Encoding을 해 줌 : 인스턴스 생성 -&gt; fit -&gt; transform | . te = TransactionEncoder() te_ary = te.fit(dataset).transform(dataset) df = pd.DataFrame(te_ary, columns=te.columns_) . frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True) ## parameter # max_len=3 : 아이템 조합이 3개까지 제한 . frequent_itemsets # 전체 구매 데이터 중 해당 itemset이 포함된 확률 . support itemsets . 0 0.8 | (Eggs) | . 1 0.6 | (Milk) | . 2 0.6 | (Onion) | . 3 0.6 | (Yogurt) | . 4 0.6 | (Eggs, Onion) | . association_rules(frequent_itemsets, metric=&quot;lift&quot;, min_threshold=1) # metric 기준 min_threshold 이상 . antecedents consequents antecedent support consequent support support confidence lift leverage conviction . 0 (Eggs) | (Onion) | 0.8 | 0.6 | 0.6 | 0.75 | 1.25 | 0.12 | 1.6 | . 1 (Onion) | (Eggs) | 0.6 | 0.8 | 0.6 | 1.00 | 1.25 | 0.12 | inf | . 첫 줄 해석 . antencedents와 consequents가 있는데 각각의 support를 보여줌. | 그리고 조합의 support, confidence, lift를 보여주는데 | confidence : Onion을 사는 고객 중 Eggs+Onion이 75% | lift: 1이면 서로 영향이 없는 것. 그냥 Onion을 사는 것보다 Egg를 샀을 때 구매율이 1.25배 높아진다는 소리 | . 요인분석 . from sklearn.datasets import load_digits X, _ = load_digits(return_X_y=True) . X.shape . (1797, 64) . X . array([[ 0., 0., 5., ..., 0., 0., 0.], [ 0., 0., 0., ..., 10., 0., 0.], [ 0., 0., 0., ..., 16., 9., 0.], ..., [ 0., 0., 1., ..., 6., 0., 0.], [ 0., 0., 2., ..., 12., 0., 0.], [ 0., 0., 10., ..., 12., 1., 0.]]) . from sklearn.decomposition import FactorAnalysis transformer = FactorAnalysis(n_components=5, random_state=0) X_transformed = transformer.fit_transform(X) X_transformed.shape . (1797, 5) . X_transformed . array([[-0.15740939, 0.30545241, 1.88630105, 0.89678859, -0.17029374], [-0.87586253, 0.13827044, -1.75345561, -0.83281075, -0.74288303], [-0.99892214, -0.43236642, -1.22222905, -0.82192628, -0.77094974], ..., [-0.70066938, 0.09868465, -0.99651414, -0.14234655, -0.61502155], [-0.37322424, -0.18103725, 1.07294051, -0.6538424 , -0.28351881], [ 0.64021206, -0.87404644, -0.04237855, 0.32160612, -0.47697811]]) . # ... .",
            "url": "https://nueees.github.io/techblog/statistics/python/2020/01/05/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D3.html",
            "relUrl": "/statistics/python/2020/01/05/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D3.html",
            "date": " • Jan 5, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "통계분석2 (Ridge, Lasso)",
            "content": ". 06_통계분석_2 . Ridge | Lasso | Elastic Net | 참고자료 . 파이썬 머신러닝 완벽 가이드/권철민 | ICHI.PRO | 쿠의블로그 | . 짤막 설명 . 이번 세 규제모델은 머신러닝 파트에서 모델 고르고 튜닝하라는 문제에 사용하면 됨 | ADP에서 선형회귀 모형을 모델링할 일은 거의 없겠지만, 만약 이전 기출처럼 3개의 모델을 만들고 각각 튜닝해보라는 문제가 나온다면 첫번째 모델로 머신러닝의 기본of기본인 선형회귀 모형을 쓸 수 있을 것 같고, 이때 릿지라쏘를 활용해서 모델을 튜닝했다는 걸 보여주면 될 것 같음. . | 릿지, 라쏘, 엘라스틱넷 모형은 회귀모형 잔차합을 최소화시키는 기존의 회귀계수 산출 공식에 비용함수를 추가한 것임 | 회귀모형의 잔차합을 최소화시키는 방식으로 회귀계수를 구하면, 모델을 과적합시키는 방향으로 나아가기 때문에(회귀계수 w의 값이 커짐) 회귀계수벡터 w를 줄여주는 규제 항이 필요함 릿지회귀: 회귀계수의 제곱을 규제 항으로 사용 | 라쏘회귀: 회귀계수의 절대값을 규제 항으로 사용 (특정 w는 0이 됨) | 엘라스틱넷: 제곱과 절대값 모두를 규제항으로 사용 | . | When to use? 변수개수 &gt; 데이터의 양 : 라쏘 이용 라쏘는 주로 변수들이 많은 경우에 변수 선택을 하기 위해 사용 | 데이터의 양&gt;변수개수 : 릿지 이용 릿지는 변수들 간 다중공선성이 발견될 때 x의 분산을 줄이기 위해 사용 (정보 압축) | 엘라스틱넷은 교란변수 A가 있을 때, 실제 영향을 주는 변수인 B만 사라지고 A만 남아있는 상황(라쏘 이용 시)이나 A와 B의 회귀계수가 크게 줄어서 A의 영향력이 과소평가되는 상황(릿지 이용 시)을 피하기 위해 사용 | . | . . import pandas as pd import numpy as np from scipy import stats from sklearn.datasets import load_boston import matplotlib.pyplot as plt import seaborn as sns . ## 예제 데이터 load boston = load_boston() df_boston = pd.DataFrame(boston.data, columns = boston.feature_names) df_boston[&#39;PRICE&#39;] = boston.target . df_boston.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . X_data = df_boston.drop([&#39;PRICE&#39;], axis = 1) y_target = df_boston[&#39;PRICE&#39;] . 릿지회귀(L2규제) . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score ridge = Ridge(alpha = 10) ## alpha는 규제항에 곱해지는 것으로 alpha가 커질수록 회귀계수는 작아짐 (alpha = 회귀식의 람다) neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring = &quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) . ## 규제항에 곱해지는 alpha(=람다) 값의 변화에 따른 정확도, 회귀계수의 크기 변화 측정 alphas = [0, 0.1, 1, 10, 100] ## alpha = 0일때는 규제가 없는 경우와 동일 for alpha in alphas: ridge = Ridge(alpha = alpha) neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring = &quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print(&#39;alpha {0} 일 때 5 folds의 평균 RMSE: {1:.3f}&#39;.format(alpha, avg_rmse)) . alpha 0 일 때 5 folds의 평균 RMSE: 5.829 alpha 0.1 일 때 5 folds의 평균 RMSE: 5.788 alpha 1 일 때 5 folds의 평균 RMSE: 5.653 alpha 10 일 때 5 folds의 평균 RMSE: 5.518 alpha 100 일 때 5 folds의 평균 RMSE: 5.330 . coeff_df = pd.DataFrame() for pos, alpha in enumerate(alphas): ridge = Ridge(alpha = alpha) ridge.fit(X_data, y_target) coeff = pd.Series(data = ridge.coef_, index = X_data.columns) colname = &#39;alpha:&#39; + str(alpha) coeff_df[colname] = coeff print(coeff_df) . alpha:0 alpha:0.1 alpha:1 alpha:10 alpha:100 CRIM -0.108011 -0.107474 -0.104595 -0.101435 -0.102202 ZN 0.046420 0.046572 0.047443 0.049579 0.054496 INDUS 0.020559 0.015999 -0.008805 -0.042962 -0.052826 CHAS 2.686734 2.670019 2.552393 1.952021 0.638335 NOX -17.766611 -16.684645 -10.777015 -2.371619 -0.262847 RM 3.809865 3.818233 3.854000 3.702272 2.334536 AGE 0.000692 -0.000269 -0.005415 -0.010707 0.001212 DIS -1.475567 -1.459626 -1.372654 -1.248808 -1.153390 RAD 0.306049 0.303515 0.290142 0.279596 0.315358 TAX -0.012335 -0.012421 -0.012912 -0.013993 -0.015856 PTRATIO -0.952747 -0.940759 -0.876074 -0.797945 -0.829218 B 0.009312 0.009368 0.009673 0.010037 0.009393 LSTAT -0.524758 -0.525966 -0.533343 -0.559366 -0.660764 . 위의 릿지회귀모델에서는 alpha가 100일 때 CV를 이용한 모델평가 정확도가 가장 높게 나왔다. 또한 규제항의 alpha가 높아지면 회귀계수의 값은 점점 더 작아짐을 확인할 수 있는데, 0이 되는 항은 하나도 없는 것에서 릿지 모델의 특징을 알 수 있다. . 라쏘회귀(L1규제) . 회귀계수 w의 절대값을 비용함수로 사용하여 w가 0이 되는 경우가 쉽게 생김 | . ## Lasso는 Ridge회귀에서 사용했던 식 그대로에서 함수만 바꿔주면 된다. from sklearn.linear_model import Lasso alphas = [0, 0.1, 1, 10, 100] for alpha in alphas: lasso = Lasso(alpha = alpha) neg_mse_scores = cross_val_score(lasso, X_data, y_target, scoring = &quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print(&#39;Lasso 회귀 alpha {0} 일 때 5 folds의 평균 RMSE: {1:.3f}&#39;.format(alpha, avg_rmse)) coeff_df = pd.DataFrame() for pos, alpha in enumerate(alphas): lasso = Lasso(alpha = alpha) lasso.fit(X_data, y_target) coeff = pd.Series(data = lasso.coef_, index = X_data.columns) colname = &#39;alpha:&#39; + str(alpha) coeff_df[colname] = coeff print(coeff_df) . Lasso 회귀 alpha 0 일 때 5 folds의 평균 RMSE: 5.829 Lasso 회귀 alpha 0.1 일 때 5 folds의 평균 RMSE: 5.615 Lasso 회귀 alpha 1 일 때 5 folds의 평균 RMSE: 5.776 Lasso 회귀 alpha 10 일 때 5 folds의 평균 RMSE: 6.586 Lasso 회귀 alpha 100 일 때 5 folds의 평균 RMSE: 8.393 alpha:0 alpha:0.1 alpha:1 alpha:10 alpha:100 CRIM -0.108011 -0.097894 -0.063437 -0.000000 -0.000000 ZN 0.046420 0.049211 0.049165 0.026146 0.000000 INDUS 0.020559 -0.036619 -0.000000 -0.000000 -0.000000 CHAS 2.686734 0.955190 0.000000 0.000000 0.000000 NOX -17.766611 -0.000000 -0.000000 0.000000 -0.000000 RM 3.809865 3.703202 0.949811 0.000000 0.000000 AGE 0.000692 -0.010037 0.020910 0.000000 -0.000000 DIS -1.475567 -1.160538 -0.668790 -0.000000 0.000000 RAD 0.306049 0.274707 0.264206 0.000000 -0.000000 TAX -0.012335 -0.014570 -0.015212 -0.009282 -0.020972 PTRATIO -0.952747 -0.770654 -0.722966 -0.000000 -0.000000 B 0.009312 0.010249 0.008247 0.007496 0.004466 LSTAT -0.524758 -0.568769 -0.761115 -0.564038 -0.000000 . 라쏘 회귀는 ALPHA가 0.1일 때 가장 좋은 성능을 보였고, ALPHA값이 증가함에 따라 다수의 회귀계수가 0이 되는 모습을 보임. 릿지 회귀는 alpha를 높여도 w가 작아질지언정 0이 되지는 않았으나, lasso는 급격하게 0을 만들어나감. 따라서 feature selection에 적합한 모델이라 볼 수 있음. 라쏘와 릿지의 비교에서, 이 boston 집값 데이터는 row 수는 충분하기 때문에 정보량을 줄이는 쪽으로 모델링해야 한다고 말할 수 있을 것 같음 . 엘라스틱넷 . ## 다 똑같다.. 그냥 함수만 계속 바꿔주면 된다 from sklearn.linear_model import ElasticNet alphas = [0, 0.1, 1, 10, 100] for alpha in alphas: elasticnet = ElasticNet(alpha = alpha) neg_mse_scores = cross_val_score(elasticnet, X_data, y_target, scoring = &quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print(&#39;ElasticNet 회귀 alpha {0} 일 때 5 folds의 평균 RMSE: {1:.3f}&#39;.format(alpha, avg_rmse)) coeff_df = pd.DataFrame() for pos, alpha in enumerate(alphas): elasticnet = ElasticNet(alpha = alpha) elasticnet.fit(X_data, y_target) coeff = pd.Series(data = elasticnet.coef_, index = X_data.columns) colname = &#39;alpha:&#39; + str(alpha) coeff_df[colname] = coeff print(coeff_df) . ElasticNet 회귀 alpha 0 일 때 5 folds의 평균 RMSE: 5.829 ElasticNet 회귀 alpha 0.1 일 때 5 folds의 평균 RMSE: 5.478 ElasticNet 회귀 alpha 1 일 때 5 folds의 평균 RMSE: 5.522 ElasticNet 회귀 alpha 10 일 때 5 folds의 평균 RMSE: 6.472 ElasticNet 회귀 alpha 100 일 때 5 folds의 평균 RMSE: 8.312 alpha:0 alpha:0.1 alpha:1 alpha:10 alpha:100 CRIM -0.108011 -0.100079 -0.080371 -0.000000 -0.000000 ZN 0.046420 0.051377 0.053240 0.040935 0.000000 INDUS 0.020559 -0.045901 -0.012657 -0.000000 -0.000000 CHAS 2.686734 0.987970 0.000000 0.000000 0.000000 NOX -17.766611 -0.059533 -0.000000 0.000000 -0.000000 RM 3.809865 3.252662 0.933936 0.000000 0.000000 AGE 0.000692 -0.007219 0.020579 0.020067 -0.000000 DIS -1.475567 -1.181402 -0.762044 -0.000000 0.000000 RAD 0.306049 0.288726 0.301569 0.000000 0.000000 TAX -0.012335 -0.014952 -0.016439 -0.008950 -0.021347 PTRATIO -0.952747 -0.793502 -0.748046 -0.000000 -0.000000 B 0.009312 0.009963 0.008339 0.007436 0.010110 LSTAT -0.524758 -0.598184 -0.758426 -0.632398 -0.000000 . alpha의 값을 range 함수를 이용해 더 촘촘히 설정하면, 분산과 편향을 가장 적게 만드는 골디락스 지점에 근접하게 찾을 수 있을 것으로 보임. | Ridge가 현재 가장 좋은 성능으로 나오는 이유는 parameter 개수보다 데이터의 수가 많기 때문에 굳이 feature를 쳐내는 것 보다는 개별 변수의 영향력을 줄이는 게 전체적으로 오류율이 떨어지는 것 같음 | .",
            "url": "https://nueees.github.io/techblog/statistics/python/2020/01/04/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D2.html",
            "relUrl": "/statistics/python/2020/01/04/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D2.html",
            "date": " • Jan 4, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "통계분석1 (Linear Regression)",
            "content": ". 06_통계분석_1 . 회귀분석 . 독립변수의 값이 주어졌을 때 종속변수의 값을 예측하거나, 독립변수와 종속변수간의 인과관계를 검증할 수 있는 통계분석 기법 ex) 아파트의 방 개수, 방 크기, 주변 학군 등 여러 개의 독립변수에 따라 아파트 가격이라는 종속변수가 어떤 관계를 나타내는지를 모델링하고 예측 또는 광고의 횟수와 매출액의 관계나 구매유형에 따른 고객만족도와 같은 관계 등을 파악 | . 회귀 유형 : 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 나뉨 . ㄱ. 독립변수의 수 단순회귀 : 독립변수와 종속변수가 각각 한개씩이면서 모두 수치형 다중회귀 : 1개의 수치형 종속변수와 2개 이상의 독립변수(수치형 또는 범주형) ㄴ. 독립변수의 척도 일반회귀 : 등간, 비율척도 더미변수를 이용한 회귀 : 명목, 서열척도 ㄷ. 독립변수와 종속변수의 관계 선형회귀 : 선형 비선형회귀 : 비선형 . 선형 회귀 : 실제 값과 예측값의 차이, 즉 RSS(Residual Sum of Squares)를 최소화하는 직선형 회귀선을 최적화하는 방식 . 규제가 없는 일반 선형 회귀, 선형 회귀에 L1 규제를 추가한 라쏘(Lasso) 회귀, L2 규제를 추가한 릿지(Ridge) 회귀, L1/L2규제를 결합한 엘라스틱넷(ElasticNet) 회귀, 분류에 사용되는 선형 모델인 로지스틱 회귀(Logistic Regression) . 전제조건(가정) . 선형성(선형회귀에 한함) : 독립변수와 종속변수 간에는 선형관계가 존재 일부가 선형성을 만족하지 않는다면, 일단 선형 회귀모델을 만들고 변수 선택법으로 처리하거나, 다른 새로운 변수를 추가해보거나, 선형성을 만족하지 않는 변수를 제거하거나, 로그/지수/루트 등 변수 변환하는 방법으로 처리 등분산성 : 잔차들은 동일한 분산을 가짐, 분산이 같다는 것은 특정한 패턴 없이 고르게 분포했다는 의미 독립성(다중회귀에 해당) : 잔차들은 서로 독립(독립변수 x 간에 상관관계가 없이 독립성을 만족) Stepwise를 사용해서 다중공선성을 일으키는 변수들을 제거하는 방법으로 처리 정규성 : 잔차가 정규분포를 따름. Shapiro-Wilk Test로 확인 . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt # print를 하지 않아도 행을 모두 보여줌 from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; # Columns를 생략 없이 모두 보여줌 from IPython.display import display pd.options.display.max_columns = None . [ sklearn package의 LinearRegression Class ] . 예측값과 실제값의 RSS(Residual Sum of Squares)를 최소화해 OLS(Ordinary Least Squares, 최소제곱법) 추정 방식으로 구현한 클래스 | 따라서, 수렴할 때까지 얼마나 반복할 것인지(num_iterations), 얼마나 꼼꼼히 학습할 것인지(learning_rate) 이런 것들을 복잡하게 고려하지 않아도 됨 | 단, OLS기반의 회귀 계수 계산은 입력 피처의 독립성에 많은 영향을 받음. 피처간의 상관관계까 매우 높은 경우 분산이 매추 커져서 오류에 매우 민감해짐 &lt;- 이러한 현상을 다중공선성(multi-collinearity) 문제라고 함 상관관계가 높은 피처가 많은 경우 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용함. 또한 PCA를 통해 차원 축소를 수행하는것도 고려해볼 수 있음 . | library load from sklearn.linear_model import LinearRegression . | LinearRegression Class는 fit() method로 X, y 배열을 입력받으면 회귀 계수(Coefficients)인 W를 coef_속성에 저장함 . | LinearRegression 모델을 생성하고 line_fitter = LinearRegression() &lt;– 모형에 상수항(절편)이 있으면 True line_fitter = LinearRegression(fit_intercept=False) &lt;– 모형에 상수항(절편)이 없으면 False로 ※ fit_intercept : default TRUE, 절편(intercept)값 계산여부를 지정함. False로 지정하면 0으로 지정됨 ※ normalize : default FALSE, fit_intercept가 False인 경우에는 이 파라미터 무시됨. True로 지정하면 회귀 수행 전 입력 데이터셋을 정규화함 . | 그 안에 X, y 데이터를 fit함 fit() 메서드는 선형 회귀 모델에 필요한 두 가지 변수를 전달하는 것 line_fitter.fit(X, y) 이렇게 하면 새로운 X 값을 넣어 y값을 예측할 수 있음 y_predicted = line_fitter.predict(X) . | 기울기(회귀계수) 확인 : line_fitter.coef_ | 절편 확인 : line_fitter.intercept_ | . [ statsmodels package의 formula.api Module ] . import statsmodels.formula.api as smf . | 회귀분석을 지원하는 ols() 함수 : ols(formula=’종속변수 ~ 독립변수’, data=data1) model_smf = smf.ols(formula=’height ~ weight’, data = body).fit() . | 분석 결과 확인 model_smf.summary() 결과 해석은 아래 단순회귀 참조 . | . [ 회귀 평가 지표 ] . MAE(Mean Absolute Error) : 실제값과 예측값의 차이를 절댓값으로 변환해 평균한 것 사이킷런 평가지표 API : metrics.mean_absolute_error Scoring 함수 적용 값 : ‘neg_mean_absolute_error’ . | MSE(Mean Squared ERror, 평균제곱오차) : 실제값과 예측값의 차이를 제곱해 평균한 것 전체 에러를 표현하기 위해서 사용, 부호로 인한 오차를 의미를 없애고자 오차에 제곱 오차의 제곱에 대해 평균을 취한 것, 수치가 작을수록 원본과의 오차가 적은 것 사이킷런 평가지표 API : metrics.mean_squared_error scoring=”neg_mean_squared_error” : 접두어 neg_는 Negative(음수)값을 가진다는 의미로, cross_val_score()의 인자로 scoring=”neg_mean_squared_error”를 지정해 음수값을 반환하는 이유는 사이킷런의 Scoring함수가 score값이 클수록 좋은 평가 결과로 자동 평가하기 때문 이 인자를 지정한 경우 반환되는 음수 값에 -1을 곱해서 양의 값으로 변경하여 출력 mse = mean_squared_error(y_test, y_preds) print(‘MSE : {0:.3f} , RMSE : {1:.3F}’.format(mse , rmse)) . | RMSE(Root Mean Squared Error, 평균제곱근오차) : MSE에 루트를 씌운 것 MSE값은 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있으므로 특정 수치에 대한 예측의 정확도를 표현할 때, Accuracy로 판단하기에는 정확도를 올바르게 표기할 수 없어, RMSE 수치로 정확도를 판단함 일반적으로 해당 수치가 낮을수록 정확도가 높다고 판단 사이킷런에서 제공하지 않음. MSE에 제곱근을 씌워서 계산하는 함수를 직접 만들어야함 rmse = np.sqrt(mse) . | R2 : 분산 기반으로 예측 성능을 평가함. 실제값의 분산 대비 예측값의 분산 비율을 지표로 하며, 1에 가까울수록 예측 정확도가 높음 확인필요 R2는 적합된 회귀선의 설명력 아니었나? 예측 정확도와 같음? 사이킷런 평가지표 API : metrics.r2_score Scoring 함수 적용 값 : ‘r2’ print(‘Variance score : {0:.3f}’.format(r2_score(y_test, y_preds))) . | . 1. 단순회귀분석 . 종속변수(수치형)를 하나의 독립변수(수치형)로 설명하는 것 | y ~ X1 | . # 예제 데이터셋 height = [170, 168, 177, 181 ,172, 171, 169, 175, 174, 178, 170, 167, 177, 182 ,173, 171, 170, 179, 175, 177, 186, 166, 183, 168] weight = [70, 66, 73, 77, 74, 73, 69, 79, 77, 80, 74, 68, 71, 76, 78, 72, 68, 79, 77, 81, 84, 73, 78, 69] body = pd.DataFrame( {&#39;height&#39;: height, &#39;weight&#39;: weight } ) body.head(3) . height weight . 0 170 | 70 | . 1 168 | 66 | . 2 177 | 73 | . plt.plot(body[&#39;weight&#39;], body[&#39;height&#39;], &#39;o&#39;) plt.show() . [&lt;matplotlib.lines.Line2D at 0x2a0371e55e0&gt;] . . 1-1) LinearRegression . # 단순 선형 회귀 모델 import numpy as np from sklearn.linear_model import LinearRegression # X데이터를 넣을 때 .values.reshape(-1,1) 이유 : X는 2차원 array 형태여야 하기 때문, 차원증가 X = body[&#39;weight&#39;].values.reshape(-1,1) #단순회귀일 때는 reshape 필수 y = body[&#39;height&#39;] model = LinearRegression() model.fit(X, y) print(&#39;R2 : &#39;, model.score(X, y)) # 1에 가까울수록 설명력이 높은데... 0.589 print(&#39;회귀계수 :&#39;,model.coef_) print(&#39;절편 : &#39;, model.intercept_) # y = 107.86 + 0.89x . LinearRegression() R2 : 0.5893075473608536 회귀계수 : [0.89042657] 절편 : 107.86242266362746 . # 예측 # print(np.array([80, 70, 100])) # print(new) new = np.array([80, 70, 100]).reshape(-1,1) # predict할 때도 reshape 필수 model.predict(new) . array([179.09654836, 170.19228264, 196.90507978]) . # 모델 시각화 plt.plot(X, y, &#39;o&#39;) plt.plot(X,model.predict(X)) plt.show() . [&lt;matplotlib.lines.Line2D at 0x1e22ea61e10&gt;] [&lt;matplotlib.lines.Line2D at 0x1e22ea6b550&gt;] . . 1-2) statsmodels - formula.api . # statsmodels package 사용 import statsmodels.formula.api as smf # 단순회귀분석 실행하고 분석결과 출력 # 회귀분석을 지원하는 ols() 함수 : ols(formula=&#39;종속변수 ~ 독립변수&#39;, data=data1) model_smf = smf.ols(formula=&#39;height ~ weight&#39;, data = body).fit() model_smf.summary() # 첫번째 테이블 : 수행된 회귀분석 결과의 일반적인 사항을 요약함 # 종속변수(Dep.Variable), 분석모델(Model), Model의 계산방식(Method) # F-statistic : 해당 회귀모형의 적합도를 나타내는 통계량 # F검정통계량의 유의 확률(Prob(F-statistic) : 1.20e-05로 유의확률이 0.01이하이므로 본 회귀모형은 유의미함 # 모형의 결정계수(R-squared) 즉 설명력은 0.589 수준으로 나타남 # 잔차의 자유도(Df Residuals)는 22, 모형의 매개변수 수(DF Model)는 1 # AIC와 BIC는 복수개 모형의 적합도를 비교할 때 사용하는데 일반적으로 해당 통계량이 작을수록 더 좋은 모형이라고 판단함 # (이 예제는 1개의 모형만 사용했으므로 해당사항 없음) # 두번째 테이블 : 모형에 의해 추정된 회귀계수 테이블 # 계수 추정치의 기본 표준오차(std err), t-value(t), 유의확률(P&gt;|t|) # 95% 신뢰구간의 하한/상한값 제시 # 분석결과 절편(intercept)과 독립변수인 weight는 모두 유의미하게 나타남(0.000???) # 세번째 테이블 : 잔차의 분포를 평가하기 위한 몇가지 통계량을 보여줌 # Omnibus : 잔차의 왜도와 첨도를 이용한 검정 통계량으로 0에 가까운 값이 나올수록 정규분포를 따른다고 판단 # Prob(Omnibus) : Omnibus의 p-value임. 귀무가설로써 정규분포인지 평가함 # Jarque-Bera(JB)/Prob(JB) : Omnibus와 유사하게 잔차의 왜도와 첨도를 이용해 정규성을 검정하는 통계량 # 왜도(Skew), 첨도(Kurtosis) # Durbin-Watson : 회귀분석 시 잔차의 독립성을 검증할 때 사용하는 통계량. 보통 0~4 값이 출력되며 2에 가까울수록 독립적이라고 판단 # Cond. No.(Condition Number) : 다중공선성을 평가할 수 있는 통계량. 30보다 작으면 다중공선성이 없다고 판단함 # (이 예제는 하나의 독립변수만 사용했으므로 해당없음) . OLS Regression Results Dep. Variable: height | R-squared: 0.589 | . Model: OLS | Adj. R-squared: 0.571 | . Method: Least Squares | F-statistic: 31.57 | . Date: Sat, 27 Nov 2021 | Prob (F-statistic): 1.20e-05 | . Time: 21:14:49 | Log-Likelihood: -63.655 | . No. Observations: 24 | AIC: 131.3 | . Df Residuals: 22 | BIC: 133.7 | . Df Model: 1 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . Intercept 107.8624 | 11.816 | 9.128 | 0.000 | 83.357 | 132.368 | . weight 0.8904 | 0.158 | 5.619 | 0.000 | 0.562 | 1.219 | . Omnibus: 0.796 | Durbin-Watson: 2.201 | . Prob(Omnibus): 0.672 | Jarque-Bera (JB): 0.829 | . Skew: 0.329 | Prob(JB): 0.661 | . Kurtosis: 2.371 | Cond. No. 1.20e+03 | . Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.2e+03. This might indicate that there arestrong multicollinearity or other numerical problems. . 2. 다중회귀분석 . X가 두 개 이상 | 종속변수(1개)는 수치형이고, 독립변수는 수치형이거나 범주형일 수 있음, 범주형 변수는 더미변수로 전환하여 사용 | . # 예제 데이터셋 X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) print(&#39;X : n&#39;, X) # y = ( ( x_0 * 1 ) + ( x_1 * 2 ) ) + 3 y = np.dot(X, np.array([1, 2])) + 3 print(&#39; ny : n&#39;, y) . X : [[1 1] [1 2] [2 2] [2 3]] y : [ 6 8 9 11] . 2-1) 교호작용을 고려하지 않을 때 . import numpy as np from sklearn.linear_model import LinearRegression #model = LinearRegression() #model.fit(X, y) model = LinearRegression().fit(X, y) print(&#39;R2 : &#39;, model.score(X, y)) print(&#39;회귀계수 :&#39;,model.coef_) print(&#39;절편 : &#39;, model.intercept_) # 예측 print(model.predict(np.array([[3, 5]]))) # 정수가 아니라 실수로 출력되네 . R2 : 1.0 회귀계수 : [1. 2.] 절편 : 3.0000000000000018 [16.] . 2-2) 교호작용을 고려할 때 . 교호작용(Interaction) : 한 요인의 효과가 다른 요인의 수준에 의존하는 경우 . | 두 인자 A, B 간에는 교호작용이 없음 : A의 효과가 B의 서로 다른 수준 B1과 B2에서 일관성 있게 나타날 때 | 두 인자 A, B 간에 교호작용이 존재함 : B가 ‘B1수준에 있을 때 A의 효과’와 ‘B2수준에 있을 때 A의 효과’간에 차이가 있을 때 . | X1, X2, X3가 있을 때 2차 교호작용 변수는 두 개 변수까지 곱하는 것, 3차는 세 개까지의 조합 | 제곱을 하는 것은 교호가 아니라 다항의 개념 | 교호작용도를 사용하여 가능한 교호작용을 시각화할 수 있다. 교호작용도가 평행선으로 나타나면 교호작용이 없다는 것을 나타낸다. | . # 맛보기(다항회귀에서 상세하게) from sklearn.preprocessing import PolynomialFeatures poly_d2 = PolynomialFeatures(degree=2, interaction_only=True) # degree = 2 는 2차까지 만든다는 뜻, X3까지 있으면 degree=3도 가능할 듯 # interaction = True는 교호작용 변수만 만들고, 다항(제곱) 변수는 만들지 않겠다는 것 X_poly = poly_d2.fit_transform(X) X_poly # 1, X1, X2, X1*X2 순 . array([[1., 1., 1., 1.], [1., 1., 2., 2.], [1., 2., 2., 4.], [1., 2., 3., 6.]]) . import numpy as np from sklearn.linear_model import LinearRegression model = LinearRegression().fit(X_poly, y) print(&#39;R2 : &#39;, model.score(X_poly, y)) print(&#39;회귀계수 :&#39;,model.coef_) print(&#39;절편 : &#39;, model.intercept_) # 예측 print(model.predict(np.array([[1, 3, 5, 15]]))) . R2 : 1.0 회귀계수 : [ 0.00000000e+00 1.00000000e+00 2.00000000e+00 -1.55360188e-15] 절편 : 2.9999999999999964 [16.] . 2-3) 다중회귀분석 예시 : 집값 예측 . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline # boston 데이터셋 로드 boston = load_boston() boston . {&#39;data&#39;: array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02, 4.9800e+00], [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02, 9.1400e+00], [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02, 4.0300e+00], ..., [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 5.6400e+00], [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02, 6.4800e+00], [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 7.8800e+00]]), &#39;target&#39;: array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. , 18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6, 15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2, 13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7, 21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9, 35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5, 19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. , 20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2, 23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8, 33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4, 21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. , 20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6, 23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4, 15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4, 17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7, 25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4, 23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. , 32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3, 34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4, 20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. , 26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3, 31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1, 22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6, 42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. , 36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4, 32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. , 20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1, 20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2, 22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1, 21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6, 19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7, 32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1, 18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8, 16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8, 13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3, 8.8, 7.2, 10.5, 7.4, 10.2, 11.5, 15.1, 23.2, 9.7, 13.8, 12.7, 13.1, 12.5, 8.5, 5. , 6.3, 5.6, 7.2, 12.1, 8.3, 8.5, 5. , 11.9, 27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3, 7. , 7.2, 7.5, 10.4, 8.8, 8.4, 16.7, 14.2, 20.8, 13.4, 11.7, 8.3, 10.2, 10.9, 11. , 9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4, 9.6, 8.7, 8.4, 12.8, 10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4, 15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7, 19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2, 29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8, 20.6, 21.2, 19.1, 20.6, 15.2, 7. , 8.1, 13.6, 20.1, 21.8, 24.5, 23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), &#39;feature_names&#39;: array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;, &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;), &#39;DESCR&#39;: &quot;.. _boston_dataset: n nBoston house prices dataset n n n**Data Set Characteristics:** n n :Number of Instances: 506 n n :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. n n :Attribute Information (in order): n - CRIM per capita crime rate by town n - ZN proportion of residential land zoned for lots over 25,000 sq.ft. n - INDUS proportion of non-retail business acres per town n - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) n - NOX nitric oxides concentration (parts per 10 million) n - RM average number of rooms per dwelling n - AGE proportion of owner-occupied units built prior to 1940 n - DIS weighted distances to five Boston employment centres n - RAD index of accessibility to radial highways n - TAX full-value property-tax rate per $10,000 n - PTRATIO pupil-teacher ratio by town n - B 1000(Bk - 0.63)^2 where Bk is the proportion of black people by town n - LSTAT % lower status of the population n - MEDV Median value of owner-occupied homes in $1000&#39;s n n :Missing Attribute Values: None n n :Creator: Harrison, D. and Rubinfeld, D.L. n nThis is a copy of UCI ML housing dataset. nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/ n n nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. n nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic nprices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, nvol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics n...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on npages 244-261 of the latter. n nThe Boston house-price data has been used in many machine learning papers that address regression nproblems. n n.. topic:: References n n - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. n - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. n&quot;, &#39;filename&#39;: &#39;C: Users Administrator anaconda3 lib site-packages sklearn datasets data boston_house_prices.csv&#39;} . # boston 데이터셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names) bostonDF.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . # boston dataset의 target array는 주택 가격. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;Boston 데이터셋 크기 :&#39;, bostonDF.shape, &#39; n&#39;) # 506 rows, 14 cols print(&#39;Boston 데이터셋 정보 :&#39;, bostonDF.info()) # Null 없음. 14 cols 모두 float type bostonDF.head() . Boston 데이터셋 크기 : (506, 14) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 CRIM 506 non-null float64 1 ZN 506 non-null float64 2 INDUS 506 non-null float64 3 CHAS 506 non-null float64 4 NOX 506 non-null float64 5 RM 506 non-null float64 6 AGE 506 non-null float64 7 DIS 506 non-null float64 8 RAD 506 non-null float64 9 TAX 506 non-null float64 10 PTRATIO 506 non-null float64 11 B 506 non-null float64 12 LSTAT 506 non-null float64 13 PRICE 506 non-null float64 dtypes: float64(14) memory usage: 55.5 KB Boston 데이터셋 정보 : None . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . CRIM: 지역별 범죄 발생률 | ZN: 25,000평방피트를 초과하는 거주 지역의 비율 | NDUS: 비상업 지역 넓이 비율 | CHAS: 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0) | NOX: 일산화질소 농도 | RM: 거주할 수 있는 방 개수 | AGE: 1940년 이전에 건축된 소유 주택의 비율 | DIS: 5개 주요 고용센터까지의 가중 거리 | RAD: 고속도로 접근 용이도 | TAX: 10,000달러당 재산세율 | PTRATIO: 지역의 교사와 학생 수 비율 | B: 지역의 흑인 거주 비율 | LSTAT: 하위 계층의 비율 | MEDV: 본인 소유의 주택 가격(중앙값) | . # 각 컬럼별로 PRICE(주택가격)에 미치는 영향도 조사 # 8개 col에 대해 값이 증가할 수록 PRICE가 어떻게 변하는지 확인 # matplotlib의 subplots() : 여러개의 그래프를 한번에 표현하기 위해 사용 # (ncols : 열 방향으로 위치할 그래프의 수, nrows : 행 방향으로 위치할 그래프의 수, 총 ncols*nrows개의 그래프를 그림) # 아래는 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐 fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2) lm_features = [&#39;RM&#39;,&#39;ZN&#39;,&#39;INDUS&#39;,&#39;NOX&#39;,&#39;AGE&#39;,&#39;PTRATIO&#39;,&#39;LSTAT&#39;,&#39;RAD&#39;] for i , feature in enumerate(lm_features): row = int(i/4) col = i%4 # seaborn의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현 sns.regplot(x=feature , y=&#39;PRICE&#39;,data=bostonDF , ax=axs[row][col]) # RM과 LSTAT의 PRICE 영향도가 가장 두드러지게 나타남 # RM은 양 방향의 선형성이 가장 큼. 즉, 방의 크기가 클수록 가격이 증가하는 모습 # LSTAT은 음 방향의 선형성이 가장 큼. 즉, 하위 계층의 비율이 적을수록 가격이 증가하는 모습 . &lt;AxesSubplot:xlabel=&#39;RM&#39;, ylabel=&#39;PRICE&#39;&gt; &lt;AxesSubplot:xlabel=&#39;ZN&#39;, ylabel=&#39;PRICE&#39;&gt; &lt;AxesSubplot:xlabel=&#39;INDUS&#39;, ylabel=&#39;PRICE&#39;&gt; &lt;AxesSubplot:xlabel=&#39;NOX&#39;, ylabel=&#39;PRICE&#39;&gt; &lt;AxesSubplot:xlabel=&#39;AGE&#39;, ylabel=&#39;PRICE&#39;&gt; &lt;AxesSubplot:xlabel=&#39;PTRATIO&#39;, ylabel=&#39;PRICE&#39;&gt; &lt;AxesSubplot:xlabel=&#39;LSTAT&#39;, ylabel=&#39;PRICE&#39;&gt; &lt;AxesSubplot:xlabel=&#39;RAD&#39;, ylabel=&#39;PRICE&#39;&gt; . . # 학습과 테스트 데이터 세트로 분리하고 학습/예측/평가 수행 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156) # Linear Regression OLS로 학습/예측 수행 lr = LinearRegression() lr.fit(X_train ,y_train ) y_preds = lr.predict(X_test) # 평균제곱오차(MSE, Mean squared error) : 수치가 작을 수록 원본과의 오차가 적은 것 mse = mean_squared_error(y_test, y_preds) # 평균제곱근오차(RMSE, Root mean squared error) : 수치가 낮을수록 정확도가 높다고 판단 rmse = np.sqrt(mse) print(&#39;MSE : {0:.3f} nRMSE : {1:.3F}&#39;.format(mse , rmse)) print(&#39;Variance score : {0:.3f}&#39;.format(r2_score(y_test, y_preds))) . LinearRegression() MSE : 17.297 RMSE : 4.159 Variance score : 0.757 . print(&#39;절편 :&#39;,lr.intercept_) print(&#39;회귀 계수 :&#39;, np.round(lr.coef_, 1)) . 절편 : 40.995595172164336 회귀 계수 : [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] . # 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. index 컬럼명에 유의 coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns ) coeff.sort_values(ascending=False) # RM이 양의 값으로 회귀 계수가 가장 큼 # NOX의 회귀 계수 - 값이 상대적으로 너무 커 보임. 최적화 필요할 듯 . RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 INDUS 0.0 AGE 0.0 TAX -0.0 B 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 . #### Cross Validation 해보기 from sklearn.model_selection import cross_val_score y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) lr = LinearRegression() # cross_val_score( )로 5 Fold 셋으로 MSE를 구한 뒤 이를 기반으로 다시 RMSE 구함 # cross_val_score()의 인자로 scoring=&quot;neg_mean_squared_error&quot;를 지정하면 반환되는 수치 값은 음수 값이므로 -1을 곱해서 양의 값으로 neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) # cross_val_score(scoring=&quot;neg_mean_squared_error&quot;)로 반환된 값은 모두 음수 print(&#39; 5 folds 의 개별 Negative MSE scores : &#39;, np.round(neg_mse_scores, 2)) print(&#39; 5 folds 의 개별 RMSE scores : &#39;, np.round(rmse_scores, 2)) print(&#39; 5 folds 의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . 5 folds 의 개별 Negative MSE scores : [-12.46 -26.05 -33.07 -80.76 -33.31] 5 folds 의 개별 RMSE scores : [3.53 5.1 5.75 8.99 5.77] 5 folds 의 평균 RMSE : 5.829 .",
            "url": "https://nueees.github.io/techblog/statistics/python/2020/01/03/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D1.html",
            "relUrl": "/statistics/python/2020/01/03/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D1.html",
            "date": " • Jan 3, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "기본통계2 (Fisher, Macnemar)",
            "content": ". 2. Fisher의 직접확률 검정 . 만약 사건 발생 수, 빈도 수가 극히 적거나(특히 5개 이하의 빈도가 전체 셀 중 20%이상 존재하는 경우) 서로의 빈도 수의 차이가 큰 경우 → 카이제곱검정 의 정확도가 낮아짐 . → 피셔의 직접확률 검정(피셔의 정확검정) 사용 . . 귀무가설 : A의 효과가 없었다. | 대립가설 : A의 효과가 있었다. | . # 패키지 import import pandas as pd import numpy as np import scipy.stats as stats . # fake : 가짜약, real : 진짜약, good : 효과 있음, bad : 효과 없음 # 귀무가설 : 약의 효과가 없었다, 대립가설: 약의 효과가 있었다. data = pd.DataFrame([[1,6], [5,2]], index = [&#39;good&#39;,&#39;bad&#39;], columns = [&#39;fake&#39;,&#39;real&#39;]) data . fake real . good 1 | 6 | . bad 5 | 2 | . odds, pvalue = stats.fisher_exact(data) print(&quot;오즈비 : &quot;, round(odds,3),&#39; n&#39;&#39;p-value : &#39;, round(pvalue,3 )) . 오즈비 : 0.067 p-value : 0.103 . 결과해석 : p-value가 0.103으로 귀무가설을 채택한다. 즉, 약의 효과가 없었다고 볼 수 있다(약의 유효성 확인x) . . 3. 맥네마의 검정 . 짝지어진 자료(paired data, 같은 대상자로부터 두 번 측정한 자료)에서는 그룹 간 값들이 종속적임(독립x) 따라서, 그룹 간 비율을 비교하려면 카이제곱 검정보단 맥네마 검정을 해야함 . paired data 예) 인슐린 치료 전후의 당뇨병성 케톤산혈증 발생 여부 조사, 동일 환자의 MRI와 CT로 한 번씩 반복 검사하여 질병 발생 여부 확인 . 귀무가설 : 전후의 차이가 없다. | 대립가설 : 전후의 차이가 있다. | . . # 필요한 패키지 import from statsmodels.stats.contingency_tables import mcnemar import pandas as pd import numpy as np . # BF : 투약 전, AF : 투약 후, 0 : 발작 발생x, 1 : 발작 발생 data = pd.DataFrame([[16, 10], [27, 31]], index = [&#39;BF_0&#39;,&#39;BF_1&#39;], columns = [&#39;AF_0&#39;,&#39;AF_1&#39;]) data . AF_0 AF_1 . BF_0 16 | 10 | . BF_1 27 | 31 | . # b+c &gt; 25일때 # 데이터 프레임 사용x, 배열 형태여야 함 data = np.array(data) result = mcnemar(data, exact = False, correction = True) print(&#39;stat : &#39;, round(result.statistic,3),&#39; n&#39;&#39;p-value : &#39;, round(result.pvalue,3)) . stat : 6.919 p-value : 0.009 . # b+c &lt; 25일때 result2 = mcnemar(data, exact = True) print(&#39;stat : &#39;, round(result2.statistic,3),&#39; n&#39;&#39;p-value : &#39;, round(result2.pvalue,3)) . stat : 10.0 p-value : 0.008 . 결과해석 : p-value가 0.009로 귀무가설을 기각한다. 즉, 투약 전후의 발작 발생 비율이 유의하다. . 참고 사이트 1) https://hongl.tistory.com/105 2) https://alex-blog.tistory.com/entry/데이터-분석을-위한-통계분석카이스퀘어-검정-feat-python 3) https://www.statsmodels.org/dev/generated/statsmodels.stats.contingency_tables.mcnemar.html 4) https://mansoostat.tistory.com/39 . .",
            "url": "https://nueees.github.io/techblog/statistics/python/2020/01/02/%EA%B8%B0%EB%B3%B8%ED%86%B5%EA%B3%842.html",
            "relUrl": "/statistics/python/2020/01/02/%EA%B8%B0%EB%B3%B8%ED%86%B5%EA%B3%842.html",
            "date": " • Jan 2, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "기본통계1 (CDA, T-Test, ANOVA)",
            "content": "kaggle/GREG HAMEL/Hypothesis Testing . . 05_CDA(Confirmatory Data Analysis) . 어떤 현상이 ‘우연’인지 그렇지 않은지를 확인하기 위함 | . import scipy.stats as spst import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import math . Comparative Type Test . One Sample T-Test . 샘플 A의 평균이 x와 다른가? - p-value가 낮으면 ‘다르다!’ | 귀무가설 : 같다 | 대립가설 : 다르다 | . print(spst.poisson.rvs.__doc__) . Random variates of given type. Parameters - arg1, arg2, arg3,... : array_like The shape parameter(s) for the distribution (see docstring of the instance object for more information). loc : array_like, optional Location parameter (default=0). size : int or tuple of ints, optional Defining number of random variates (Default is 1). Note that `size` has to be given as keyword, not as positional argument. random_state : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional This parameter defines the object to use for drawing random variates. If `random_state` is `None` the `~np.random.RandomState` singleton is used. If `random_state` is an int, a new ``RandomState`` instance is used, seeded with random_state. If `random_state` is already a ``RandomState`` or ``Generator`` instance, then that object is used. Default is None. Returns - rvs : ndarray or scalar Random variates of given `size`. . np.random.seed(6) population_ages1 = spst.poisson.rvs(loc=18, mu=35, size=150000) # loc: lowest x value, mu: middle of distribution population_ages2 = spst.poisson.rvs(loc=18, mu=10, size=100000) population_ages = np.concatenate((population_ages1, population_ages2)) minnesota_ages1 = spst.poisson.rvs(loc=18, mu=30, size=30) minnesota_ages2 = spst.poisson.rvs(loc=18, mu=10, size=20) minnesota_ages = np.concatenate((minnesota_ages1, minnesota_ages2)) print( population_ages.mean()) print( minnesota_ages.mean()) . 43.000112 39.26 . spst.ttest_1samp(a = minnesota_ages, # Sample data popmean = population_ages.mean()) # Pop mean . Ttest_1sampResult(statistic=-2.5742714883655027, pvalue=0.013118685425061678) . pvalue : 0.013118685425061678 . 정직한 설명 : 귀무가설이 참이라는 전제하에 이렇게 데이터가 관찰될 확률이 0.0….01%정도라는 뜻이다. | 발칙한 설명 : 기존 배너보다 나을 확률이 99%를 넘는다는 뜻이다. | . statistic : 2.5742714883655027 . 신호/노이즈, 즉 신호가 노이즈보다 2.57배 높다는 뜻 | . vals = spst.t.ppf([0.05, 0.95] # Quantile to check , 49)# minnesota_ages&#39; Degrees of freedom print(vals) . [-1.67655089 1.67655089] . # print(spst.t.ppf.__doc__) # print(spst.t.cdf.__doc__) spst.t.cdf(vals, 49) . array([0.05, 0.95]) . 1) For a lower-tailed test, p-value = cdf(test_statistic) 2) For an upper-tailed test, p-value = 1 - cdf(test_statistic) 3) For a two-tailed test, p-value = 2 * (1 - cdf(|test_statistic|)) . spst.t.cdf(x= -2.5742, # T-test statistic df= 49) * 2 # two-tailed test . 0.013121066545690117 . t-test의 p-value 0.013121066545690117과 같음 . sigma = minnesota_ages.std()/math.sqrt(50) # Sample stdev/sample size spst.t.interval(0.95, # Confidence level 95로 할 때 43 평균이 포함 안됨 df = 49, # Degrees of freedom loc = minnesota_ages.mean(), # Sample mean scale= sigma) # Standard dev estimate . (36.369669080722176, 42.15033091927782) . spst.t.interval(0.99, # 99로 하면 43 평균이 포함됨 df = 49, loc = minnesota_ages.mean(), scale= sigma) . (35.40547994092107, 43.11452005907893) . Two Sample T-Test . A와 B가 다른가? p-value가 낮으면 ‘다르다’! | . np.random.seed(12) wisconsin_ages1 = spst.poisson.rvs(loc=18, mu=33, size=30) wisconsin_ages2 = spst.poisson.rvs(loc=18, mu=13, size=20) wisconsin_ages = np.concatenate((wisconsin_ages1, wisconsin_ages2)) print( wisconsin_ages.mean() ) . 42.8 . # 분산의 동일성 검정 spst.levene(minnesota_ages, wisconsin_ages) # pvalue가 통상적인 기준인 0.1보다 크면 분산 동일하다는 가정 받아들임. . LeveneResult(statistic=0.028047686012903684, pvalue=0.8673418686154897) . spst.ttest_ind(minnesota_ages, wisconsin_ages, equal_var=True) # equal_var는 등분산 여부인데, 모르면 False . Ttest_indResult(statistic=-1.7083870793286842, pvalue=0.09073015386514256) . minnesota와 wisconsin 두 그룹이 동일할 경우, 다른(차이가 나는) 샘플 데이터를 볼 가능성은 p값인 0.09073015386514256 확률임. p값이 유의수준 5%보다 크기 때문에 두 그룹은 같다고 판단. . Paired T-Test . 한 집단에서 전-후 비교(Before-After) - 당연히 p-value 낮으면 다른 것 | . np.random.seed(11) before= spst.norm.rvs(scale=30, loc=250, size=100) after = before + spst.norm.rvs(scale=5, loc=-1.25, size=100) weight_df = pd.DataFrame({&quot;weight_before&quot;:before, &quot;weight_after&quot;:after, &quot;weight_change&quot;:after-before}) weight_df.describe() . weight_before weight_after weight_change . count 100.000000 | 100.000000 | 100.000000 | . mean 250.345546 | 249.115171 | -1.230375 | . std 28.132539 | 28.422183 | 4.783696 | . min 170.400443 | 165.913930 | -11.495286 | . 25% 230.421042 | 229.148236 | -4.046211 | . 50% 250.830805 | 251.134089 | -1.413463 | . 75% 270.637145 | 268.927258 | 1.738673 | . max 314.700233 | 316.720357 | 9.759282 | . plt.subplot(1,2,1) weight_df[&#39;weight_before&#39;].plot(kind = &#39;box&#39;, ylim = (100,300)) plt.subplot(1,2,2) weight_df[&#39;weight_after&#39;].plot(kind = &#39;box&#39;, ylim = (100,300)) . &lt;AxesSubplot:&gt; . . spst.ttest_rel(a = before, b = after) . Ttest_relResult(statistic=2.5720175998568284, pvalue=0.011596444318439857) . ANOVA(Analysis of Variance) . 귀무가설 : A,B,C 다 똑같다 대립가설 : A,B,C 중 ‘무언가 하나는’ 다를 것이다. . 대립가설 조심, A, B, C 중 뭐가 다르고, 얼마나 다르고 등은 전혀 알 수 없다. 따로 계산해야 한다. . [ 분산분석 검정의 가정사항 (assumptions of ANOVA test) ] . (1) 독립성: 각 샘플 데이터는 서로 독립이다. (2) 정규성: 각 샘플 데이터는 정규분포를 따르는 모집단으로 부터 추출되었다. (3) 등분산성: 그룹들의 모집단의 분산은 모두 동일하다. . np.random.seed(12) races = [&quot;asian&quot;,&quot;black&quot;,&quot;hispanic&quot;,&quot;other&quot;,&quot;white&quot;] # Generate random data voter_race = np.random.choice(a= races, p = [0.05, 0.15 ,0.25, 0.05, 0.5], size=1000) voter_age = spst.poisson.rvs(loc=18, mu=30, size=1000) # Group age data by race voter_frame = pd.DataFrame({&quot;race&quot;:voter_race,&quot;age&quot;:voter_age}) groups = voter_frame.groupby(&quot;race&quot;).groups print(voter_frame) # Etract individual groups asian = voter_age[groups[&quot;asian&quot;]] black = voter_age[groups[&quot;black&quot;]] hispanic = voter_age[groups[&quot;hispanic&quot;]] other = voter_age[groups[&quot;other&quot;]] white = voter_age[groups[&quot;white&quot;]] print(asian) # 첫번째 방법 Perform the ANOVA spst.f_oneway(asian, black, hispanic, other, white) . race age 0 black 51 1 white 49 2 hispanic 51 3 white 48 4 asian 56 .. ... ... 995 white 47 996 asian 40 997 white 50 998 white 51 999 hispanic 43 [1000 rows x 2 columns] [56 52 37 50 53 47 56 43 46 54 45 54 42 44 55 50 45 49 51 57 56 46 43 53 48 54 54 44 40 46 51 52 44 54 43 44 53 42 54 44 59 47 54 40] F_onewayResult(statistic=1.7744689357329695, pvalue=0.13173183201930463) . f통계량 1.774와 p값 0.1317은 각 그룹의 평균이 큰 차이가 없다는 것을 보여줌. . # 두번째 방법 statsmodels lib 사용 import statsmodels.api as sm from statsmodels.formula.api import ols model = ols(&#39;age ~ race&#39;, # Model formula data = voter_frame).fit() anova_result = sm.stats.anova_lm(model, typ=2) print (anova_result) . sum_sq df F PR(&gt;F) race 199.369 4.0 1.774469 0.131732 Residual 27948.102 995.0 NaN NaN . 정석적인 해석 : 귀무가설이 참일 때, 이러한 데이터가 관측될 확률은 1.85% 정도이다. (1.85%확률을 뚫고 이런 데이터가 관측될 수도 있다.) . 발칙한 해석 : 뭔가 하나는 차이가 날 확률이 98%는 넘는다. . from statsmodels.stats.multicomp import pairwise_tukeyhsd tukey = pairwise_tukeyhsd(endog=voter_age, # Data groups=voter_race, # Groups alpha=0.05) # Significance level tukey.plot_simultaneous() # Plot group confidence intervals plt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=&quot;red&quot;) plt.show() tukey.summary() # See test summary . C: Users Administrator anaconda3 lib site-packages statsmodels sandbox stats multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, &#39;&#39;)) . . Multiple Comparison of Means - Tukey HSD, FWER=0.05 group1 group2 meandiff p-adj lower upper reject . asian | black | -1.3353 | 0.5735 | -3.8242 | 1.1535 | False | . asian | hispanic | -0.7593 | 0.9 | -3.1315 | 1.6129 | False | . asian | other | -0.0264 | 0.9 | -3.0202 | 2.9674 | False | . asian | white | -1.4184 | 0.4336 | -3.6932 | 0.8564 | False | . black | hispanic | 0.576 | 0.8145 | -0.9362 | 2.0882 | False | . black | other | 1.309 | 0.5494 | -1.0622 | 3.6801 | False | . black | white | -0.0831 | 0.9 | -1.4374 | 1.2713 | False | . hispanic | other | 0.733 | 0.8996 | -1.5154 | 2.9813 | False | . hispanic | white | -0.6591 | 0.4974 | -1.7847 | 0.4665 | False | . other | white | -1.392 | 0.3912 | -3.5374 | 0.7533 | False | . Associative Type Test . Correlation Coefficient . 귀무가설 : X와 Y는 상관이 없다.(상관계수 = 0) 대립가설 : 상관계수가 0이 아니다. . X와 Y를 별도로 시각화 해 볼 것 | . spst.pearsonr(wisconsin_ages,minnesota_ages) # X와 Y의 상관계수와 p-value . (0.7749658250313621, 3.969124248684934e-11) . 결과는 튜플로 나오는데 | . 튜플의 첫 번째 값 : 상관계수를 뜻한다. 두 데이터의 선형성의 정도를 나타낸다. | p-value는 상관계수가 우연에 의해 일어나진 않았는지 판단한다. 귀무가설 : 상관 계수가 0이다. | 대립가설 : 상관 계수가 0이 아니다. | . | pvalue가 0.57로 0.05보다 크기 때문에 귀무가설 받아들임 피어슨상관계쑤는 -0.25이므로 약한 음의 상관관계가 있음 . 교차분석(Chisquare_test) . 티셔츠 구매여부와 반바지 구매여부는 관계가.. 있을까?! 귀무가설 : 티셔츠 구매와 바지 구매는 별개이다.(독립이다) 대립가설 : 티셔츠를 구매와 바지는 독립이 아니다.관련이 있다.. . # 고객별 셔츠와 바지의 구매 여부 데이터 shirt_raw = {&#39;shirts&#39;:[0,0,0,0,0,0,1,1,0,0,1,1,1,1,1,0,0,0,0,0,1], &#39;pants&#39;:[0,0,1,0,0,0,1,0,1,1,1,1,1,1,1,0,0,0,1,0,1] } shirt_df = pd.DataFrame(shirt_raw) print(shirt_df.head(10)) . shirts pants 0 0 0 1 0 0 2 0 1 3 0 0 4 0 0 5 0 0 6 1 1 7 1 0 8 0 1 9 0 1 . # 데이터의 Crosstable contingency = pd.crosstab(shirt_df[&#39;shirts&#39;], shirt_df[&#39;pants&#39;]) contingency . pants 0 1 . shirts . 0 9 | 4 | . 1 1 | 7 | . chiresult = spst.chi2_contingency(contingency) # 카이제곱 검정 . # 결과 : 튜플로 4개 값 출력됨 print(&quot;카이제곱통계량 : {}&quot;.format(chiresult[0])) print(&quot;p-value : {:.20f}&quot;.format(chiresult[1])) print(&quot;자유도 : {}&quot;.format(chiresult[2])) print(&quot;기대 빈도 분할표: n&quot;, chiresult[3] ) #귀무가설에 대한 기대빈도. . 카이제곱통계량 : 4.317941433566433 p-value : 0.03771251880967476516 자유도 : 1 기대 빈도 분할표: [[6.19047619 6.80952381] [3.80952381 4.19047619]] . 유의수준 0.05 하에 p-value가 매우 낮으므로 두 집단간 차이가 있다(바지 구매는 셔츠 구매와 관련이 있다) | pants 0과 pants 1 그룹을 비교했을 때 shirts 0, 1의 차이가 있다. pants가 0, 1, 2였다면 0, 1, 2에 따라 차이가 있다라고 해석할 수 있음 | .",
            "url": "https://nueees.github.io/techblog/statistics/python/2020/01/01/%EA%B8%B0%EB%B3%B8%ED%86%B5%EA%B3%841.html",
            "relUrl": "/statistics/python/2020/01/01/%EA%B8%B0%EB%B3%B8%ED%86%B5%EA%B3%841.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "추정",
            "content": "R과 함께하는 통계학의 이해 - 최용석 . . 6.1 통계적 추론 . 통계적 추론(statistical inference): 모집단의 수치적 특성을 나타내는 모수(parameter)에 대한 정보를 얻어내기 위한 일련의 과정 . 통계량(statistic)들의 값을 계산하고 이것을 이용하여 모집단의 특성(모수)를 알아보는 것이다. . 1) 추정(estimation): 모수에 대한 추측값을 얻되, 그 값의 정밀도를 함께 구하는 것 . 2) 가설 검정(hypotheses testing): 표본의 자료가 모수의 참값에 대한 조사자의 추측을 뒷받침하는지 혹은 반증하는지 결정하는 것 -&gt; 7장 . . 6.2 모평균에 대한 점추정 . 점추정: 모수의 참값과 유사할 것이라고 예상되는 하나의 값 제시 . 모집단의 크기가 $n$인 표본을 임의로 추출할 때 이를 $n$개의 확률변수 $X_1,X_2,…,X_n$으로 표현 추정하고자 하는 하나의 모수에 대하여 이들 $n$개의 확률변수를 이용하여 하나의 통계량을 만들고, 나아가 주어진 표본으로부터 그 실제값을 계산하여 하나의 수치를 제시하는 것 . 추정량(estimator): 모수를 추정하기 위해 만들어진 통계량 (ex. $ hat{ mu}, hat{ sigma} $) . 추정치(estimate): 주어진 표본으로부터 계산된 추정량의 실제값 . $ hat{ mu} = bar{X} = frac{1}{n}(X_1+X_2+…+X_n) = frac{1}{n} sum{X_i}$ . 여기서 $ hat{ mu}$는 모평균 $ mu$에 대한 추정량 . 이러한 추정량은 확률변수들로부터 만들어진 하나의 확률변수이므로 추출된 표본의 값에 따라 그 값(추정치)가 달라질 수 있다. . 표준오차(standard error, S.E.): 수치(추정치)들의 변동은 추정량의 정확도와 관계가 있는데, 이 정확도를 측정하기 위해 추정량의 표준편차를 계산한 것 . $ E( bar{X}) = mu , quad S.E.( bar{X}) = frac{ sigma}{ sqrt{n}} $ 따라서 표본평균 $ bar{X} $를 이용하여 모평균 $ mu$를 추정하고자 할 경우 표본의 크기 $n$이 클수록 표준오차가 작아져서 보다 정확한 추정이 가능하다. 하지만 모수인 $ sigma $(모집단의 표준편차)를 모르는 경우 계산할 수 없다. 표본의 표준편차 $ hat{ sigma} $를 이용하여 추정할 수 있다. . $ hat{ sigma} = s = sqrt{ frac{1}{n-1} sum{(X_i- bar{X})^2} } $ . 소나무 성장 연구를 위한 1년생 소나무 묘목 40그루의 크기를 조사한 자료 : 2.6, 1.9, 1.8, 1.6, 1.4, 2.2,…,1.2 전체 1년생 소나무 묘목의 평균 크기에 대한 추정치(= 표본평균($ bar{x}$)) 와 표준오차($ frac{s}{ sqrt{n}}$) : $ bar{x} = frac{1}{40} times (2.6+1.9+…+1.2) = 1.715 $ $ s^2 = frac{1}{40-1} times { (2.6 - 1.715)^2 + … + (1.2-1.715)^2 } = 0.2254 $ $ frac{s}{ sqrt{n}} = frac{ sqrt{0.2254} } { sqrt{40}} = 0.0751 $ | . . 6.3 모평균에 대한 구간추정 . 구간추정: 모수의 참값을 포함할 것으로 예상되는 적절한 구간을 제시 . . 6.4 모비율에 대한 추정 . .",
            "url": "https://nueees.github.io/techblog/statistics/2019/09/04/%EC%B6%94%EC%A0%95.html",
            "relUrl": "/statistics/2019/09/04/%EC%B6%94%EC%A0%95.html",
            "date": " • Sep 4, 2019"
        }
        
    
  
    
        ,"post34": {
            "title": "표집분포와 중심극한정리",
            "content": "R과 함께하는 통계학의 이해 - 최용석 . . 5.1 표집분포 . 모수(parameter): 모집단에 대한 수치적 특성값 (모평균, 모비율, 모분산…) . 통계량(statistic): 표본으로부터 획득한 수치적 정보 . 통계량은 그 자체가 하나의 확률변수로서 확률분포를 가지게 된다. . 표집분포(sampling distribution): 통계량이 가지는 확률분포 . 표본 추출과정에서 발생하는 통계량의 값이 가지는 변동은 이 표집분포에 의해 설명될 수 있다. 표집분포는 모집단의 분포에 영향을 받기도 하고 표본의 크기 $n$에도 영향을 받는다. . 확률표본(random sample): 크기가 큰 모집단으로부터 임의 추출된 크기 $n$의 표본 $X_1,X_2,…,X_n$ . $X_1,X_2,…,X_n$은 서로 독립이고 모집단의 분포와 같은 분포를 가진다. . . 5.2 표본평균의 분포와 중심극한정리 . 크기가 $n$인 확률표본 $X_1,X_2,…,X_n$에 대해 $ E(X) = mu $, $ Var(X) = sigma^2 $, $i=1,2,…,n$이고, . $ bar{X} = frac{1}{n} sum{X_i} $일 때, . 표본평균 $ bar{X}$의 기대값과 분산 . $ E( bar{X}) = mu $ $ Var( bar{X}) = frac{ sigma^2}{n} $ . 정규모집단으로부터의 표본평균에 대한 확률분포 . 크기가 $n$인 확률표본 $X_1,X_2,…,X_n$에 대해 $ X_i sim N( mu, sigma^2) $, $i=1,2,…,n$일때, . 표본평균 $ bar{X}$의 확률분포는 $ bar{X} sim N( mu, frac{ sigma^2}{n}) $를 따르게 된다. . 중심극한정리 : 평균이 $ mu$이고 분산이 $ sigma^2$인 모집단으로부터 추출한 크기 $n$의 확률표본의 표본평균 $ bar{X}$는 표본의 크기가 큰 경우(보통 30 이상), 근사적으로 $ mu$이고 분산이 $ frac{ sigma^2}{n}$인 정규분포를 따르게 된다. . .",
            "url": "https://nueees.github.io/techblog/statistics/2019/09/03/%ED%91%9C%EC%A7%91%EB%B6%84%ED%8F%AC%EC%99%80-%EC%A4%91%EC%8B%AC%EA%B7%B9%ED%95%9C%EC%A0%95%EB%A6%AC.html",
            "relUrl": "/statistics/2019/09/03/%ED%91%9C%EC%A7%91%EB%B6%84%ED%8F%AC%EC%99%80-%EC%A4%91%EC%8B%AC%EA%B7%B9%ED%95%9C%EC%A0%95%EB%A6%AC.html",
            "date": " • Sep 3, 2019"
        }
        
    
  
    
        ,"post35": {
            "title": "연속확률변수 및 분포",
            "content": "R과 함께하는 통계학의 이해 - 최용석 . . 4.1 연속확률변수의 확률분포함수 . 연속확률변수: 확률변수가 특정 구간의 모든 값을 다 가질 수 있기 때문에 가질 수 있는 값들을 일일이 지칭할 수 없는 확률변수를 의미 . 연속확률분포함수: 확률변수 $X$가 가질 수 있는 특정 구간에서 확률이 어떻게 분포하는가를 나타낼 수 있는 함수 . $X$의 확률분포는 확률의 밀도를 나타내는 확률밀도함수 . 확률밀도함수(probability density function) . 모든 $x$에 대해 $f(x) geq 0 $ | $P(a leq X leq b) = int_{a}^{b}{f(x)dx}$ | $P( infty leq X leq - infty) = int_{ infty}^{- infty}{f(x)dx} = 1$ | . 연속확률변수 $X$가 특정한 값 $x$를 갖게 되는 확률은 0이므로, 구간의 확률을 구할 때는 그 구간의 경계점의 포함 유무는 영향을 받지 않는다. . 연속확률변수의 기대값과 분산 . $ E(X) = int{xf(x)dx} $ $ Var(X) = int{(x- mu)^2f(x)dx} = sigma^2 $ . . 4.2 정규분포 . 정규분포(normal distribution): 좌우대칭의 종모양 곡선 (=Gaussian distribution) . 정규확률변수 : 종 모양의 확률밀도함수를 가지는 연속형확률변수 X . 정규확률변수의 확률분포에 대한 식은 모집단에 대한 평균 $ mu$와 분산 $ sigma^2$에 의존하므로 $ X sim N( mu, sigma^2) $ 로 표기 . . 4.3 정규분포의 확률계산 . 표준정규분포(standard normal distribution): 평균이 0이고, 분산이 1인 정규분포 . 일반적으로 표준정규분포를 따르는 확률변수는 $Z$로 표현 $ Z sim N(0,1) $이므로 확률변수 $Z$는 평균 $E(Z) = mu_z = 0 $이며, 분산은 $ Var(Z) = sigma_z^2 = 1 $과 같다. . 일반적인 정규분포를 따르는 확률변수 $X$에 대한 확률을 표준정규분포를 따르는 확률변수 $Z$로 변환하는 과정을 표준화(Standardization)라고 한다. . $ frac{X- mu_x}{ sigma_x} $ . . 4.4 이항분포의 정규근사 . . $n$이 증가함에 따라 분포의 형태가 점차 좌우대칭의 종 모양에 가까워짐을 확인할 수 있다. . 이항분포의 정규근사 . 확률변수 $X$가 성공 횟수를 나타내는 이산형 확률변수이고 $X sim Bin(n,p) $일 때, $np$나 $n(1-p)$이 모두 충분히 클 경우(보통 10 이상)에 확률변수 $X$는 근사적으로 다음의 정규 분포를 따르게 된다. $ X sim N(np, np(1-p)) $ . .",
            "url": "https://nueees.github.io/techblog/statistics/2019/09/02/%EC%97%B0%EC%86%8D%ED%99%95%EB%A5%A0%EB%B3%80%EC%88%98-%EB%B0%8F-%EB%B6%84%ED%8F%AC.html",
            "relUrl": "/statistics/2019/09/02/%EC%97%B0%EC%86%8D%ED%99%95%EB%A5%A0%EB%B3%80%EC%88%98-%EB%B0%8F-%EB%B6%84%ED%8F%AC.html",
            "date": " • Sep 2, 2019"
        }
        
    
  
    
        ,"post36": {
            "title": "이산확률변수 및 분포",
            "content": "R과 함께하는 통계학의 이해 - 최용석 . . 3.1 사건의 확률 . 확률(probability) : 실험의 결과에 대해 확신하는 정도를 수치적으로 나타는 척도 . 사건(event) : 어떤 특성을 갖는 결과들의 집합을 ($A$, $B$, …) . P(A)=frac사건A에속하는결과수표본공간에속하는결과수P(A) = frac{사건A에 속하는결과 수}{표본공간에 속하는 결과 수}P(A)=frac사건A에속하는결과수표본공간에속하는결과수 . . 3.2. 확률변수 . 확률변수(random variable) : 표본공간에 속하는 각각의 결과들에 대해 실수값 대응 시켜준 변수 ($X$, $Y$, …) . 한주에 경기 수 (0회, 1회, 2회) | . 확률변수가 가지는 특정값 : ($x$, $y$, …) . 가질 수 있는 값에 따라, . 1) 이산확률변수(discrete random variable) 2) 연속확률변수(continuous random variable) . . 3.3 이산확률변수의 확률분포함수 . 확률분포(probability distribution) : 확률변수가 가지는 값과 그 값을 가질 확률을 정해주는 규칙 . 한주의 경기 수와 그 비율 (0회: $ frac{2}{10}$, 1회: $ frac{5}{10}$, 2회: $ frac{3}{10}$) | . (이산확률변수의) 확률분포함수(probability distribution function) : . $ f(x)= P(X=x) $ . 이산 확률분포함수의 성질 . 모든 $x$값에 대해 $0 leq f(x) leq 1$ $ sum f(x) =1 $ . . 3.4 확률변수의 기대값과 표준편차 . 확률변수의 기대값(expected value) : 확률변수가 가질 수 있는 값들에 대한 확률분포 상의 중심위치 . $ E(X) = sum xf(x)$ . 평균적으로 한주에 수행하는 경기 수 ($ 0회 times frac{2}{10} + 1회 times frac{5}{10} + 2회 times frac{3}{10} = 1.1회$ ) | . 평균과 다른 점? 기대값은 동일한 실험을 무수히 반복했을 때의 평균을 의미하고 10회 던져서 8번 나왔다고 0.8이라고 하지 않음 실수값을 갖는 확률변수에 대해서, 모평균은 확률변수의 기대값이 된다. . 기대값의 성질 . $ E(X) = mu $ $ E(a) = a $ $ E(aX) = a mu $ $ E(aX pm b) = a mu pm b$ . 확률변수의 분산(variance) . 평균적으로 한주에 수행하는 경기 수의 분산 ($ (0회-1.1회)^2 times frac{2}{10} + (1회-1.1회)^2 times frac{5}{10} + (2회-1.1회)^2 times frac{3}{10} = 0.49 $ ) | . $ Var(X) = sum (x- mu)^2f(x)$ . $ Var(X) = E[ (X - E(X) )^2 ] $ $ Var(X) = E(X^2) - E(X)^2 $ . . 분산의 성질 . $ Var(X) = sigma^2 $ $ Var(a) = 0 $ $ Var(aX) = a^2 sigma^2 $ $ Var(aX pm b) = a^2 sigma^2 $ . 확률변수의 표준편차(standard deviation) . $ sqrt{ Var(X) } = sqrt{ sum (x- mu)^2f(x) } $ . . 3.5 이항분포 . 3.3에서 확률분포란 확률변수가 가지는 값과 그 값을 가질 확률을 정해주는 규칙이라고 정의하였는데, 규칙이 밝혀져 이름이 부여된 것들이 있는데 대표적인 예가 이항분포 . 베르누이 시행 : 단 1회의 실험 지칭 . 베르누이 시행의 특징 . 각 시행은 성공(success, $S$ )과 실패(fail, $F$ )의 두 가지 결과만을 갖는다. | 각 시행에서 성공할 확률은 $P(S)$ , 실패할 확률은 $P(F)$ 로 매 시행마다 동일하다. | 각 시행은 상호 독립으로 각각의 시행이 다른 시행의 결과에 영향을 미치지 않는다. | . 성공 확률이 $p$인 베르누이 시행을 $n$번 시행한 경우 성공 횟수를 나타내는 확률변수 $X$의 확률분포함수는 . $ f(x) = P(X=x) = _{n}C_{x}p^x(1-p)^{n-x} , x=0,1,…,n$ . 이항부포의 기대값과 분산 . 성공 횟수를 나타내는 확률변수 $X$가 $X ~ Bin(n,p)$일 때, $ E(X) = np $ $ E(X) = np(1-p) $ . .",
            "url": "https://nueees.github.io/techblog/statistics/2019/09/01/%EC%9D%B4%EC%82%B0%ED%99%95%EB%A5%A0%EB%B3%80%EC%88%98-%EB%B0%8F-%EB%B6%84%ED%8F%AC.html",
            "relUrl": "/statistics/2019/09/01/%EC%9D%B4%EC%82%B0%ED%99%95%EB%A5%A0%EB%B3%80%EC%88%98-%EB%B0%8F-%EB%B6%84%ED%8F%AC.html",
            "date": " • Sep 1, 2019"
        }
        
    
  
    
        ,"post37": {
            "title": "Internet",
            "content": "📎 Coding Everybody . . 1.1. IP &amp; Domain . IP: 각 장치를 나타내는 IP 주소를 가리키는 말 | Domain: 네트워크상에서 컴퓨터를 식별하는 호스트명 | . IP 기억하기 어렵기 때문에 Domain Name 사용 Domain name -&gt; IP Address로 접속 . DNS (Domain Name Server): 도메인별 IP 정보를 갖고 있는 서버 인터넷에 있는 Name Server에 접속해서 google.com의 IP를 알아낼 수 있음 | . . 1.2. Public &amp; Private IP . public ip: ISP(Internet Service Provider)가 제공하는 인터넷 상에서의 컴퓨터의 주소 | private ip: 어떠한 네트워크(Network) 안에서 사용되는 주소 | . 컴퓨터 여러대 있는 가정의 경우, 공유기(Router)가 설치되어 있으면 IP 접속 실패 Router IP (public IP)에 연결되는 여러대 컴퓨터들(private IP)은 Port forwarding이 필요 . . 1.3. Port . 한 server의 port마다 다른 소프트웨어로 접속됨 | . 예를들어 80번에 있는건 http (web server) port list 확인 . . 1.4. Port Forwarding . 공유기 관리자 192.168.219.1(U+Net) 접속 (ipconfig해서 나오는 무선 LAN 어댑터 기본 게이트웨이) | 고급설정 &gt; NAT 설정 &gt; Port Forwarding | . 80포트로 들어올 때, 80으로 포워딩 되게 내 IPv4 주소 넣고 설정 . . 1.5. DHCP &amp; DDNS . IP 고갈로 인해 IPv4(0.0.0.0 ~ 255.255.255.255) 더이상 사용하기 어려움 현재는 IPv4와 IPv6를 동시에 사용 중 오랫동안 사용하지 않으면 public IP를 회수하고 다시 사용시 새로운 IP로 재할당 . 동적 호스트 구성 프로토콜(Dynamic Host Configuration Protocol, DHCP): 장비에 고정적으로 IP를 부여하지 않고 컴퓨터를 사용할 때 남아 있는 IP 중에서 돌아가면서 부여하는 IP . | 동적 DNS (Dynamic Domain Name System, DDNS): 실시간으로 DNS를 갱신하는 방식으로 도메인과 호스트의 IP를 지속적을 일치화 시켜줌 . | .",
            "url": "https://nueees.github.io/techblog/internet/network/2015/04/15/internet.html",
            "relUrl": "/internet/network/2015/04/15/internet.html",
            "date": " • Apr 15, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Seeeun Cho . @nueees . CONTACT . Phone: | Email: nueees@gmail.com | Linkedin: https://www.linkedin.com/in/nueees/ | . EXPERTISE . Program languages: SQL, Java, Javascript, Python, R | Frameworks: Spring, MDD (Model-Driven Development) | Version control systems: SVN, Git, frism | Workflow Control: Job-PaSS | DBMSs: Oracle, Mysql | OSs: Linux, Windows | ETL Tools: TDS, TeraStream, BXI, Oracle GoldenGate, DataPump | . EXPERIENCE . Junior data engineer &amp; developer, JT savings bank | Mar 2020 - Sep 2021 | Junior database engineer, Serends IT service | Feb 2019 - Feb 2020 | Java full stack developer, Kyobo information and communication | Sep 2017 - Nov 2018 | . .",
          "url": "https://nueees.github.io/techblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nueees.github.io/techblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}