---
toc: true
layout: post
description: section1
categories: [spark]
title: Spark
---
 
ğŸ“ [Spark User Guide](https://spark.apache.org/docs/latest/api/python/index.html), Spark The Definitive Guide

---

# 1. Spark ê°œìš”

Unified Computing Engine and a Set of Libraries for Parallel Data Processing on Computer Clusters  
ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ìì›ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ì‚° ë³‘ë ¬ ì²˜ë¦¬    


## 1.1. Spark ì£¼ìš” ìš©ë„
- Spark SQL (Batch processing): big & complex, processing massive data, higher latencies
- Spark Streaming (Real time analytics): relatively simple and generally independent, one at a time processing, sub-second latency  
- MLlib (Machine learning)
- GraphX (Graph processing)  

_spark streamingì˜ ê²½ìš° ê°œë°œí•˜ê¸°ëŠ” ì–´ë µì§€ ì•Šìœ¼ë‚˜, ìš´ì˜(debugging)ì´ ì–´ë ¤ì›€_


## 1.2. Hadoop vs Spark
Hadoopì˜ MPP[^1] (Massive Parallel Processing)ì„ ê¸°ë°˜ìœ¼ë¡œ ìì› ëŒ€ìš©ëŸ‰ ì²˜ë¦¬í•˜ê³   
ê¸°ì¡´ Hadoopì˜ MapReduceì—ì„œ disk I/O ë¶€í•˜ê°€ ì‹¬í–ˆë˜ ë¶€ë¶„ì„ memoryì—ì„œ ë¹ ë¥´ê²Œ ì‘ì—…   

- RDD -> DataFrame -> SQL ë°ì´í„° ì²˜ë¦¬ ì¶”ìƒí™”
- DAG ê¸°ë°˜ì˜ ì‹¤í–‰ ìµœì í™”
- In-memory writeìœ¼ë¡œ Map Reduce ì‘ì—…ì„ ìµœì í™”

[^1]: DW workloadë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ìì£¼ ì‚¬ìš© <->SMP (Symmetric Multi processor)


## 1.3. RDD (Resilient Distributed Datasets)  
Immutable distributed collection of your data, partitioned across nodes in your cluster  
- ë¶„ì‚° ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„°ë¥¼ ì¶”ìƒí™”  
- Immutable ë°ì´í„° ì§‘í•©  
- í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ì„œë¡œ ë‹¤ë¥¸ ë…¸ë“œì— ì €ì¥í•˜ê³  ì½ìŒ  

### RDD vs DF vs DataSet(SQL)  
[ì¶œì²˜_loustler ë¸”ë¡œê·¸](https://loustler.io/data_eng/spark-rdd-dataframe-and-dataset/)
<table>
  <tbody>
    <tr>
      <td>&nbsp;</td>
      <td><strong>RDD</strong></td>
      <td><strong>Data Frame</strong></td>
      <td><strong>DataSet</strong></td>
    </tr>
    <tr>
      <td>Release</td>
      <td><code class="language-plaintext highlighter-rouge">1.0</code></td>
      <td><code class="language-plaintext highlighter-rouge">1.3</code></td>
      <td><code class="language-plaintext highlighter-rouge">1.6</code></td>
    </tr>
    <tr>
      <td>Data Representation</td>
      <td>Javaë‚˜ Scala object</td>
      <td><em>column</em>ì˜ ì´ë¦„ì´ ìˆëŠ” organizedëœ ë¶„ì‚° ì»¬ë ‰ì…˜. RDB í…Œì´ë¸”ê³¼ ìœ ì‚¬</td>
      <td><strong>Data Frame</strong>ì˜ í™•ì¥. type-safe, oop interface ì œê³µ, Catalyst query optimizer ì˜ ì„±ëŠ¥ ì´ì , off heap storage ë©”ì»¤ë‹ˆì¦˜ ì œê³µ</td>
    </tr>
    <tr>
      <td>Data Format</td>
      <td>ë¹„ì •í˜•/ì •í˜•ë°ì´í„°ì˜ ì‰½ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬. <strong>Data Frame</strong>ê³¼ <strong>DataSet</strong>ì²˜ëŸ¼ ìŠ¤í‚¤ë§ˆë¥¼ ìœ ì¶”í•˜ì§€ ì•ŠìŒ</td>
      <td>ë¹„ì •í˜•/ë°˜ì •í˜• ë°ì´í„°ì—ì„œë§Œ ë™ì‘. <em>column name</em>ì„ ê°€ì§„ í˜•íƒœë¡œ <strong>Spark</strong>ê°€ ìŠ¤í‚¤ë§ˆë¥¼ ê´€ë¦¬</td>
      <td><strong>DataFrame</strong>ê³¼ ê°™ì´ ì •í˜•/ë°˜ì •í˜• ë°ì´í„°ì˜ ì‰½ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬. <em>row</em>ì˜ JVM object í˜•íƒœë‚˜ <em>row object collection</em> í˜•íƒœë¡œ í‘œí˜„. <strong>encoder</strong>ë¥¼ í†µí•´ í…Œì´ë¸” í˜•íƒœë¡œ í‘œí˜„</td>
    </tr>
    <tr>
      <td>Data Source API</td>
      <td>ì–´ë–¤ ë°ì´í„°ì†ŒìŠ¤ë“  ì‚¬ìš©ê°€ëŠ¥</td>
      <td><strong>AVRO</strong>, <strong>CSV</strong>, <strong>TSV</strong>, <strong>JSON</strong>, <strong>HDFS</strong>, <strong>HIVE Table</strong>, <strong>RDB</strong> ë“±ì˜ ë°ì´í„° ì†ŒìŠ¤ê°€ ê°€ëŠ¥</td>
      <td>ì–´ë–¤ ë°ì´í„°ì†ŒìŠ¤ë„ ê°€ëŠ¥</td>
    </tr>
    <tr>
      <td>immutablility, interoperability</td>
      <td><strong>RDD</strong>ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¶ˆë³€. <strong>RDD</strong>ê°€ í‘œí˜•ì‹ì¼ê²½ìš° <code class="language-plaintext highlighter-rouge">RDD.toDF</code>ë¡œ <strong>DataFrame</strong>ìœ¼ë¡œ ë³€ê²½ê°€ëŠ¥</td>
      <td><strong>DataFrame</strong>ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë‚˜ë©´ domain objectë¥¼ ì¬ìƒì„±í•  ìˆ˜ ì—†ìŒ. <code class="language-plaintext highlighter-rouge">RDD.toDF</code>ë¥¼ í• ê²½ìš° ì›ë˜ í˜•íƒœì˜ <code class="language-plaintext highlighter-rouge">RDD</code>ë¡œ ëŒì•„ì˜¬ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒ</td>
      <td><strong>DataFrame</strong>ì˜ í™•ì¥ë²„ì „ìœ¼ë¡œ <strong>RDD</strong>ì™€ <strong>DataFrame</strong>ì„ <strong>DataSet</strong>ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŒ</td>
    </tr>
    <tr>
      <td>compile time type safety</td>
      <td>ì¹œìˆ™í•œ OOP ìŠ¤íƒ€ì¼ê³¼ compile-time typesafey ì œê³µ</td>
      <td><strong>DataFrame</strong>ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” <em>column</em>ì— ì ‘ê·¼í•˜ë ¤ê³  í•˜ë©´ compile-time typesafeë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ. run-timeì—ì„œë§Œ í™•ì¸ê°€ëŠ¥</td>
      <td>compile-time safe ì§€ì›</td>
    </tr>
    <tr>
      <td>Optimization</td>
      <td>Optimization engineì´ ì—†ìŒ. ê°œë°œìê°€ ìµœì í™”í•´ì•¼ ë¨</td>
      <td>Catalyst Optimizerê°€ ìˆìŒ. ì´ë¥¼ ì´ìš©í•´ì„œ ìµœì í™” ì§„í–‰</td>
      <td>ì¿¼ë¦¬ ê³„íš ìµœì í™”ë¥¼ ìœ„í•´ <strong>DataFrame</strong>ì˜ Catalyst optimizationì´ ìˆìŒ</td>
    </tr>
    <tr>
      <td>Serialization</td>
      <td>Java Serization ì‚¬ìš©</td>
      <td>Off Heap storageë¥¼ ì‚¬ìš©(InMemory)í•˜ì—¬ binary formatìœ¼ë¡œ. Schemaë¥¼ ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥. Tungsten ì‹¤í–‰ ë°±ì—”ë“œë¥¼ ê°€ì§€ê³  ìˆì–´ì„œ ë©”ëª¨ë¦¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ë™ì ìœ¼ë¡œ bytecodeë¥¼ ë§Œë“¬</td>
      <td><strong>Encoder</strong>ê°€ ìˆê¸° ë•Œë¬¸ì— Spark ë‚´ë¶€ì˜ Tungsten binary format ì‚¬ìš©</td>
    </tr>
    <tr>
      <td>GC(Garbage Collection)</td>
      <td>ê° Objectì˜ ìƒì„±ê³¼ íŒŒê´´ë¡œ ì¸í•œ GCì˜ overheadê°€ ìˆìŒ</td>
      <td>ê° <em>row</em>ì˜ ê°œë³„ Objectë¥¼ êµ¬ì„±í•  ë•Œ GC ì½”ìŠ¤íŠ¸ ë°©ì§€</td>
      <td>Serializationë•Œë¬¸ì— GCê°€ OBjectë¥¼ íŒŒê´´í•  í•„ìš”ê°€ ì—†ìŒ. <em>off heap data serialization</em>ì„ ì‚¬ìš©í•¨</td>
    </tr>
    <tr>
      <td>Efficiency / Memory use</td>
      <td>Javaì™€ Scala objectì—ì„œ ê°œë³„ì ìœ¼ë¡œ Serializationì„ í•˜ë©´ íš¨ìœ¨ì„±ì´ ì €í•˜ë¨</td>
      <td>Serializationì— <code class="language-plaintext highlighter-rouge">off heap memory</code>ë¥¼ ì‚¬ìš©í•˜ë©´ overheadê°€ ì¤„ì–´ë“¬.</td>
      <td>Serializaed dataì— ëŒ€í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŒ</td>
    </tr>
    <tr>
      <td>Lazy Evaluation</td>
      <td>ê¸°ë³¸ì ìœ¼ë¡œ lazy Evaluation. <strong>Transformation</strong>ì˜ ê²½ìš° ì´ê²ƒì„ ì‹¤í–‰í–ˆë‹¤ëŠ” ê²ƒë§Œ ê¸°ì–µí•˜ê³  <strong>Action</strong>ì´ <strong>Driver Program</strong>ì— ê²°ê³¼ë¥¼ ë³´ë‚¼ í•„ìš”ê°€ ìˆì„ ë•Œë§Œ ê³„ì‚°</td>
      <td>Lazy Evaluation. <strong>Action</strong>ì´ ë‚˜íƒ€ë‚  ë•Œë§Œ ë™ì‘í•¨</td>
      <td><strong>RDD</strong>ë‚˜ <strong>DataFrame</strong>ê³¼ ê°™ìŒ</td>
    </tr>
    <tr>
      <td>Language Support</td>
      <td>Java, Scala, Python, R</td>
      <td><strong>RDD</strong>ì™€ ê°™ìŒ</td>
      <td>Scalaì™€ Java. Spark 2.1.1 ê¸°ì¤€</td>
    </tr>
    <tr>
      <td>Schema Projection</td>
      <td>ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©ë¨. ìŠ¤í‚¤ë§ˆ ì •ì˜ê°€ í•„ìš”í•¨</td>
      <td>ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ìŠ¤í‚¤ë§ˆë¥¼ ê²€ìƒ‰í•˜ê³  ìˆ˜í–‰í•¨. Hiveì˜ ê²½ìš° Hive Meta store, DBì˜ ê²½ìš° DB Engine.</td>
      <td>Spark SQL Engineì„ ì‚¬ìš©í•˜ì—¬ ìë™ íƒìƒ‰</td>
    </tr>
    <tr>
      <td>Aggregation</td>
      <td>ëŠë¦¼</td>
      <td>ëŒ€ê·œëª¨ ë°ì´í„° ì…‹ì— ëŒ€í•œ ì§‘ê³„ í†µê³„ë¥¼ ì‘ì„±í•˜ê¸° ë•Œë¬¸ì— ë¹ ë¥´ê²Œ ë¶„ì„í•¨</td>
      <td>ë¹ ë¦„</td>
    </tr>
    <tr>
      <td>Use-cases</td>
      <td>low-level ì‘ì—…ì´ í•„ìš”í•˜ê±°ë‚˜ ì¶”ìƒí™”ê°€ í•„ìš”í•œ ê²½ìš°ì— ì‚¬ìš©</td>
      <td>high-level ì‘ì—…ì´ í•„ìš”í•˜ê±°ë‚˜ ì •í˜•/ë°˜ì •í˜• ë“± ë§ì€ ì¼€ì´ìŠ¤</td>
      <td><strong>DataFrame</strong>ê³¼ ë™ì¼</td>
    </tr>
  </tbody>
</table> 



### RDD ì˜ˆì‹œ

low level interface, Data Containers  
ê°ê¸° ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ìš”ì†Œë“¤ì„ ì¶”ìƒí™”í•œ ê°™ì€ í˜•íƒœ  
```
# Hadoopì—ì„œ ì½ì€ RDD
path = hdfs://...   

# Filteredëœ RDD
func = contains(...)   

# Mappedëœ RDD
func = split(...)   
```
Faultë˜ë©´ Lossëœ ë°ì´í„° ì´ì „ë¶€í„° ë‹¤ì‹œ ê³„ì‚°

```
data = sc.textFile(...).split("\t")
data.map(lambda x: (x[0], [int(x[1]), 1]))
    .reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]])
    .map(lambda x: [x[0], x[1][0] / x[1][1]])
    .collect()
```

### DataFrame ì˜ˆì‹œ
ì½”ë“œê°€ ì§ê´€ì ì´ê³  schemaë¥¼ ê°€ì§€ëŠ” interface

```
sqlCtx.table("people")
    .groupBy("name")
    .agg("name", avg("age"))
    .collect()
```

### DataSet ì˜ˆì‹œ
SQLë¡œ ìˆ˜í–‰ ê°€ëŠ¥  

```
SELECT name, avg(age) 
FROM people 
GROUP BY name
```

<br><br>
---

# 2. Spark Architecture
![]({{site.baseurl}}/images/post/spark-cluster-overview.png)

executor ê°œìˆ˜ì™€ resource í• ë‹¹  

## 2.1. Cluster Manager
workerì™€ executors ì‚¬ì´ì— ìì›ì„ ì¤‘ê³„í•´ì£¼ëŠ” ì—­í•   
resourceë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë°°  

ex) Spark StandAlone(cluster X), (Hadoop)Yarn, Mesos, Kubernetes

## 2.2. Driver Process
SparkContextë¥¼ ìƒì„±í•˜ê³  RDDë¥¼ ë§Œë“¤ê³  operationì„ ì‹¤í–‰í•˜ëŠ” í”„ë¡œì„¸ìŠ¤  

Spark-submitì„ í•˜ë©´,  
Spark Driverê°€ Cluster Managerë¡œë¶€í„° Executor ì‹¤í–‰ì„ ìœ„í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ìš”ì²­  
Spark ContextëŠ” ì‘ì—… ë‚´ìš©ì„ task ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ Executorë¡œ ë³´ëƒ„


## 2.3. Executors
ì£¼ì–´ì§„ ì‘ì—…ì˜ ê°œë³„ taskë“¤ì„ ì‹¤í–‰í•˜ëŠ” ì‘ì—… ì‹¤í–‰í•˜ê³  ê²°ê³¼ return

1) ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì„±í•˜ëŠ” ì‘ì—…ë“¤ì„ ì‹¤í–‰í•˜ì—¬ driverì— ê·¸ ê²°ê³¼ë¥¼ return  
2) ê° executor ì•ˆì— ì¡´ì¬í•˜ëŠ” **block manager**ë¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ì í”„ë¡œê·¸ë¨ì—ì„œ ìºì‹œí•˜ëŠ” RDDë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œë¥¼ ì œê³µ   

Python,R Process <-> JVM(Spark session) -> Executors  

End to End: csv file read(**narrow**) -> DataFrame sort(**wide**) -> Array  



### Operations
1) Transformations: map, filter, groupBy, join
lazy operationìœ¼ë¡œ ì¦‰ì‹œ ì‹¤í–‰í•˜ëŠ” ë‹¨ê³„ê°€ ì•„ë‹˜

2) Actions: count, collect, save
ì‹¤ì œë¡œ ì‹¤í–‰í›„ driverë¡œ ê²°ê³¼ return


### Variables
- Accumulator: aggregate multiple values as it progresses
- Broadcast: large read-only variable shared across tasks, operations, large lookup tables

_ë””ë²„ê¹…ì´ë‚˜ ê²€ì¦ìš©ìœ¼ë¡œ ì“°ê³ , ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” accumulator, broadcast ì§€ì–‘ (GCë¬¸ì œ..)_



## 2.4. Scheduling

![]({{site.baseurl}}/images/post/DAG scheduler.jpeg)

### DAG (Directed Acyclic Graphs) Scheduler


<br><br>
---

# 3. Spark SQL & DataFrame

## 3.1. Pandas DF vs Spark DF

Pandas DF: ë³‘ë ¬ cpu X ë‹¨ì¼ ì„œë²„ memoryë¡œë§Œ ì²˜ë¦¬  
numpyì˜ ndarray ê¸°ë°˜ìœ¼ë¡œ 2ì°¨ì› ë°ì´í„° ë¶„ì„ìš©

Spark DF: SQL ì—°ì‚° ì§€ì› (df.select('col1'), df.groupBy('col1').count(), df.withColumns('col1'))  
immutableí•˜ê²Œ df.drop('col1') ì‹œì— ìƒˆë¡œìš´ df ë°˜í™˜ (inplace ì˜µì…˜ ì—†ìŒ)  
ì»¬ëŸ¼ ì§€ì • í˜¹ì€ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ì‹œ ëŒ€ê´„í˜¸ ì—°ì‚°ì \[\] ì‚¬ìš© X

### ê¸°ë³¸

```
select gender from tab
->  spark_df.select(â€˜genderâ€™) 

select * from tab where gender = â€˜Fâ€™
->  spark_df.filter(spark_df[â€˜genderâ€™] == â€˜Fâ€™) 

update set age=age + 10 from tab
->  spark_df.withColumns(â€˜ageâ€™, col(â€˜ageâ€™)+10) 

select gender, count(*) from tab group by gender
->  spark_df.groupBy(â€˜genderâ€™).count() 

```

### ë³€í™˜
```
spark_df = spark.createDataFrame(pandas_df) # spark to pandas
pandas_df = spark_df.select(â€˜*â€™).toPandas() # pandas to spark
```


### ì •ë ¬





<br><br>
---

# 4. MLlib Classification


<br><br>
---

# 5. MLlib Regression


<br><br>
---

# 6. Use Case


<br><br>
---

