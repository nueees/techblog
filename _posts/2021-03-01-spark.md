---
toc: true
layout: post
description: section1
categories: [spark]
title: Spark
---
 
📎 [Spark User Guide](https://spark.apache.org/docs/latest/api/python/index.html), Spark The Definitive Guide

---

# 1. Spark 개요

Unified Computing Engine and a Set of Libraries for Parallel Data Processing on Computer Clusters  
대량 데이터 처리를 위한 클러스터 자원으로 데이터를 분산 병렬 처리    


## 1.1. Spark 주요 용도
- Spark SQL (Batch processing): big & complex, processing massive data, higher latencies
- Spark Streaming (Real time analytics): relatively simple and generally independent, one at a time processing, sub-second latency  
- MLlib (Machine learning)
- GraphX (Graph processing)  

_spark streaming의 경우 개발하기는 어렵지 않으나, 운영(debugging)이 어려움_


## 1.2. Hadoop vs Spark
Hadoop의 MPP[^1] (Massive Parallel Processing)을 기반으로 자원 대용량 처리하고  
기존 Hadoop의 MapReduce에서 disk I/O 부하가 심했던 부분을 memory에서 빠르게 작업   

- RDD -> DataFrame -> SQL 데이터 처리 추상화
- DAG 기반의 실행 최적화
- In-memory write으로 Map Reduce 작업을 최적화

[^1]: DW workload를 처리하기 위해 자주 사용 <->SMP (Symmetric Multi processor)


## 1.3. RDD (Resilient Distributed Datasets)  
Immutable distributed collection of your data, partitioned across nodes in your cluster  
- 분산 병렬 처리를 위한 데이터를 추상화  
- Immutable 데이터 집합  
- 클러스터 내의 서로 다른 노드에 저장하고 읽음  

### RDD vs DF vs DataSet(SQL)  
[출처_loustler 블로그](https://loustler.io/data_eng/spark-rdd-dataframe-and-dataset/)
<table>
  <tbody>
    <tr>
      <td>&nbsp;</td>
      <td><strong>RDD</strong></td>
      <td><strong>Data Frame</strong></td>
      <td><strong>DataSet</strong></td>
    </tr>
    <tr>
      <td>Release</td>
      <td><code class="language-plaintext highlighter-rouge">1.0</code></td>
      <td><code class="language-plaintext highlighter-rouge">1.3</code></td>
      <td><code class="language-plaintext highlighter-rouge">1.6</code></td>
    </tr>
    <tr>
      <td>Data Representation</td>
      <td>Java나 Scala object</td>
      <td><em>column</em>의 이름이 있는 organized된 분산 컬렉션. RDB 테이블과 유사</td>
      <td><strong>Data Frame</strong>의 확장. type-safe, oop interface 제공, Catalyst query optimizer 의 성능 이점, off heap storage 메커니즘 제공</td>
    </tr>
    <tr>
      <td>Data Format</td>
      <td>비정형/정형데이터의 쉽고 효율적인 처리. <strong>Data Frame</strong>과 <strong>DataSet</strong>처럼 스키마를 유추하지 않음</td>
      <td>비정형/반정형 데이터에서만 동작. <em>column name</em>을 가진 형태로 <strong>Spark</strong>가 스키마를 관리</td>
      <td><strong>DataFrame</strong>과 같이 정형/반정형 데이터의 쉽고 효율적인 처리. <em>row</em>의 JVM object 형태나 <em>row object collection</em> 형태로 표현. <strong>encoder</strong>를 통해 테이블 형태로 표현</td>
    </tr>
    <tr>
      <td>Data Source API</td>
      <td>어떤 데이터소스든 사용가능</td>
      <td><strong>AVRO</strong>, <strong>CSV</strong>, <strong>TSV</strong>, <strong>JSON</strong>, <strong>HDFS</strong>, <strong>HIVE Table</strong>, <strong>RDB</strong> 등의 데이터 소스가 가능</td>
      <td>어떤 데이터소스도 가능</td>
    </tr>
    <tr>
      <td>immutablility, interoperability</td>
      <td><strong>RDD</strong>는 기본적으로 불변. <strong>RDD</strong>가 표형식일경우 <code class="language-plaintext highlighter-rouge">RDD.toDF</code>로 <strong>DataFrame</strong>으로 변경가능</td>
      <td><strong>DataFrame</strong>으로 변환하고 나면 domain object를 재생성할 수 없음. <code class="language-plaintext highlighter-rouge">RDD.toDF</code>를 할경우 원래 형태의 <code class="language-plaintext highlighter-rouge">RDD</code>로 돌아올 수 없다는 것</td>
      <td><strong>DataFrame</strong>의 확장버전으로 <strong>RDD</strong>와 <strong>DataFrame</strong>을 <strong>DataSet</strong>로 변환할 수 있음</td>
    </tr>
    <tr>
      <td>compile time type safety</td>
      <td>친숙한 OOP 스타일과 compile-time typesafey 제공</td>
      <td><strong>DataFrame</strong>에 존재하지 않는 <em>column</em>에 접근하려고 하면 compile-time typesafe를 보장하지 않음. run-time에서만 확인가능</td>
      <td>compile-time safe 지원</td>
    </tr>
    <tr>
      <td>Optimization</td>
      <td>Optimization engine이 없음. 개발자가 최적화해야 됨</td>
      <td>Catalyst Optimizer가 있음. 이를 이용해서 최적화 진행</td>
      <td>쿼리 계획 최적화를 위해 <strong>DataFrame</strong>의 Catalyst optimization이 있음</td>
    </tr>
    <tr>
      <td>Serialization</td>
      <td>Java Serization 사용</td>
      <td>Off Heap storage를 사용(InMemory)하여 binary format으로. Schema를 알고 있기 때문에 가능. Tungsten 실행 백엔드를 가지고 있어서 메모리를 명시적으로 관리하고 동적으로 bytecode를 만듬</td>
      <td><strong>Encoder</strong>가 있기 때문에 Spark 내부의 Tungsten binary format 사용</td>
    </tr>
    <tr>
      <td>GC(Garbage Collection)</td>
      <td>각 Object의 생성과 파괴로 인한 GC의 overhead가 있음</td>
      <td>각 <em>row</em>의 개별 Object를 구성할 때 GC 코스트 방지</td>
      <td>Serialization때문에 GC가 OBject를 파괴할 필요가 없음. <em>off heap data serialization</em>을 사용함</td>
    </tr>
    <tr>
      <td>Efficiency / Memory use</td>
      <td>Java와 Scala object에서 개별적으로 Serialization을 하면 효율성이 저하됨</td>
      <td>Serialization에 <code class="language-plaintext highlighter-rouge">off heap memory</code>를 사용하면 overhead가 줄어듬.</td>
      <td>Serializaed data에 대한 작업을 수행하고 메모리 사용성을 개선할 수 있음</td>
    </tr>
    <tr>
      <td>Lazy Evaluation</td>
      <td>기본적으로 lazy Evaluation. <strong>Transformation</strong>의 경우 이것을 실행했다는 것만 기억하고 <strong>Action</strong>이 <strong>Driver Program</strong>에 결과를 보낼 필요가 있을 때만 계산</td>
      <td>Lazy Evaluation. <strong>Action</strong>이 나타날 때만 동작함</td>
      <td><strong>RDD</strong>나 <strong>DataFrame</strong>과 같음</td>
    </tr>
    <tr>
      <td>Language Support</td>
      <td>Java, Scala, Python, R</td>
      <td><strong>RDD</strong>와 같음</td>
      <td>Scala와 Java. Spark 2.1.1 기준</td>
    </tr>
    <tr>
      <td>Schema Projection</td>
      <td>명시적으로 사용됨. 스키마 정의가 필요함</td>
      <td>데이터 소스로부터 스키마를 검색하고 수행함. Hive의 경우 Hive Meta store, DB의 경우 DB Engine.</td>
      <td>Spark SQL Engine을 사용하여 자동 탐색</td>
    </tr>
    <tr>
      <td>Aggregation</td>
      <td>느림</td>
      <td>대규모 데이터 셋에 대한 집계 통계를 작성하기 때문에 빠르게 분석함</td>
      <td>빠름</td>
    </tr>
    <tr>
      <td>Use-cases</td>
      <td>low-level 작업이 필요하거나 추상화가 필요한 경우에 사용</td>
      <td>high-level 작업이 필요하거나 정형/반정형 등 많은 케이스</td>
      <td><strong>DataFrame</strong>과 동일</td>
    </tr>
  </tbody>
</table> 



### RDD 예시

low level interface, Data Containers  
각기 다른 프로세스 요소들을 추상화한 같은 형태  
```
# Hadoop에서 읽은 RDD
path = hdfs://...   

# Filtered된 RDD
func = contains(...)   

# Mapped된 RDD
func = split(...)   
```
Fault되면 Loss된 데이터 이전부터 다시 계산

```
data = sc.textFile(...).split("\t")
data.map(lambda x: (x[0], [int(x[1]), 1]))
    .reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]])
    .map(lambda x: [x[0], x[1][0] / x[1][1]])
    .collect()
```

### DataFrame 예시
코드가 직관적이고 schema를 가지는 interface

```
sqlCtx.table("people")
    .groupBy("name")
    .agg("name", avg("age"))
    .collect()
```

### DataSet 예시
SQL로 수행 가능  

```
SELECT name, avg(age) 
FROM people 
GROUP BY name
```

<br><br>
---

# 2. Spark Architecture
![]({{site.baseurl}}/images/post/spark-cluster-overview.png)

executor 개수와 resource 할당  

## 2.1. Cluster Manager
worker와 executors 사이에 자원을 중계해주는 역할  
resource를 효율적으로 분배  

ex) Spark StandAlone(cluster X), (Hadoop)Yarn, Mesos, Kubernetes

## 2.2. Driver Process
SparkContext를 생성하고 RDD를 만들고 operation을 실행하는 프로세스  

Spark-submit을 하면,  
Spark Driver가 Cluster Manager로부터 Executor 실행을 위한 리소스를 요청  
Spark Context는 작업 내용을 task 단위로 분할하여 Executor로 보냄


## 2.3. Executors
주어진 작업의 개별 task들을 실행하는 작업 실행하고 결과 return

1) 애플리케이션을 구성하는 작업들을 실행하여 driver에 그 결과를 return  
2) 각 executor 안에 존재하는 **block manager**라는 서비스를 통해 사용자 프로그램에서 캐시하는 RDD를 저장하기 위한 메모리 저장소를 제공   

Python,R Process <-> JVM(Spark session) -> Executors  

End to End: csv file read(**narrow**) -> DataFrame sort(**wide**) -> Array  



### Operations
1) Transformations: map, filter, groupBy, join
lazy operation으로 즉시 실행하는 단계가 아님

2) Actions: count, collect, save
실제로 실행후 driver로 결과 return


### Variables
- Accumulator: aggregate multiple values as it progresses
- Broadcast: large read-only variable shared across tasks, operations, large lookup tables

_디버깅이나 검증용으로 쓰고, 스파크 스트리밍에서는 accumulator, broadcast 지양 (GC문제..)_



## 2.4. Scheduling

![]({{site.baseurl}}/images/post/DAG scheduler.jpeg)

### DAG (Directed Acyclic Graphs) Scheduler


<br><br>
---

# 3. Spark SQL & DataFrame

## 3.1. Pandas DF vs Spark DF

Pandas DF: 병렬 cpu X 단일 서버 memory로만 처리  
numpy의 ndarray 기반으로 2차원 데이터 분석용

Spark DF: SQL 연산 지원 (df.select('col1'), df.groupBy('col1').count(), df.withColumns('col1'))  
immutable하게 df.drop('col1') 시에 새로운 df 반환 (inplace 옵션 없음)  
컬럼 지정 혹은 리스트 출력 시 대괄호 연산자 \[\] 사용 X

### 기본

```
select gender from tab
->  spark_df.select(‘gender’) 

select * from tab where gender = ‘F’
->  spark_df.filter(spark_df[‘gender’] == ‘F’) 

update set age=age + 10 from tab
->  spark_df.withColumns(‘age’, col(‘age’)+10) 

select gender, count(*) from tab group by gender
->  spark_df.groupBy(‘gender’).count() 

```

### 변환
```
spark_df = spark.createDataFrame(pandas_df) # spark to pandas
pandas_df = spark_df.select(‘*’).toPandas() # pandas to spark
```


### 정렬





<br><br>
---

# 4. MLlib Classification


<br><br>
---

# 5. MLlib Regression


<br><br>
---

# 6. Use Case


<br><br>
---

