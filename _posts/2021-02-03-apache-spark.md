---
toc: true
layout: post
description: section3
categories: [spark,etl]
title: Apache Spark
---
 
ğŸ“ Tacademy

---

## 3.1. Spark
Unified Computing Engine and a Set of Libraries for Parallel Data Processing on Computer Clusters  
ìš©ë„: Machine learning, real time analytics, graph processing, etc.  

- Structured Streaming, Advanced Analytics, Libraries & Ecosystem
- High level APIs (Java, Python, ...)
- Low level APIs (RDDs, Distributed Variables)
- Structured APIs (Datasets, DataFrames, SQL)

ê¸°ì¡´ MapReduceì—ì„œ disk I/O ë¶€í•˜ê°€ ì‹¬í–ˆë˜ ë¶€ë¶„ì„ memoryì—ì„œ ë¹ ë¥´ê²Œ ì‘ì—…  

1) Low level APIs
ì§ì ‘ mapì™€ reduceë¥¼ functionìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ì•¼ í•¨  

2) High level APIs (functional language)
scalaì™€ pythonìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬ ê°€ëŠ¥ (ì¶”ìƒí™”ë˜ì–´ ìˆìŒ)  


---
## 3.2. Spark Architecture
![]({{site.baseurl}}/images/post/spark-cluster-overview.png)


executor ê°œìˆ˜ì™€ resource í• ë‹¹  

### Cluster Manager
workerì™€ executors ì‚¬ì´ì— ìì›ì„ ì¤‘ê³„í•´ì£¼ëŠ” ì—­í•   
resourceë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë°°  

ex) Spark StandAlone(cluster X), (Hadoop)Yarn, Mesos, Kubernetes

### Driver Process
SparkContextë¥¼ ìƒì„±í•˜ê³  RDDë¥¼ ë§Œë“¤ê³  operationì„ ì‹¤í–‰í•˜ëŠ” í”„ë¡œì„¸ìŠ¤  

Spark-submitì„ í•˜ë©´,  
Spark Driverê°€ Cluster Managerë¡œë¶€í„° Executor ì‹¤í–‰ì„ ìœ„í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ìš”ì²­  
Spark ContextëŠ” ì‘ì—… ë‚´ìš©ì„ task ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ Executorë¡œ ë³´ëƒ„


### Executors
ì£¼ì–´ì§„ ì‘ì—…ì˜ ê°œë³„ taskë“¤ì„ ì‹¤í–‰í•˜ëŠ” ì‘ì—… ì‹¤í–‰í•˜ê³  ê²°ê³¼ return

1) ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì„±í•˜ëŠ” ì‘ì—…ë“¤ì„ ì‹¤í–‰í•˜ì—¬ driverì— ê·¸ ê²°ê³¼ë¥¼ return  
2) ê° executor ì•ˆì— ì¡´ì¬í•˜ëŠ” **block manager**ë¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ì í”„ë¡œê·¸ë¨ì—ì„œ ìºì‹œí•˜ëŠ” RDDë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œë¥¼ ì œê³µ   

Python,R Process <-> JVM(Spark session) -> Executors  

End to End: csv file read(**narrow**) -> DataFrame sort(**wide**) -> Array  



#### Operations
1) Transformations: map, filter, groupBy, join
lazy operationìœ¼ë¡œ ì¦‰ì‹œ ì‹¤í–‰í•˜ëŠ” ë‹¨ê³„ê°€ ì•„ë‹˜

2) Actions: count, collect, save
ì‹¤ì œë¡œ ì‹¤í–‰í›„ driverë¡œ ê²°ê³¼ return


---
## 3.3. Processing ë°©ì‹

### Batch Processing
- big & complex
- processing massive data
- higher latencies


### Real-time Processing (Stream)
- relatively simple and generally independent
- one at a time processing
- sub-second latency

input data stream -> spark streaming -> batch input data -> spark engine -> processed batch data

__spark streamingì˜ ê²½ìš° ê°œë°œí•˜ê¸°ëŠ” ì–´ë µì§€ ì•Šìœ¼ë‚˜, ìš´ì˜(debugging)ì´ ì–´ë ¤ì›€__


---
## 3.4. Variables
### Accumulator
aggregate multiple values as it progresses


### Broadcast
large read-only variable shared across tasks, operations
large lookup tables


ë””ë²„ê¹…ì´ë‚˜ ê²€ì¦ìš©ìœ¼ë¡œ ì“°ê³ 
ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” accumulator, broadcast ì§€ì–‘ (GCë¬¸ì œ..)


Fault Tolerance
ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ê±´ì— ë”°ë¼,
ì‹¤íŒ¨ì‹œ ëˆ„ë½ì´ ë˜ë©´ ë˜ëŠ”ì§€ ì•ˆë˜ëŠ”ì§€ì— ë”°ë¼ ê³ ë ¤ (Kafka)



---
## 3.5. Interface

### RDDs (Resilient Distributed Datasets) 
low level interface, Data Containers  
ê°ê¸° ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ìš”ì†Œë“¤ì„ ì¶”ìƒí™”í•œ ê°™ì€ í˜•íƒœ  

1) Hadoopì—ì„œ ì½ì€ RDD
path = hdfs://...   

2) Filteredëœ RDD
func = contains(...)   

3) Mappedëœ RDD
func = split(...)   

Faultë˜ë©´ Lossëœ ë°ì´í„° ì´ì „ë¶€í„° ë‹¤ì‹œ ê³„ì‚°

```
data = sc.textFile(...).split("\t")
data.map(lambda x: (x[0], [int(x[1]), 1]))
.reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]])
.map(lambda x: [x[0], x[1][0] / x[1][1]])
.collect()
```


### DataFrames
ì½”ë“œê°€ ì§ê´€ì ì´ê³  schemaë¥¼ ê°€ì§€ëŠ” interface


```
sqlCtx.table("people").groupBy("name").agg("name", 
avg("age")).collect()
```



---
## 3.6. Scheduling

![]({{site.baseurl}}/images/post/DAG scheduler.jpeg)

### DAG (Directed Acyclic Graphs) Scheduler














