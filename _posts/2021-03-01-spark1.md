---
toc: true
layout: post
description: section1-3
categories: [spark]
title: Spark
---
 
ğŸ“ [Spark User Guide](https://spark.apache.org/docs/latest/api/python/index.html), Spark The Definitive Guide

---

# 1. Spark ê°œìš”

Unified Computing Engine and a Set of Libraries for Parallel Data Processing on Computer Clusters  
ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ìì›ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ì‚° ë³‘ë ¬ ì²˜ë¦¬    


## 1.1. Spark ì£¼ìš” ìš©ë„
- Spark SQL (Batch processing): big & complex, processing massive data, higher latencies
- Spark Streaming (Real time analytics): relatively simple and generally independent, one at a time processing, sub-second latency  
- MLlib (Machine learning)
- GraphX (Graph processing)  

_spark streamingì˜ ê²½ìš° ê°œë°œí•˜ê¸°ëŠ” ì–´ë µì§€ ì•Šìœ¼ë‚˜, ìš´ì˜(debugging)ì´ ì–´ë ¤ì›€_


## 1.2. Hadoop vs Spark
Hadoopì˜ MPP[^1] (Massive Parallel Processing)ì„ ê¸°ë°˜ìœ¼ë¡œ ìì› ëŒ€ìš©ëŸ‰ ì²˜ë¦¬í•˜ê³   
ê¸°ì¡´ Hadoopì˜ MapReduceì—ì„œ disk I/O ë¶€í•˜ê°€ ì‹¬í–ˆë˜ ë¶€ë¶„ì„ memoryì—ì„œ ë¹ ë¥´ê²Œ ì‘ì—…   

- RDD -> DataFrame -> SQL ë°ì´í„° ì²˜ë¦¬ ì¶”ìƒí™”
- DAG ê¸°ë°˜ì˜ ì‹¤í–‰ ìµœì í™”
- In-memory writeìœ¼ë¡œ Map Reduce ì‘ì—…ì„ ìµœì í™”

[^1]: DW workloadë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ìì£¼ ì‚¬ìš© <->SMP (Symmetric Multi processor)


## 1.3. RDD (Resilient Distributed Datasets)  
Immutable distributed collection of your data, partitioned across nodes in your cluster  
- ë¶„ì‚° ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„°ë¥¼ ì¶”ìƒí™”  
- Immutable ë°ì´í„° ì§‘í•©  
- í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ì„œë¡œ ë‹¤ë¥¸ ë…¸ë“œì— ì €ì¥í•˜ê³  ì½ìŒ  

### RDD vs DF vs DataSet(SQL)  
[ì¶œì²˜_loustler ë¸”ë¡œê·¸](https://loustler.io/data_eng/spark-rdd-dataframe-and-dataset/)
<table>
  <tbody>
    <tr>
      <td>&nbsp;</td>
      <td><strong>RDD</strong></td>
      <td><strong>Data Frame</strong></td>
      <td><strong>DataSet</strong></td>
    </tr>
    <tr>
      <td>Release</td>
      <td><code class="language-plaintext highlighter-rouge">1.0</code></td>
      <td><code class="language-plaintext highlighter-rouge">1.3</code></td>
      <td><code class="language-plaintext highlighter-rouge">1.6</code></td>
    </tr>
    <tr>
      <td>Data Representation</td>
      <td>Javaë‚˜ Scala object</td>
      <td><em>column</em>ì˜ ì´ë¦„ì´ ìˆëŠ” organizedëœ ë¶„ì‚° ì»¬ë ‰ì…˜. RDB í…Œì´ë¸”ê³¼ ìœ ì‚¬</td>
      <td><strong>Data Frame</strong>ì˜ í™•ì¥. type-safe, oop interface ì œê³µ, Catalyst query optimizer ì˜ ì„±ëŠ¥ ì´ì , off heap storage ë©”ì»¤ë‹ˆì¦˜ ì œê³µ</td>
    </tr>
    <tr>
      <td>Data Format</td>
      <td>ë¹„ì •í˜•/ì •í˜•ë°ì´í„°ì˜ ì‰½ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬. <strong>Data Frame</strong>ê³¼ <strong>DataSet</strong>ì²˜ëŸ¼ ìŠ¤í‚¤ë§ˆë¥¼ ìœ ì¶”í•˜ì§€ ì•ŠìŒ</td>
      <td>ë¹„ì •í˜•/ë°˜ì •í˜• ë°ì´í„°ì—ì„œë§Œ ë™ì‘. <em>column name</em>ì„ ê°€ì§„ í˜•íƒœë¡œ <strong>Spark</strong>ê°€ ìŠ¤í‚¤ë§ˆë¥¼ ê´€ë¦¬</td>
      <td><strong>DataFrame</strong>ê³¼ ê°™ì´ ì •í˜•/ë°˜ì •í˜• ë°ì´í„°ì˜ ì‰½ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬. <em>row</em>ì˜ JVM object í˜•íƒœë‚˜ <em>row object collection</em> í˜•íƒœë¡œ í‘œí˜„. <strong>encoder</strong>ë¥¼ í†µí•´ í…Œì´ë¸” í˜•íƒœë¡œ í‘œí˜„</td>
    </tr>
    <tr>
      <td>Data Source API</td>
      <td>ì–´ë–¤ ë°ì´í„°ì†ŒìŠ¤ë“  ì‚¬ìš©ê°€ëŠ¥</td>
      <td><strong>AVRO</strong>, <strong>CSV</strong>, <strong>TSV</strong>, <strong>JSON</strong>, <strong>HDFS</strong>, <strong>HIVE Table</strong>, <strong>RDB</strong> ë“±ì˜ ë°ì´í„° ì†ŒìŠ¤ê°€ ê°€ëŠ¥</td>
      <td>ì–´ë–¤ ë°ì´í„°ì†ŒìŠ¤ë„ ê°€ëŠ¥</td>
    </tr>
    <tr>
      <td>immutablility, interoperability</td>
      <td><strong>RDD</strong>ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¶ˆë³€. <strong>RDD</strong>ê°€ í‘œí˜•ì‹ì¼ê²½ìš° <code class="language-plaintext highlighter-rouge">RDD.toDF</code>ë¡œ <strong>DataFrame</strong>ìœ¼ë¡œ ë³€ê²½ê°€ëŠ¥</td>
      <td><strong>DataFrame</strong>ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë‚˜ë©´ domain objectë¥¼ ì¬ìƒì„±í•  ìˆ˜ ì—†ìŒ. <code class="language-plaintext highlighter-rouge">RDD.toDF</code>ë¥¼ í• ê²½ìš° ì›ë˜ í˜•íƒœì˜ <code class="language-plaintext highlighter-rouge">RDD</code>ë¡œ ëŒì•„ì˜¬ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒ</td>
      <td><strong>DataFrame</strong>ì˜ í™•ì¥ë²„ì „ìœ¼ë¡œ <strong>RDD</strong>ì™€ <strong>DataFrame</strong>ì„ <strong>DataSet</strong>ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŒ</td>
    </tr>
    <tr>
      <td>compile time type safety</td>
      <td>ì¹œìˆ™í•œ OOP ìŠ¤íƒ€ì¼ê³¼ compile-time typesafey ì œê³µ</td>
      <td><strong>DataFrame</strong>ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” <em>column</em>ì— ì ‘ê·¼í•˜ë ¤ê³  í•˜ë©´ compile-time typesafeë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ. run-timeì—ì„œë§Œ í™•ì¸ê°€ëŠ¥</td>
      <td>compile-time safe ì§€ì›</td>
    </tr>
    <tr>
      <td>Optimization</td>
      <td>Optimization engineì´ ì—†ìŒ. ê°œë°œìê°€ ìµœì í™”í•´ì•¼ ë¨</td>
      <td>Catalyst Optimizerê°€ ìˆìŒ. ì´ë¥¼ ì´ìš©í•´ì„œ ìµœì í™” ì§„í–‰</td>
      <td>ì¿¼ë¦¬ ê³„íš ìµœì í™”ë¥¼ ìœ„í•´ <strong>DataFrame</strong>ì˜ Catalyst optimizationì´ ìˆìŒ</td>
    </tr>
    <tr>
      <td>Serialization</td>
      <td>Java Serization ì‚¬ìš©</td>
      <td>Off Heap storageë¥¼ ì‚¬ìš©(InMemory)í•˜ì—¬ binary formatìœ¼ë¡œ. Schemaë¥¼ ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥. Tungsten ì‹¤í–‰ ë°±ì—”ë“œë¥¼ ê°€ì§€ê³  ìˆì–´ì„œ ë©”ëª¨ë¦¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ë™ì ìœ¼ë¡œ bytecodeë¥¼ ë§Œë“¬</td>
      <td><strong>Encoder</strong>ê°€ ìˆê¸° ë•Œë¬¸ì— Spark ë‚´ë¶€ì˜ Tungsten binary format ì‚¬ìš©</td>
    </tr>
    <tr>
      <td>GC(Garbage Collection)</td>
      <td>ê° Objectì˜ ìƒì„±ê³¼ íŒŒê´´ë¡œ ì¸í•œ GCì˜ overheadê°€ ìˆìŒ</td>
      <td>ê° <em>row</em>ì˜ ê°œë³„ Objectë¥¼ êµ¬ì„±í•  ë•Œ GC ì½”ìŠ¤íŠ¸ ë°©ì§€</td>
      <td>Serializationë•Œë¬¸ì— GCê°€ OBjectë¥¼ íŒŒê´´í•  í•„ìš”ê°€ ì—†ìŒ. <em>off heap data serialization</em>ì„ ì‚¬ìš©í•¨</td>
    </tr>
    <tr>
      <td>Efficiency / Memory use</td>
      <td>Javaì™€ Scala objectì—ì„œ ê°œë³„ì ìœ¼ë¡œ Serializationì„ í•˜ë©´ íš¨ìœ¨ì„±ì´ ì €í•˜ë¨</td>
      <td>Serializationì— <code class="language-plaintext highlighter-rouge">off heap memory</code>ë¥¼ ì‚¬ìš©í•˜ë©´ overheadê°€ ì¤„ì–´ë“¬.</td>
      <td>Serializaed dataì— ëŒ€í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŒ</td>
    </tr>
    <tr>
      <td>Lazy Evaluation</td>
      <td>ê¸°ë³¸ì ìœ¼ë¡œ lazy Evaluation. <strong>Transformation</strong>ì˜ ê²½ìš° ì´ê²ƒì„ ì‹¤í–‰í–ˆë‹¤ëŠ” ê²ƒë§Œ ê¸°ì–µí•˜ê³  <strong>Action</strong>ì´ <strong>Driver Program</strong>ì— ê²°ê³¼ë¥¼ ë³´ë‚¼ í•„ìš”ê°€ ìˆì„ ë•Œë§Œ ê³„ì‚°</td>
      <td>Lazy Evaluation. <strong>Action</strong>ì´ ë‚˜íƒ€ë‚  ë•Œë§Œ ë™ì‘í•¨</td>
      <td><strong>RDD</strong>ë‚˜ <strong>DataFrame</strong>ê³¼ ê°™ìŒ</td>
    </tr>
    <tr>
      <td>Language Support</td>
      <td>Java, Scala, Python, R</td>
      <td><strong>RDD</strong>ì™€ ê°™ìŒ</td>
      <td>Scalaì™€ Java. Spark 2.1.1 ê¸°ì¤€</td>
    </tr>
    <tr>
      <td>Schema Projection</td>
      <td>ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©ë¨. ìŠ¤í‚¤ë§ˆ ì •ì˜ê°€ í•„ìš”í•¨</td>
      <td>ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ìŠ¤í‚¤ë§ˆë¥¼ ê²€ìƒ‰í•˜ê³  ìˆ˜í–‰í•¨. Hiveì˜ ê²½ìš° Hive Meta store, DBì˜ ê²½ìš° DB Engine.</td>
      <td>Spark SQL Engineì„ ì‚¬ìš©í•˜ì—¬ ìë™ íƒìƒ‰</td>
    </tr>
    <tr>
      <td>Aggregation</td>
      <td>ëŠë¦¼</td>
      <td>ëŒ€ê·œëª¨ ë°ì´í„° ì…‹ì— ëŒ€í•œ ì§‘ê³„ í†µê³„ë¥¼ ì‘ì„±í•˜ê¸° ë•Œë¬¸ì— ë¹ ë¥´ê²Œ ë¶„ì„í•¨</td>
      <td>ë¹ ë¦„</td>
    </tr>
    <tr>
      <td>Use-cases</td>
      <td>low-level ì‘ì—…ì´ í•„ìš”í•˜ê±°ë‚˜ ì¶”ìƒí™”ê°€ í•„ìš”í•œ ê²½ìš°ì— ì‚¬ìš©</td>
      <td>high-level ì‘ì—…ì´ í•„ìš”í•˜ê±°ë‚˜ ì •í˜•/ë°˜ì •í˜• ë“± ë§ì€ ì¼€ì´ìŠ¤</td>
      <td><strong>DataFrame</strong>ê³¼ ë™ì¼</td>
    </tr>
  </tbody>
</table> 



### RDD ì˜ˆì‹œ

low level interface, Data Containers  
ê°ê¸° ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ìš”ì†Œë“¤ì„ ì¶”ìƒí™”í•œ ê°™ì€ í˜•íƒœ  
```
# Hadoopì—ì„œ ì½ì€ RDD
path = hdfs://...   

# Filteredëœ RDD
func = contains(...)   

# Mappedëœ RDD
func = split(...)   
```
Faultë˜ë©´ Lossëœ ë°ì´í„° ì´ì „ë¶€í„° ë‹¤ì‹œ ê³„ì‚°

```
data = sc.textFile(...).split("\t")
data.map(lambda x: (x[0], [int(x[1]), 1]))
    .reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]])
    .map(lambda x: [x[0], x[1][0] / x[1][1]])
    .collect()
```

### DataFrame ì˜ˆì‹œ
ì½”ë“œê°€ ì§ê´€ì ì´ê³  schemaë¥¼ ê°€ì§€ëŠ” interface

```
sqlCtx.table("people")
    .groupBy("name")
    .agg("name", avg("age"))
    .collect()
```

### DataSet ì˜ˆì‹œ
SQLë¡œ ìˆ˜í–‰ ê°€ëŠ¥  

```
SELECT name, avg(age) 
FROM people 
GROUP BY name
```

<br><br>
---

# 2. Spark Architecture
![]({{site.baseurl}}/images/post/spark-cluster-overview.png)

executor ê°œìˆ˜ì™€ resource í• ë‹¹  

## 2.1. Cluster Manager
workerì™€ executors ì‚¬ì´ì— ìì›ì„ ì¤‘ê³„í•´ì£¼ëŠ” ì—­í•   
resourceë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë°°  

ex) Spark StandAlone(cluster X), (Hadoop)Yarn, Mesos, Kubernetes

## 2.2. Driver Process
SparkContextë¥¼ ìƒì„±í•˜ê³  RDDë¥¼ ë§Œë“¤ê³  operationì„ ì‹¤í–‰í•˜ëŠ” í”„ë¡œì„¸ìŠ¤  

Spark-submitì„ í•˜ë©´,  
Spark Driverê°€ Cluster Managerë¡œë¶€í„° Executor ì‹¤í–‰ì„ ìœ„í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ìš”ì²­  
Spark ContextëŠ” ì‘ì—… ë‚´ìš©ì„ task ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ Executorë¡œ ë³´ëƒ„


## 2.3. Executors
ì£¼ì–´ì§„ ì‘ì—…ì˜ ê°œë³„ taskë“¤ì„ ì‹¤í–‰í•˜ëŠ” ì‘ì—… ì‹¤í–‰í•˜ê³  ê²°ê³¼ return

1) ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì„±í•˜ëŠ” ì‘ì—…ë“¤ì„ ì‹¤í–‰í•˜ì—¬ driverì— ê·¸ ê²°ê³¼ë¥¼ return  
2) ê° executor ì•ˆì— ì¡´ì¬í•˜ëŠ” **block manager**ë¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ì í”„ë¡œê·¸ë¨ì—ì„œ ìºì‹œí•˜ëŠ” RDDë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œë¥¼ ì œê³µ   

Python,R Process <-> JVM(Spark session) -> Executors  

End to End: csv file read(**narrow**) -> DataFrame sort(**wide**) -> Array  



### Operations
1) Transformations: map, filter, groupBy, join
lazy operationìœ¼ë¡œ ì¦‰ì‹œ ì‹¤í–‰í•˜ëŠ” ë‹¨ê³„ê°€ ì•„ë‹˜

2) Actions: count, collect, save
ì‹¤ì œë¡œ ì‹¤í–‰í›„ driverë¡œ ê²°ê³¼ return


### Variables
- Accumulator: aggregate multiple values as it progresses
- Broadcast: large read-only variable shared across tasks, operations, large lookup tables

_ë””ë²„ê¹…ì´ë‚˜ ê²€ì¦ìš©ìœ¼ë¡œ ì“°ê³ , ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” accumulator, broadcast ì§€ì–‘ (GCë¬¸ì œ..)_



## 2.4. Scheduling

![]({{site.baseurl}}/images/post/DAG scheduler.jpeg)

### DAG (Directed Acyclic Graphs) Scheduler


<br><br>
---

# 3. PySpark

### Pandas vs PySpark 

### Pandas:  
 ë‹¨ì¼ ì„œë²„ memoryë¡œë§Œ ì²˜ë¦¬  
 numpyì˜ ndarray ê¸°ë°˜ìœ¼ë¡œ 2ì°¨ì› ë°ì´í„° ë¶„ì„ìš©  

### PySpark:  
 ë³‘ë ¬ CPUë¡œ ì²˜ë¦¬  
 SQL ì—°ì‚° ì§€ì›  
```
select gender from tab
->  spark_df.select(â€˜genderâ€™) 

select * from tab where gender = â€˜Fâ€™
->  spark_df.filter(spark_df[â€˜genderâ€™] == â€˜Fâ€™) 

update set age=age + 10 from tab
->  spark_df.withColumns(â€˜ageâ€™, col(â€˜ageâ€™)+10) 

select gender, count(*) from tab group by gender
->  spark_df.groupBy(â€˜genderâ€™).count() 

```
 immutableí•˜ê²Œ df.drop('col1') ì‹œì— ìƒˆë¡œìš´ df ë°˜í™˜ (inplace ì˜µì…˜ ì—†ìŒ)  
 ì»¬ëŸ¼ ì§€ì • í˜¹ì€ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ì‹œ ëŒ€ê´„í˜¸ ì—°ì‚°ì \[\] ì‚¬ìš© X  


## 3.1. Creating the spark session and context

### Session ìƒì„±  

```python
import pandas as pd
from PySpark import SparkContext, SparkConf
from PySpark.sql import SparkSession

# Creating a spark context class
sc = SparkContext()

# Creating a spark session
spark = SparkSession \
    .builder \
    .appName("Python Spark DataFrames basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
 
spark
```

<br>
    <div>
        <p><b>SparkSession - in-memory</b></p>

<div>
    <p><b>SparkContext</b></p>

    <p><a href="http://karthiks-mbp.attlocal.net:4041">Spark UI</a></p>

    <dl>
      <dt>Version</dt>
        <dd><code>v3.1.1</code></dd>
      <dt>Master</dt>
        <dd><code>local[*]</code></dd>
      <dt>AppName</dt>
        <dd><code>PySpark-shell</code></dd>
    </dl>
</div>

    </div>

<br>
_DataBricksì—ì„œëŠ” ë”°ë¡œ session ìƒì„± ì•ˆí•´ë„ sparkì¹˜ë©´ ë– ìˆëŠ” SparkUI í™•ì¸ ê°€ëŠ¥_  

  
## 3.2. Load a data file into a DataFrame

### File ì½ê¸°  

* spark.read.csv  

```python
# pyspark_df = pd.read_csv('https://www.kaggle.com/.../titanic_train.csv', header='infer')  
# databricks FSì—ì„œëŠ” readë¶€í„° spark.read_csv í•¨ìˆ˜ ì½ì§€ ì•Šìœ¼ë©´ FileNotFoundErrorë‚¨
pyspark_df = spark.read.csv('/FileStore/tables/titanic_train.csv', header=True, inferSchema=True)

pyspark_df.head() # pyspark.sql.types.Row íƒ€ì…ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶œë ¥
> Out[17]: Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S')
```

### ì½ì€ df í™•ì¸
* show  
* display  

```
pyspark_df.limit(10).show()
> +-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
  |PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|
  +-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
  |          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|
  |          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|
  |          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|
  |          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|
  |          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|
  |          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|
  |          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|
  |          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|
  |          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|
  |         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|
  +-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+

display(pyspark_df.limit(10))
```
<br>
<style scoped>
  .table-result-container {
    max-height: 300px;
    overflow: auto;
  }
  table, th, td {
    border: 1px solid black;
    border-collapse: collapse;
  }
  th, td {
    padding: 5px;
  }
  th {
    text-align: left;
  }
</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PassengerId</th><th>Survived</th><th>Pclass</th><th>Name</th><th>Sex</th><th>Age</th><th>SibSp</th><th>Parch</th><th>Ticket</th><th>Fare</th><th>Cabin</th><th>Embarked</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22.0</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td>null</td><td>S</td></tr><tr><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38.0</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr><tr><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26.0</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td>null</td><td>S</td></tr><tr><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35.0</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr><tr><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35.0</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td>null</td><td>S</td></tr><tr><td>6</td><td>0</td><td>3</td><td>Moran, Mr. James</td><td>male</td><td>null</td><td>0</td><td>0</td><td>330877</td><td>8.4583</td><td>null</td><td>Q</td></tr><tr><td>7</td><td>0</td><td>1</td><td>McCarthy, Mr. Timothy J</td><td>male</td><td>54.0</td><td>0</td><td>0</td><td>17463</td><td>51.8625</td><td>E46</td><td>S</td></tr><tr><td>8</td><td>0</td><td>3</td><td>Palsson, Master. Gosta Leonard</td><td>male</td><td>2.0</td><td>3</td><td>1</td><td>349909</td><td>21.075</td><td>null</td><td>S</td></tr><tr><td>9</td><td>1</td><td>3</td><td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td><td>female</td><td>27.0</td><td>0</td><td>2</td><td>347742</td><td>11.1333</td><td>null</td><td>S</td></tr><tr><td>10</td><td>1</td><td>2</td><td>Nasser, Mrs. Nicholas (Adele Achem)</td><td>female</td><td>14.0</td><td>1</td><td>0</td><td>237736</td><td>30.0708</td><td>null</td><td>C</td></tr></tbody></table></div>
<br>




## 3.3. View the data schema of a DataFrame


### pandas DFë¡œ ë³€ê²½  

* toPandas  

```
pd_df = pyspark_df.select('*').toPandas()

pd_df.head() # ê¸°ì¡´ pandasì²˜ëŸ¼ read 
>   <class 'pandas.core.frame.DataFrame'>
       PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked
    0            1         0       3  ...   7.2500  None         S
    1            2         1       1  ...  71.2833   C85         C
    2            3         1       3  ...   7.9250  None         S
    3            4         1       1  ...  53.1000  C123         S
    4            5         0       3  ...   8.0500  None         S
    5            6         0       3  ...   8.4583  None         Q
    6            7         0       1  ...  51.8625   E46         S
    7            8         0       3  ...  21.0750  None         S
    8            9         1       3  ...  11.1333  None         S
    9           10         1       2  ...  30.0708  None         C
    
    [10 rows x 12 columns]
    
```

### spark DFë¡œ ë³€ê²½  
* spark.createDataFrame  

```
pd_df = pd.read_csv('https://www.kaggle.com/.../mtcars.csv')
sdf = spark.createDataFrame(pd_df) 
```

### Schema í™•ì¸  
pandasì˜ infoì™€ ê°™ì€ í•¨ìˆ˜  
* printSchema  

```python
pd_df.info()
> RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int32  
 1   Survived     891 non-null    int32  
 2   Pclass       891 non-null    int32  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int32  
 7   Parch        891 non-null    int32  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int32(5), object(5)

sdf.printSchema()
> root
  |-- Unnamed: 0: string (nullable = true)
  |-- mpg: double (nullable = true)
  |-- cyl: long (nullable = true)
  |-- disp: double (nullable = true)
  |-- hp: long (nullable = true)
  |-- drat: double (nullable = true)
  |-- wt: double (nullable = true)
  |-- qsec: double (nullable = true)
  |-- vs: long (nullable = true)
  |-- am: long (nullable = true)
  |-- gear: long (nullable = true)
  |-- carb: long (nullable = true)
```

## 3.4. Perform basic data manipulation

### Select  

* show  

```
sdf.show(5)
> +-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
 |       Unnamed: 0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|
 +-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
 |        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|
 |    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|
 |       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|
 |   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|
 |Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|
 +-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
```

* filter  

```python
sdf.filter(sdf['mpg'] < 18).show(5)
>+-----------+----+---+-----+---+----+----+-----+---+---+----+----+
 | Unnamed: 0| mpg|cyl| disp| hp|drat|  wt| qsec| vs| am|gear|carb|
 +-----------+----+---+-----+---+----+----+-----+---+---+----+----+
 | Duster 360|14.3|  8|360.0|245|3.21|3.57|15.84|  0|  0|   3|   4|
 |  Merc 280C|17.8|  6|167.6|123|3.92|3.44| 18.9|  1|  0|   4|   4|
 | Merc 450SE|16.4|  8|275.8|180|3.07|4.07| 17.4|  0|  0|   3|   3|
 | Merc 450SL|17.3|  8|275.8|180|3.07|3.73| 17.6|  0|  0|   3|   3|
 |Merc 450SLC|15.2|  8|275.8|180|3.07|3.78| 18.0|  0|  0|   3|   3|
 +-----------+----+---+-----+---+----+----+-----+---+---+----+----+

```

### Update  

* withColumn  

```python
sdf.withColumn('wtTon', sdf['wt'] * 0.45).show(5)
>+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------+
 |       Unnamed: 0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|  wtTon|
 +-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------+
 |        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|  1.179|
 |    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|1.29375|
 |       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|  1.044|
 |   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|1.44675|
 |Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|  1.548|
 +-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------+

```

### Case When

* col  
* when  
* count  
* isnan  

```
from pyspark.sql.functions import count, isnan, when, col

sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in titanic_sdf.columns]).show()
>+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
 |PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|
 +-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
 |          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|
 +-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+
```

## 3.5. Aggregate data in a DataFrame

### Group by  

```python
sdf.groupby(['cyl'])\
.agg({"wt": "AVG"})\
.show(5)
>+---+------------------+
 |cyl|           avg(wt)|
 +---+------------------+
 |  6| 3.117142857142857|
 |  8|3.9992142857142854|
 |  4| 2.285727272727273|
 +---+------------------+
```






<br><br>
---


