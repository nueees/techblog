---
toc: true
layout: post
description: 6장
categories: [statistics,python]
title: QDA
---

<!-- [kaggle/GREG HAMEL/Hypothesis Testing](https://www.kaggle.com/hamelg/python-for-data-24-hypothesis-testing) -->

---

# 06_통계분석_2

# 판별분석 QDA(Quadratic Discriminant Analysis) 
- LDA는 선형 판별분석 - 07_기계학습_1에서 차원축소를 다룸
- 여기서는 이차 판별분석으로 "분류"하는 예시
- 모든 클래스k에 대하여 동일한 covariance matrix를 가정했던 LDA와 달리 QDA는 k클래스 마다 각각의 covariance matrix를 가지게 함  
- k의 클래스 별 공분산 구조가 확연히 다를때 사용
- 설명변수가 많아질 수록 추정하는 모수도 많아지므로 샘플이 많이 필요 (+속도 저하)
- 샘플이 적어서 분산을 줄이는 것이 중요할 경우 LDA를, 샘플이 많아서 분산에 대한 우려가 적을때, 혹은 공분산에 대한 가정이 비현실적으로 판단될 때에는 QDA를 사용



```python
# 데이터 생성
import numpy as np
X = np.array([[-1,-1], [-2,-1], [-3,-2], [1,1], [2,1], [3,2]])
y = np.array([1,1,1,2,2,2])
```


```python
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
clf2 = QuadraticDiscriminantAnalysis()
clf2.fit(X,y)
```




    QuadraticDiscriminantAnalysis()




```python
clf2.predict([[-0.8,-1]])
```




    array([1])



# MultiDimensional Scaling (MDS) 다차원척도법

여러 차원 축소 기법 중 하나
![차원축소예시들](https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_001.png)

## 종류
1) 계량적: PCoA (principle coordinates analysis)  
Classical multidimensional scaling으로, PCA (principle component analysis)와 매우 비슷하나,  
PCA: Euclidean 거리 사용하고 선형 관계 있으면 사용 (대부분 geological data)  
PCoA: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 있으면 사용 (biogeographic data)  

2) 비계량적: Non-MultiDimensional Scaling (NMDS)  
NMDS: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 없으면 사용 (어떤 지역 species 개체 수 많은 지)


## 계량적(구간척도, 비율척도)

mds 객체 생설할 때 dissimilarity로 euclidean할지 precomputed 미리 계산된 걸로 할 지 정하고  
mds.fit_transform 옵션에서 계산된 manhattan_distances 넣어주면 됨  





```python
from sklearn.manifold import MDS
from matplotlib import pyplot as plt
import sklearn.datasets as dt
import seaborn as sns         
import numpy as np
from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from sklearn.datasets import load_digits
```


```python
X2 = np.array([[0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 0], [0, 1, 1]])

mds2 = MDS(random_state=0)
X2_transform = mds2.fit_transform(X2)
print(X2_transform)

stress2 = mds2.stress_
print(stress2)
```

    [[ 0.72521687  0.52943352]
     [ 0.61640884 -0.48411805]
     [-0.9113603  -0.47905115]
     [-0.2190564   0.71505714]
     [-0.21120901 -0.28132146]]
    0.18216844548575456
    

stress는 잘 피팅되었는지 검증용으로, 계산된 거리가 dissimilarity 차이를 보여주는데  

stress가 통상 0.2 이상이면 차원 높여야 함  


```python
colors = ['r', 'g', 'b', 'c', 'm']
size = [64, 64, 64, 64, 64]
fig = plt.figure(2, (10,4))

ax = fig.add_subplot(121, projection='3d')
plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors)
plt.title('Original Points')

ax = fig.add_subplot(122)
plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors)
plt.title('Embedding in 2D')
fig.subplots_adjust(wspace=.4, hspace=0.5)
plt.show()
```


    
    



```python
dist_manhattan = manhattan_distances(X2)
mds3 = MDS(dissimilarity='precomputed', random_state=0)
# Get the embeddings
X2_transform_L1 = mds3.fit_transform(dist_manhattan)
print(X2_transform_L1)
print(mds3.stress_)
```

    [[ 0.9847767   0.84738596]
     [ 0.81047787 -0.37601578]
     [-1.104849   -1.06040621]
     [-0.29311254  0.87364759]
     [-0.39729303 -0.28461157]]
    0.4047164940033806
    


```python
fig = plt.figure(2, (15,6))

ax = fig.add_subplot(131, projection='3d')
plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors)
plt.title('Original Points')

ax = fig.add_subplot(132)
plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors)
plt.title('Embedding in 2D')
fig.subplots_adjust(wspace=.4, hspace=0.5)

ax = fig.add_subplot(133)
plt.scatter(X2_transform_L1[:,0], X2_transform_L1[:,1], s=size, c=colors)
plt.title('Embedding in 2D L1')
fig.subplots_adjust(wspace=.4, hspace=0.5)

plt.show()
```


    
    



```python
# print(load_digits.__doc__)
X, y = load_digits(return_X_y=True)
X = X[:100]
print(X.shape)

mds = MDS(n_components=2)
X_transformed = mds.fit_transform(X[:100])
print(X_transformed.shape)
Y = y[:100]
print(Y.size)
# print(X_transformed[:5,0])
# print(X_transformed[:5,1])
print(mds.stress_)
```

    (100, 64)
    (100, 2)
    100
    1133807.722583498
    


```python
colormap = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'w', 'w'])
# colormap[Y]
fig = plt.figure(2, (10,4))

ax = fig.add_subplot(122)
plt.scatter(X_transformed[:,0], X_transformed[:,1], c=colormap[Y])
plt.title('Embedding in 2D')
plt.show()
```


    
    



```python
nmds = MDS(n_components=2, metric=False)
nX_transformed2 = nmds.fit_transform(X)
# print(nX_transformed2)
nX_transformed2 *= np.sqrt((X ** 2).sum()) / np.sqrt((nX_transformed ** 2).sum())
# print(nX_transformed2)
Y = y[:100]
# print(Y.size)
# print(nX_transformed[:5,0])
# print(nX_transformed[:5,1])
# print(nmds.stress_)

colormap = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'w', 'w'])
# colormap[Y]
fig = plt.figure(2, (10,4))

ax = fig.add_subplot(122)
plt.scatter(nX_transformed2[:,0], nX_transformed2[:,1], c=colormap[Y])
plt.title('Embedding in non mds 2D')
plt.show()

```


    
    


## non-metric MDS: 다차원척도법 비계량적(순서척도)

1) 차이에 대해 수치화(quantified) 한 값을 얻기 힘들때, 순서만 알 수 있을 때 사용  
예) 검정색-진회색-연회색-흰색... 중 가장 밝은 색, 빈도 수가 많은 데이터  

2) 유클리디안 외 user-selected 거리 메트릭을 사용하고 싶을 때 (Jaccard,...)

Metric = False 옵션 주면 됨.

3) 차원이 미리 결정되어야 하고, local minima(지역 최소값) 수렴 가능성이 있고, 시간 오래 걸리는 게 단점





```python

from sklearn.preprocessing import MinMaxScaler
from mpl_toolkits import mplot3d


df = pd.read_csv('../data/yeast-transcriptomics/SC_expression.csv')
df = df.iloc[:,1:]
# print(df.corr())
# print(df.T)
df1 = df.T.values
sc = MinMaxScaler()
scaled = sc.fit_transform(df1)
# print(scaled)
mds = MDS(n_components=2)
mds_scaled = mds.fit_transform(scaled)
nmds = MDS(n_components=2, metric=False)
nmds_scaled = nmds.fit_transform(scaled)

plt.subplot(121)
sns.scatterplot(x=mds_scaled[:,0],y=mds_scaled[:,1])
plt.legend(loc='best')
plt.title('MDS')

plt.subplot(122)
sns.scatterplot(x=nmds_scaled[:,0],y=nmds_scaled[:,1])
plt.legend(loc='best')
plt.title('nMDS')


```

    No handles with labels found to put in legend.
    No handles with labels found to put in legend.
    




    Text(0.5, 1.0, 'MDS')




    
    


# 대응분석
- 카이제곱 검정은 두 범주형 변수과 의 연관성 여부를 결정하는 것이며, 구체적으로 두 변수가 가지고 있는 범주들 사이의 관계를 살펴볼 수는 없다. 이러한 문제점을 해결해 주는 통계적 기법이 대응분석이다. 
- 대응분석은 두 개 이상의 범주 군 사이의 상관성을 분석하는 기법이라 할 수 있다.


```python
from sklearn.cross_decomposition import CCA
X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]
Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
cca = CCA(n_components=1)
cca.fit(X, Y)

X_c, Y_c = cca.transform(X, Y)
```


```python
X_c
```




    array([[-1.3373174 ],
           [-1.10847164],
           [ 0.40763151],
           [ 2.03815753]])




```python
Y_c
```




    array([[-0.85511537],
           [-0.70878547],
           [ 0.26065014],
           [ 1.3032507 ]])




```python
X_test = [[2,4,5]]
Y_test = [[0.4, 5,5]]
cca.predict(X_test, Y_test)
```




    array([[14.04112465, 14.35630774]])



# 시계열 분석 - fbprophet
- prophet은 페이스북에서 개발한 시계열 예측 패키지다. ARIMA와 같은 확률론적이고 이론적인 모형이 아니라 몇가지 경험적 규칙(heuristic rule)을 사용하는 단순 회귀모형이지만 단기적 예측에서는 큰 문제 없이 사용할 수 있다.


```python
import pandas as pd

url = "https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv"
df = pd.read_csv(url)
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2900</th>
      <td>2016-01-16</td>
      <td>7.817223</td>
    </tr>
    <tr>
      <th>2901</th>
      <td>2016-01-17</td>
      <td>9.273878</td>
    </tr>
    <tr>
      <th>2902</th>
      <td>2016-01-18</td>
      <td>10.333775</td>
    </tr>
    <tr>
      <th>2903</th>
      <td>2016-01-19</td>
      <td>9.125871</td>
    </tr>
    <tr>
      <th>2904</th>
      <td>2016-01-20</td>
      <td>8.891374</td>
    </tr>
  </tbody>
</table>
</div>




```python
from fbprophet import Prophet
m = Prophet()
m.fit(df)
```

    INFO:numexpr.utils:NumExpr defaulting to 4 threads.
    INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.
    




    <fbprophet.forecaster.Prophet at 0x1ec2c8e2308>




```python
future = m.make_future_dataframe(periods=365)
future.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3265</th>
      <td>2017-01-15</td>
    </tr>
    <tr>
      <th>3266</th>
      <td>2017-01-16</td>
    </tr>
    <tr>
      <th>3267</th>
      <td>2017-01-17</td>
    </tr>
    <tr>
      <th>3268</th>
      <td>2017-01-18</td>
    </tr>
    <tr>
      <th>3269</th>
      <td>2017-01-19</td>
    </tr>
  </tbody>
</table>
</div>



- yhat이 예측값


```python
forecast = m.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ds</th>
      <th>yhat</th>
      <th>yhat_lower</th>
      <th>yhat_upper</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3265</th>
      <td>2017-01-15</td>
      <td>8.203217</td>
      <td>7.465164</td>
      <td>8.969418</td>
    </tr>
    <tr>
      <th>3266</th>
      <td>2017-01-16</td>
      <td>8.528203</td>
      <td>7.758541</td>
      <td>9.264207</td>
    </tr>
    <tr>
      <th>3267</th>
      <td>2017-01-17</td>
      <td>8.315601</td>
      <td>7.668485</td>
      <td>9.087909</td>
    </tr>
    <tr>
      <th>3268</th>
      <td>2017-01-18</td>
      <td>8.148207</td>
      <td>7.397069</td>
      <td>8.896107</td>
    </tr>
    <tr>
      <th>3269</th>
      <td>2017-01-19</td>
      <td>8.160103</td>
      <td>7.498117</td>
      <td>8.846597</td>
    </tr>
  </tbody>
</table>
</div>




```python
fig1 = m.plot(forecast)
```


    



```python
fig2 = m.plot_components(forecast)

```


    
    



```python

```


```python

```


```python

```

# 연관성 분석 = 장바구니분석


```python
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
```

- 구매한 물건이 담긴 데이터


```python
dataset = [['Milk', 'Onion', 'Nutmeg', 'Eggs', 'Yogurt'],
           ['Onion', 'Nutmeg', 'Eggs', 'Yogurt'],
           ['Milk', 'Apple', 'Eggs'],
           ['Milk', 'Unicorn', 'Corn', 'Yogurt'],
           ['Corn', 'Onion', 'Onion', 'Ice cream', 'Eggs']]
```

- Encoding을 해 줌 : 인스턴스 생성 -> fit -> transform


```python
te = TransactionEncoder()
te_ary = te.fit(dataset).transform(dataset)
df = pd.DataFrame(te_ary, columns=te.columns_)
```


```python
frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)
## parameter
# max_len=3 : 아이템 조합이 3개까지 제한
```


```python
frequent_itemsets # 전체 구매 데이터 중 해당 itemset이 포함된 확률
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>support</th>
      <th>itemsets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.8</td>
      <td>(Eggs)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.6</td>
      <td>(Milk)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.6</td>
      <td>(Onion)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.6</td>
      <td>(Yogurt)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.6</td>
      <td>(Eggs, Onion)</td>
    </tr>
  </tbody>
</table>
</div>




```python
association_rules(frequent_itemsets, metric="lift", min_threshold=1) # metric 기준 min_threshold 이상
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>antecedents</th>
      <th>consequents</th>
      <th>antecedent support</th>
      <th>consequent support</th>
      <th>support</th>
      <th>confidence</th>
      <th>lift</th>
      <th>leverage</th>
      <th>conviction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>(Eggs)</td>
      <td>(Onion)</td>
      <td>0.8</td>
      <td>0.6</td>
      <td>0.6</td>
      <td>0.75</td>
      <td>1.25</td>
      <td>0.12</td>
      <td>1.6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>(Onion)</td>
      <td>(Eggs)</td>
      <td>0.6</td>
      <td>0.8</td>
      <td>0.6</td>
      <td>1.00</td>
      <td>1.25</td>
      <td>0.12</td>
      <td>inf</td>
    </tr>
  </tbody>
</table>
</div>



**첫 줄 해석**
- antencedents와 consequents가 있는데 각각의 support를 보여줌. 
- 그리고 조합의 support, confidence, lift를 보여주는데 
- confidence : Onion을 사는 고객 중 Eggs+Onion이 75%
- lift: 1이면 서로 영향이 없는 것. 그냥 Onion을 사는 것보다 Egg를 샀을 때 구매율이 1.25배 높아진다는 소리

# 요인분석


```python
from sklearn.datasets import load_digits
X, _ = load_digits(return_X_y=True)
```


```python
X.shape
```




    (1797, 64)




```python
X
```




    array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ..., 10.,  0.,  0.],
           [ 0.,  0.,  0., ..., 16.,  9.,  0.],
           ...,
           [ 0.,  0.,  1., ...,  6.,  0.,  0.],
           [ 0.,  0.,  2., ..., 12.,  0.,  0.],
           [ 0.,  0., 10., ..., 12.,  1.,  0.]])




```python
from sklearn.decomposition import FactorAnalysis
transformer = FactorAnalysis(n_components=5, random_state=0)
X_transformed = transformer.fit_transform(X)
X_transformed.shape
```




    (1797, 5)




```python
X_transformed
```




    array([[-0.15740939,  0.30545241,  1.88630105,  0.89678859, -0.17029374],
           [-0.87586253,  0.13827044, -1.75345561, -0.83281075, -0.74288303],
           [-0.99892214, -0.43236642, -1.22222905, -0.82192628, -0.77094974],
           ...,
           [-0.70066938,  0.09868465, -0.99651414, -0.14234655, -0.61502155],
           [-0.37322424, -0.18103725,  1.07294051, -0.6538424 , -0.28351881],
           [ 0.64021206, -0.87404644, -0.04237855,  0.32160612, -0.47697811]])



# 사회연결망 분석(SNA)


```python
# 모르겠음...
```
