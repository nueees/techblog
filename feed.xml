<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://nueees.github.io/techblog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://nueees.github.io/techblog/" rel="alternate" type="text/html" /><updated>2021-11-25T21:18:07-06:00</updated><id>https://nueees.github.io/techblog/feed.xml</id><title type="html">Cho’s Tech blog</title><subtitle>A challenge-loving junior data engineer.</subtitle><entry><title type="html">QDA</title><link href="https://nueees.github.io/techblog/statistics/python/2021/12/06/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D2.html" rel="alternate" type="text/html" title="QDA" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://nueees.github.io/techblog/statistics/python/2021/12/06/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D2</id><author><name></name></author><category term="statistics" /><category term="python" /><summary type="html">06_통계분석_2 판별분석 QDA(Quadratic Discriminant Analysis) LDA는 선형 판별분석 - 07_기계학습_1에서 차원축소를 다룸 여기서는 이차 판별분석으로 “분류”하는 예시 모든 클래스k에 대하여 동일한 covariance matrix를 가정했던 LDA와 달리 QDA는 k클래스 마다 각각의 covariance matrix를 가지게 함 k의 클래스 별 공분산 구조가 확연히 다를때 사용 설명변수가 많아질 수록 추정하는 모수도 많아지므로 샘플이 많이 필요 (+속도 저하) 샘플이 적어서 분산을 줄이는 것이 중요할 경우 LDA를, 샘플이 많아서 분산에 대한 우려가 적을때, 혹은 공분산에 대한 가정이 비현실적으로 판단될 때에는 QDA를 사용 # 데이터 생성 import numpy as np X = np.array([[-1,-1], [-2,-1], [-3,-2], [1,1], [2,1], [3,2]]) y = np.array([1,1,1,2,2,2]) from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis clf2 = QuadraticDiscriminantAnalysis() clf2.fit(X,y) QuadraticDiscriminantAnalysis() clf2.predict([[-0.8,-1]]) array([1]) MultiDimensional Scaling (MDS) 다차원척도법 여러 차원 축소 기법 중 하나 종류 1) 계량적: PCoA (principle coordinates analysis) Classical multidimensional scaling으로, PCA (principle component analysis)와 매우 비슷하나, PCA: Euclidean 거리 사용하고 선형 관계 있으면 사용 (대부분 geological data) PCoA: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 있으면 사용 (biogeographic data) 2) 비계량적: Non-MultiDimensional Scaling (NMDS) NMDS: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 없으면 사용 (어떤 지역 species 개체 수 많은 지) 계량적(구간척도, 비율척도) mds 객체 생설할 때 dissimilarity로 euclidean할지 precomputed 미리 계산된 걸로 할 지 정하고 mds.fit_transform 옵션에서 계산된 manhattan_distances 넣어주면 됨 from sklearn.manifold import MDS from matplotlib import pyplot as plt import sklearn.datasets as dt import seaborn as sns import numpy as np from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances from matplotlib.offsetbox import OffsetImage, AnnotationBbox from sklearn.datasets import load_digits X2 = np.array([[0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 0], [0, 1, 1]]) mds2 = MDS(random_state=0) X2_transform = mds2.fit_transform(X2) print(X2_transform) stress2 = mds2.stress_ print(stress2) [[ 0.72521687 0.52943352] [ 0.61640884 -0.48411805] [-0.9113603 -0.47905115] [-0.2190564 0.71505714] [-0.21120901 -0.28132146]] 0.18216844548575456 stress는 잘 피팅되었는지 검증용으로, 계산된 거리가 dissimilarity 차이를 보여주는데 stress가 통상 0.2 이상이면 차원 높여야 함 colors = ['r', 'g', 'b', 'c', 'm'] size = [64, 64, 64, 64, 64] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(121, projection='3d') plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors) plt.title('Original Points') ax = fig.add_subplot(122) plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors) plt.title('Embedding in 2D') fig.subplots_adjust(wspace=.4, hspace=0.5) plt.show() dist_manhattan = manhattan_distances(X2) mds3 = MDS(dissimilarity='precomputed', random_state=0) # Get the embeddings X2_transform_L1 = mds3.fit_transform(dist_manhattan) print(X2_transform_L1) print(mds3.stress_) [[ 0.9847767 0.84738596] [ 0.81047787 -0.37601578] [-1.104849 -1.06040621] [-0.29311254 0.87364759] [-0.39729303 -0.28461157]] 0.4047164940033806 fig = plt.figure(2, (15,6)) ax = fig.add_subplot(131, projection='3d') plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors) plt.title('Original Points') ax = fig.add_subplot(132) plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors) plt.title('Embedding in 2D') fig.subplots_adjust(wspace=.4, hspace=0.5) ax = fig.add_subplot(133) plt.scatter(X2_transform_L1[:,0], X2_transform_L1[:,1], s=size, c=colors) plt.title('Embedding in 2D L1') fig.subplots_adjust(wspace=.4, hspace=0.5) plt.show() # print(load_digits.__doc__) X, y = load_digits(return_X_y=True) X = X[:100] print(X.shape) mds = MDS(n_components=2) X_transformed = mds.fit_transform(X[:100]) print(X_transformed.shape) Y = y[:100] print(Y.size) # print(X_transformed[:5,0]) # print(X_transformed[:5,1]) print(mds.stress_) (100, 64) (100, 2) 100 1133807.722583498 colormap = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'w', 'w']) # colormap[Y] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(122) plt.scatter(X_transformed[:,0], X_transformed[:,1], c=colormap[Y]) plt.title('Embedding in 2D') plt.show() nmds = MDS(n_components=2, metric=False) nX_transformed2 = nmds.fit_transform(X) # print(nX_transformed2) nX_transformed2 *= np.sqrt((X ** 2).sum()) / np.sqrt((nX_transformed ** 2).sum()) # print(nX_transformed2) Y = y[:100] # print(Y.size) # print(nX_transformed[:5,0]) # print(nX_transformed[:5,1]) # print(nmds.stress_) colormap = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'w', 'w']) # colormap[Y] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(122) plt.scatter(nX_transformed2[:,0], nX_transformed2[:,1], c=colormap[Y]) plt.title('Embedding in non mds 2D') plt.show() non-metric MDS: 다차원척도법 비계량적(순서척도) 1) 차이에 대해 수치화(quantified) 한 값을 얻기 힘들때, 순서만 알 수 있을 때 사용 예) 검정색-진회색-연회색-흰색… 중 가장 밝은 색, 빈도 수가 많은 데이터 2) 유클리디안 외 user-selected 거리 메트릭을 사용하고 싶을 때 (Jaccard,…) Metric = False 옵션 주면 됨. 3) 차원이 미리 결정되어야 하고, local minima(지역 최소값) 수렴 가능성이 있고, 시간 오래 걸리는 게 단점 from sklearn.preprocessing import MinMaxScaler from mpl_toolkits import mplot3d df = pd.read_csv('../data/yeast-transcriptomics/SC_expression.csv') df = df.iloc[:,1:] # print(df.corr()) # print(df.T) df1 = df.T.values sc = MinMaxScaler() scaled = sc.fit_transform(df1) # print(scaled) mds = MDS(n_components=2) mds_scaled = mds.fit_transform(scaled) nmds = MDS(n_components=2, metric=False) nmds_scaled = nmds.fit_transform(scaled) plt.subplot(121) sns.scatterplot(x=mds_scaled[:,0],y=mds_scaled[:,1]) plt.legend(loc='best') plt.title('MDS') plt.subplot(122) sns.scatterplot(x=nmds_scaled[:,0],y=nmds_scaled[:,1]) plt.legend(loc='best') plt.title('nMDS') No handles with labels found to put in legend. No handles with labels found to put in legend. Text(0.5, 1.0, 'MDS') 대응분석 카이제곱 검정은 두 범주형 변수과 의 연관성 여부를 결정하는 것이며, 구체적으로 두 변수가 가지고 있는 범주들 사이의 관계를 살펴볼 수는 없다. 이러한 문제점을 해결해 주는 통계적 기법이 대응분석이다. 대응분석은 두 개 이상의 범주 군 사이의 상관성을 분석하는 기법이라 할 수 있다. from sklearn.cross_decomposition import CCA X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]] Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]] cca = CCA(n_components=1) cca.fit(X, Y) X_c, Y_c = cca.transform(X, Y) X_c array([[-1.3373174 ], [-1.10847164], [ 0.40763151], [ 2.03815753]]) Y_c array([[-0.85511537], [-0.70878547], [ 0.26065014], [ 1.3032507 ]]) X_test = [[2,4,5]] Y_test = [[0.4, 5,5]] cca.predict(X_test, Y_test) array([[14.04112465, 14.35630774]]) 시계열 분석 - fbprophet prophet은 페이스북에서 개발한 시계열 예측 패키지다. ARIMA와 같은 확률론적이고 이론적인 모형이 아니라 몇가지 경험적 규칙(heuristic rule)을 사용하는 단순 회귀모형이지만 단기적 예측에서는 큰 문제 없이 사용할 수 있다. import pandas as pd url = &quot;https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv&quot; df = pd.read_csv(url) df.tail() ds y 2900 2016-01-16 7.817223 2901 2016-01-17 9.273878 2902 2016-01-18 10.333775 2903 2016-01-19 9.125871 2904 2016-01-20 8.891374 from fbprophet import Prophet m = Prophet() m.fit(df) INFO:numexpr.utils:NumExpr defaulting to 4 threads. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. &amp;lt;fbprophet.forecaster.Prophet at 0x1ec2c8e2308&amp;gt; future = m.make_future_dataframe(periods=365) future.tail() ds 3265 2017-01-15 3266 2017-01-16 3267 2017-01-17 3268 2017-01-18 3269 2017-01-19 yhat이 예측값 forecast = m.predict(future) forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail() ds yhat yhat_lower yhat_upper 3265 2017-01-15 8.203217 7.465164 8.969418 3266 2017-01-16 8.528203 7.758541 9.264207 3267 2017-01-17 8.315601 7.668485 9.087909 3268 2017-01-18 8.148207 7.397069 8.896107 3269 2017-01-19 8.160103 7.498117 8.846597 fig1 = m.plot(forecast) fig2 = m.plot_components(forecast) 연관성 분석 = 장바구니분석 import pandas as pd from mlxtend.preprocessing import TransactionEncoder from mlxtend.frequent_patterns import apriori, association_rules 구매한 물건이 담긴 데이터 dataset = [['Milk', 'Onion', 'Nutmeg', 'Eggs', 'Yogurt'], ['Onion', 'Nutmeg', 'Eggs', 'Yogurt'], ['Milk', 'Apple', 'Eggs'], ['Milk', 'Unicorn', 'Corn', 'Yogurt'], ['Corn', 'Onion', 'Onion', 'Ice cream', 'Eggs']] Encoding을 해 줌 : 인스턴스 생성 -&amp;gt; fit -&amp;gt; transform te = TransactionEncoder() te_ary = te.fit(dataset).transform(dataset) df = pd.DataFrame(te_ary, columns=te.columns_) frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True) ## parameter # max_len=3 : 아이템 조합이 3개까지 제한 frequent_itemsets # 전체 구매 데이터 중 해당 itemset이 포함된 확률 support itemsets 0 0.8 (Eggs) 1 0.6 (Milk) 2 0.6 (Onion) 3 0.6 (Yogurt) 4 0.6 (Eggs, Onion) association_rules(frequent_itemsets, metric=&quot;lift&quot;, min_threshold=1) # metric 기준 min_threshold 이상 antecedents consequents antecedent support consequent support support confidence lift leverage conviction 0 (Eggs) (Onion) 0.8 0.6 0.6 0.75 1.25 0.12 1.6 1 (Onion) (Eggs) 0.6 0.8 0.6 1.00 1.25 0.12 inf 첫 줄 해석 antencedents와 consequents가 있는데 각각의 support를 보여줌. 그리고 조합의 support, confidence, lift를 보여주는데 confidence : Onion을 사는 고객 중 Eggs+Onion이 75% lift: 1이면 서로 영향이 없는 것. 그냥 Onion을 사는 것보다 Egg를 샀을 때 구매율이 1.25배 높아진다는 소리 요인분석 from sklearn.datasets import load_digits X, _ = load_digits(return_X_y=True) X.shape (1797, 64) X array([[ 0., 0., 5., ..., 0., 0., 0.], [ 0., 0., 0., ..., 10., 0., 0.], [ 0., 0., 0., ..., 16., 9., 0.], ..., [ 0., 0., 1., ..., 6., 0., 0.], [ 0., 0., 2., ..., 12., 0., 0.], [ 0., 0., 10., ..., 12., 1., 0.]]) from sklearn.decomposition import FactorAnalysis transformer = FactorAnalysis(n_components=5, random_state=0) X_transformed = transformer.fit_transform(X) X_transformed.shape (1797, 5) X_transformed array([[-0.15740939, 0.30545241, 1.88630105, 0.89678859, -0.17029374], [-0.87586253, 0.13827044, -1.75345561, -0.83281075, -0.74288303], [-0.99892214, -0.43236642, -1.22222905, -0.82192628, -0.77094974], ..., [-0.70066938, 0.09868465, -0.99651414, -0.14234655, -0.61502155], [-0.37322424, -0.18103725, 1.07294051, -0.6538424 , -0.28351881], [ 0.64021206, -0.87404644, -0.04237855, 0.32160612, -0.47697811]]) 사회연결망 분석(SNA) # 모르겠음...</summary></entry><entry><title type="html">CDA</title><link href="https://nueees.github.io/techblog/statistics/python/2021/12/05/CDA.html" rel="alternate" type="text/html" title="CDA" /><published>2021-12-05T00:00:00-06:00</published><updated>2021-12-05T00:00:00-06:00</updated><id>https://nueees.github.io/techblog/statistics/python/2021/12/05/CDA</id><author><name></name></author><category term="statistics" /><category term="python" /><summary type="html">kaggle/GREG HAMEL/Hypothesis Testing 05_CDA(Confirmatory Data Analysis) 어떤 현상이 ‘우연’인지 그렇지 않은지를 확인하기 위함 import scipy.stats as spst import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import math Comparative Type Test One Sample T-Test 샘플 A의 평균이 x와 다른가? - p-value가 낮으면 ‘다르다!’ 귀무가설 : 같다 대립가설 : 다르다 print(spst.poisson.rvs.__doc__) Random variates of given type. Parameters ---------- arg1, arg2, arg3,... : array_like The shape parameter(s) for the distribution (see docstring of the instance object for more information). loc : array_like, optional Location parameter (default=0). size : int or tuple of ints, optional Defining number of random variates (Default is 1). Note that `size` has to be given as keyword, not as positional argument. random_state : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional This parameter defines the object to use for drawing random variates. If `random_state` is `None` the `~np.random.RandomState` singleton is used. If `random_state` is an int, a new ``RandomState`` instance is used, seeded with random_state. If `random_state` is already a ``RandomState`` or ``Generator`` instance, then that object is used. Default is None. Returns ------- rvs : ndarray or scalar Random variates of given `size`. np.random.seed(6) population_ages1 = spst.poisson.rvs(loc=18, mu=35, size=150000) # loc: lowest x value, mu: middle of distribution population_ages2 = spst.poisson.rvs(loc=18, mu=10, size=100000) population_ages = np.concatenate((population_ages1, population_ages2)) minnesota_ages1 = spst.poisson.rvs(loc=18, mu=30, size=30) minnesota_ages2 = spst.poisson.rvs(loc=18, mu=10, size=20) minnesota_ages = np.concatenate((minnesota_ages1, minnesota_ages2)) print( population_ages.mean()) print( minnesota_ages.mean()) 43.000112 39.26 spst.ttest_1samp(a = minnesota_ages, # Sample data popmean = population_ages.mean()) # Pop mean Ttest_1sampResult(statistic=-2.5742714883655027, pvalue=0.013118685425061678) pvalue : 0.013118685425061678 정직한 설명 : 귀무가설이 참이라는 전제하에 이렇게 데이터가 관찰될 확률이 0.0….01%정도라는 뜻이다. 발칙한 설명 : 기존 배너보다 나을 확률이 99%를 넘는다는 뜻이다. statistic : 2.5742714883655027 신호/노이즈, 즉 신호가 노이즈보다 2.57배 높다는 뜻 vals = spst.t.ppf([0.05, 0.95] # Quantile to check , 49)# minnesota_ages' Degrees of freedom print(vals) [-1.67655089 1.67655089] # print(spst.t.ppf.__doc__) # print(spst.t.cdf.__doc__) spst.t.cdf(vals, 49) array([0.05, 0.95]) 1) For a lower-tailed test, p-value = cdf(test_statistic) 2) For an upper-tailed test, p-value = 1 - cdf(test_statistic) 3) For a two-tailed test, p-value = 2 * (1 - cdf(|test_statistic|)) spst.t.cdf(x= -2.5742, # T-test statistic df= 49) * 2 # two-tailed test 0.013121066545690117 t-test의 p-value 0.013121066545690117과 같음 sigma = minnesota_ages.std()/math.sqrt(50) # Sample stdev/sample size spst.t.interval(0.95, # Confidence level 95로 할 때 43 평균이 포함 안됨 df = 49, # Degrees of freedom loc = minnesota_ages.mean(), # Sample mean scale= sigma) # Standard dev estimate (36.369669080722176, 42.15033091927782) spst.t.interval(0.99, # 99로 하면 43 평균이 포함됨 df = 49, loc = minnesota_ages.mean(), scale= sigma) (35.40547994092107, 43.11452005907893) Two Sample T-Test A와 B가 다른가? p-value가 낮으면 ‘다르다’! np.random.seed(12) wisconsin_ages1 = spst.poisson.rvs(loc=18, mu=33, size=30) wisconsin_ages2 = spst.poisson.rvs(loc=18, mu=13, size=20) wisconsin_ages = np.concatenate((wisconsin_ages1, wisconsin_ages2)) print( wisconsin_ages.mean() ) 42.8 # 분산의 동일성 검정 spst.levene(minnesota_ages, wisconsin_ages) # pvalue가 통상적인 기준인 0.1보다 크면 분산 동일하다는 가정 받아들임. LeveneResult(statistic=0.028047686012903684, pvalue=0.8673418686154897) spst.ttest_ind(minnesota_ages, wisconsin_ages, equal_var=True) # equal_var는 등분산 여부인데, 모르면 False Ttest_indResult(statistic=-1.7083870793286842, pvalue=0.09073015386514256) minnesota와 wisconsin 두 그룹이 동일할 경우, 다른(차이가 나는) 샘플 데이터를 볼 가능성은 p값인 0.09073015386514256 확률임. p값이 유의수준 5%보다 크기 때문에 두 그룹은 같다고 판단. Paired T-Test 한 집단에서 전-후 비교(Before-After) - 당연히 p-value 낮으면 다른 것 np.random.seed(11) before= spst.norm.rvs(scale=30, loc=250, size=100) after = before + spst.norm.rvs(scale=5, loc=-1.25, size=100) weight_df = pd.DataFrame({&quot;weight_before&quot;:before, &quot;weight_after&quot;:after, &quot;weight_change&quot;:after-before}) weight_df.describe() weight_before weight_after weight_change count 100.000000 100.000000 100.000000 mean 250.345546 249.115171 -1.230375 std 28.132539 28.422183 4.783696 min 170.400443 165.913930 -11.495286 25% 230.421042 229.148236 -4.046211 50% 250.830805 251.134089 -1.413463 75% 270.637145 268.927258 1.738673 max 314.700233 316.720357 9.759282 plt.subplot(1,2,1) weight_df['weight_before'].plot(kind = 'box', ylim = (100,300)) plt.subplot(1,2,2) weight_df['weight_after'].plot(kind = 'box', ylim = (100,300)) &amp;lt;AxesSubplot:&amp;gt; spst.ttest_rel(a = before, b = after) Ttest_relResult(statistic=2.5720175998568284, pvalue=0.011596444318439857) ANOVA(Analysis of Variance) 귀무가설 : A,B,C 다 똑같다 대립가설 : A,B,C 중 ‘무언가 하나는’ 다를 것이다. 대립가설 조심, A, B, C 중 뭐가 다르고, 얼마나 다르고 등은 전혀 알 수 없다. 따로 계산해야 한다. [ 분산분석 검정의 가정사항 (assumptions of ANOVA test) ] (1) 독립성: 각 샘플 데이터는 서로 독립이다. (2) 정규성: 각 샘플 데이터는 정규분포를 따르는 모집단으로 부터 추출되었다. (3) 등분산성: 그룹들의 모집단의 분산은 모두 동일하다. np.random.seed(12) races = [&quot;asian&quot;,&quot;black&quot;,&quot;hispanic&quot;,&quot;other&quot;,&quot;white&quot;] # Generate random data voter_race = np.random.choice(a= races, p = [0.05, 0.15 ,0.25, 0.05, 0.5], size=1000) voter_age = spst.poisson.rvs(loc=18, mu=30, size=1000) # Group age data by race voter_frame = pd.DataFrame({&quot;race&quot;:voter_race,&quot;age&quot;:voter_age}) groups = voter_frame.groupby(&quot;race&quot;).groups print(voter_frame) # Etract individual groups asian = voter_age[groups[&quot;asian&quot;]] black = voter_age[groups[&quot;black&quot;]] hispanic = voter_age[groups[&quot;hispanic&quot;]] other = voter_age[groups[&quot;other&quot;]] white = voter_age[groups[&quot;white&quot;]] print(asian) # 첫번째 방법 Perform the ANOVA spst.f_oneway(asian, black, hispanic, other, white) race age 0 black 51 1 white 49 2 hispanic 51 3 white 48 4 asian 56 .. ... ... 995 white 47 996 asian 40 997 white 50 998 white 51 999 hispanic 43 [1000 rows x 2 columns] [56 52 37 50 53 47 56 43 46 54 45 54 42 44 55 50 45 49 51 57 56 46 43 53 48 54 54 44 40 46 51 52 44 54 43 44 53 42 54 44 59 47 54 40] F_onewayResult(statistic=1.7744689357329695, pvalue=0.13173183201930463) f통계량 1.774와 p값 0.1317은 각 그룹의 평균이 큰 차이가 없다는 것을 보여줌. # 두번째 방법 statsmodels lib 사용 import statsmodels.api as sm from statsmodels.formula.api import ols model = ols('age ~ race', # Model formula data = voter_frame).fit() anova_result = sm.stats.anova_lm(model, typ=2) print (anova_result) sum_sq df F PR(&amp;gt;F) race 199.369 4.0 1.774469 0.131732 Residual 27948.102 995.0 NaN NaN 정석적인 해석 : 귀무가설이 참일 때, 이러한 데이터가 관측될 확률은 1.85% 정도이다. (1.85%확률을 뚫고 이런 데이터가 관측될 수도 있다.) 발칙한 해석 : 뭔가 하나는 차이가 날 확률이 98%는 넘는다. from statsmodels.stats.multicomp import pairwise_tukeyhsd tukey = pairwise_tukeyhsd(endog=voter_age, # Data groups=voter_race, # Groups alpha=0.05) # Significance level tukey.plot_simultaneous() # Plot group confidence intervals plt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=&quot;red&quot;) plt.show() tukey.summary() # See test summary C:\Users\Administrator\anaconda3\lib\site-packages\statsmodels\sandbox\stats\multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, '')) Multiple Comparison of Means - Tukey HSD, FWER=0.05 group1 group2 meandiff p-adj lower upper reject asian black -1.3353 0.5735 -3.8242 1.1535 False asian hispanic -0.7593 0.9 -3.1315 1.6129 False asian other -0.0264 0.9 -3.0202 2.9674 False asian white -1.4184 0.4336 -3.6932 0.8564 False black hispanic 0.576 0.8145 -0.9362 2.0882 False black other 1.309 0.5494 -1.0622 3.6801 False black white -0.0831 0.9 -1.4374 1.2713 False hispanic other 0.733 0.8996 -1.5154 2.9813 False hispanic white -0.6591 0.4974 -1.7847 0.4665 False other white -1.392 0.3912 -3.5374 0.7533 False Associative Type Test Correlation Coefficient 귀무가설 : X와 Y는 상관이 없다.(상관계수 = 0) 대립가설 : 상관계수가 0이 아니다. X와 Y를 별도로 시각화 해 볼 것 spst.pearsonr(wisconsin_ages,minnesota_ages) # X와 Y의 상관계수와 p-value (0.7749658250313621, 3.969124248684934e-11) 결과는 튜플로 나오는데 튜플의 첫 번째 값 : 상관계수를 뜻한다. 두 데이터의 선형성의 정도를 나타낸다. p-value는 상관계수가 우연에 의해 일어나진 않았는지 판단한다. 귀무가설 : 상관 계수가 0이다. 대립가설 : 상관 계수가 0이 아니다. pvalue가 0.57로 0.05보다 크기 때문에 귀무가설 받아들임 피어슨상관계쑤는 -0.25이므로 약한 음의 상관관계가 있음 교차분석(Chisquare_test) 티셔츠 구매여부와 반바지 구매여부는 관계가.. 있을까?! 귀무가설 : 티셔츠 구매와 바지 구매는 별개이다.(독립이다) 대립가설 : 티셔츠를 구매와 바지는 독립이 아니다.관련이 있다.. # 고객별 셔츠와 바지의 구매 여부 데이터 shirt_raw = {'shirts':[0,0,0,0,0,0,1,1,0,0,1,1,1,1,1,0,0,0,0,0,1], 'pants':[0,0,1,0,0,0,1,0,1,1,1,1,1,1,1,0,0,0,1,0,1] } shirt_df = pd.DataFrame(shirt_raw) print(shirt_df.head(10)) shirts pants 0 0 0 1 0 0 2 0 1 3 0 0 4 0 0 5 0 0 6 1 1 7 1 0 8 0 1 9 0 1 # 데이터의 Crosstable contingency = pd.crosstab(shirt_df['shirts'], shirt_df['pants']) contingency pants 0 1 shirts 0 9 4 1 1 7 chiresult = spst.chi2_contingency(contingency) # 카이제곱 검정 # 결과 : 튜플로 4개 값 출력됨 print(&quot;카이제곱통계량 : {}&quot;.format(chiresult[0])) print(&quot;p-value : {:.20f}&quot;.format(chiresult[1])) print(&quot;자유도 : {}&quot;.format(chiresult[2])) print(&quot;기대 빈도 분할표: \n&quot;, chiresult[3] ) #귀무가설에 대한 기대빈도. 카이제곱통계량 : 4.317941433566433 p-value : 0.03771251880967476516 자유도 : 1 기대 빈도 분할표: [[6.19047619 6.80952381] [3.80952381 4.19047619]] 유의수준 0.05 하에 p-value가 매우 낮으므로 두 집단간 차이가 있다(바지 구매는 셔츠 구매와 관련이 있다) pants 0과 pants 1 그룹을 비교했을 때 shirts 0, 1의 차이가 있다. pants가 0, 1, 2였다면 0, 1, 2에 따라 차이가 있다라고 해석할 수 있음</summary></entry><entry><title type="html">ELB (Elastic Load Balancing)</title><link href="https://nueees.github.io/techblog/aws/elb/2021/11/05/aws-elb.html" rel="alternate" type="text/html" title="ELB (Elastic Load Balancing)" /><published>2021-11-05T00:00:00-05:00</published><updated>2021-11-05T00:00:00-05:00</updated><id>https://nueees.github.io/techblog/aws/elb/2021/11/05/aws-elb</id><author><name></name></author><category term="aws" /><category term="elb" /><summary type="html">AWS Documentation</summary></entry><entry><title type="html">ECS (Elastic Container Service)</title><link href="https://nueees.github.io/techblog/aws/ecs/2021/11/04/aws-ecs.html" rel="alternate" type="text/html" title="ECS (Elastic Container Service)" /><published>2021-11-04T00:00:00-05:00</published><updated>2021-11-04T00:00:00-05:00</updated><id>https://nueees.github.io/techblog/aws/ecs/2021/11/04/aws-ecs</id><author><name></name></author><category term="aws" /><category term="ecs" /><summary type="html">AWS Documentation</summary></entry><entry><title type="html">DynamoDB</title><link href="https://nueees.github.io/techblog/aws/dynamodb/2021/11/03/aws-dynamodb.html" rel="alternate" type="text/html" title="DynamoDB" /><published>2021-11-03T00:00:00-05:00</published><updated>2021-11-03T00:00:00-05:00</updated><id>https://nueees.github.io/techblog/aws/dynamodb/2021/11/03/aws-dynamodb</id><author><name></name></author><category term="aws" /><category term="dynamodb" /><summary type="html">AWS Documentation</summary></entry></feed>