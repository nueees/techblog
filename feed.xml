<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://nueees.github.io/techblog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://nueees.github.io/techblog/" rel="alternate" type="text/html" /><updated>2021-11-27T18:06:46-06:00</updated><id>https://nueees.github.io/techblog/feed.xml</id><title type="html">Cho’s Tech blog</title><subtitle>A challenge-loving junior data engineer.</subtitle><entry><title type="html">QDA</title><link href="https://nueees.github.io/techblog/statistics/python/2021/12/06/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D2.html" rel="alternate" type="text/html" title="QDA" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://nueees.github.io/techblog/statistics/python/2021/12/06/%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D2</id><author><name></name></author><category term="statistics" /><category term="python" /><summary type="html">06_통계분석_2 판별분석 QDA(Quadratic Discriminant Analysis) LDA는 선형 판별분석 - 07_기계학습_1에서 차원축소를 다룸 여기서는 이차 판별분석으로 “분류”하는 예시 모든 클래스k에 대하여 동일한 covariance matrix를 가정했던 LDA와 달리 QDA는 k클래스 마다 각각의 covariance matrix를 가지게 함 k의 클래스 별 공분산 구조가 확연히 다를때 사용 설명변수가 많아질 수록 추정하는 모수도 많아지므로 샘플이 많이 필요 (+속도 저하) 샘플이 적어서 분산을 줄이는 것이 중요할 경우 LDA를, 샘플이 많아서 분산에 대한 우려가 적을때, 혹은 공분산에 대한 가정이 비현실적으로 판단될 때에는 QDA를 사용 # 데이터 생성 import numpy as np X = np.array([[-1,-1], [-2,-1], [-3,-2], [1,1], [2,1], [3,2]]) y = np.array([1,1,1,2,2,2]) from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis clf2 = QuadraticDiscriminantAnalysis() clf2.fit(X,y) QuadraticDiscriminantAnalysis() clf2.predict([[-0.8,-1]]) array([1]) MultiDimensional Scaling (MDS) 다차원척도법 여러 차원 축소 기법 중 하나 종류 1) 계량적: PCoA (principle coordinates analysis) Classical multidimensional scaling으로, PCA (principle component analysis)와 매우 비슷하나, PCA: Euclidean 거리 사용하고 선형 관계 있으면 사용 (대부분 geological data) PCoA: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 있으면 사용 (biogeographic data) 2) 비계량적: Non-MultiDimensional Scaling (NMDS) NMDS: Euclidean 거리 외 다른 측정방법 사용하고 선형 관계 없으면 사용 (어떤 지역 species 개체 수 많은 지) 계량적(구간척도, 비율척도) mds 객체 생설할 때 dissimilarity로 euclidean할지 precomputed 미리 계산된 걸로 할 지 정하고 mds.fit_transform 옵션에서 계산된 manhattan_distances 넣어주면 됨 from sklearn.manifold import MDS from matplotlib import pyplot as plt import sklearn.datasets as dt import seaborn as sns import numpy as np from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances from matplotlib.offsetbox import OffsetImage, AnnotationBbox from sklearn.datasets import load_digits X2 = np.array([[0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 0], [0, 1, 1]]) mds2 = MDS(random_state=0) X2_transform = mds2.fit_transform(X2) print(X2_transform) stress2 = mds2.stress_ print(stress2) [[ 0.72521687 0.52943352] [ 0.61640884 -0.48411805] [-0.9113603 -0.47905115] [-0.2190564 0.71505714] [-0.21120901 -0.28132146]] 0.18216844548575456 stress는 잘 피팅되었는지 검증용으로, 계산된 거리가 dissimilarity 차이를 보여주는데 stress가 통상 0.2 이상이면 차원 높여야 함 colors = ['r', 'g', 'b', 'c', 'm'] size = [64, 64, 64, 64, 64] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(121, projection='3d') plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors) plt.title('Original Points') ax = fig.add_subplot(122) plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors) plt.title('Embedding in 2D') fig.subplots_adjust(wspace=.4, hspace=0.5) plt.show() dist_manhattan = manhattan_distances(X2) mds3 = MDS(dissimilarity='precomputed', random_state=0) # Get the embeddings X2_transform_L1 = mds3.fit_transform(dist_manhattan) print(X2_transform_L1) print(mds3.stress_) [[ 0.9847767 0.84738596] [ 0.81047787 -0.37601578] [-1.104849 -1.06040621] [-0.29311254 0.87364759] [-0.39729303 -0.28461157]] 0.4047164940033806 fig = plt.figure(2, (15,6)) ax = fig.add_subplot(131, projection='3d') plt.scatter(X2[:,0], X2[:,1], zs=X2[:,2], s=size, c=colors) plt.title('Original Points') ax = fig.add_subplot(132) plt.scatter(X2_transform[:,0], X2_transform[:,1], s=size, c=colors) plt.title('Embedding in 2D') fig.subplots_adjust(wspace=.4, hspace=0.5) ax = fig.add_subplot(133) plt.scatter(X2_transform_L1[:,0], X2_transform_L1[:,1], s=size, c=colors) plt.title('Embedding in 2D L1') fig.subplots_adjust(wspace=.4, hspace=0.5) plt.show() # print(load_digits.__doc__) X, y = load_digits(return_X_y=True) X = X[:100] print(X.shape) mds = MDS(n_components=2) X_transformed = mds.fit_transform(X[:100]) print(X_transformed.shape) Y = y[:100] print(Y.size) # print(X_transformed[:5,0]) # print(X_transformed[:5,1]) print(mds.stress_) (100, 64) (100, 2) 100 1133807.722583498 colormap = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'w', 'w']) # colormap[Y] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(122) plt.scatter(X_transformed[:,0], X_transformed[:,1], c=colormap[Y]) plt.title('Embedding in 2D') plt.show() nmds = MDS(n_components=2, metric=False) nX_transformed2 = nmds.fit_transform(X) # print(nX_transformed2) nX_transformed2 *= np.sqrt((X ** 2).sum()) / np.sqrt((nX_transformed ** 2).sum()) # print(nX_transformed2) Y = y[:100] # print(Y.size) # print(nX_transformed[:5,0]) # print(nX_transformed[:5,1]) # print(nmds.stress_) colormap = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w', 'w', 'w']) # colormap[Y] fig = plt.figure(2, (10,4)) ax = fig.add_subplot(122) plt.scatter(nX_transformed2[:,0], nX_transformed2[:,1], c=colormap[Y]) plt.title('Embedding in non mds 2D') plt.show() non-metric MDS: 다차원척도법 비계량적(순서척도) 1) 차이에 대해 수치화(quantified) 한 값을 얻기 힘들때, 순서만 알 수 있을 때 사용 예) 검정색-진회색-연회색-흰색… 중 가장 밝은 색, 빈도 수가 많은 데이터 2) 유클리디안 외 user-selected 거리 메트릭을 사용하고 싶을 때 (Jaccard,…) Metric = False 옵션 주면 됨. 3) 차원이 미리 결정되어야 하고, local minima(지역 최소값) 수렴 가능성이 있고, 시간 오래 걸리는 게 단점 from sklearn.preprocessing import MinMaxScaler from mpl_toolkits import mplot3d df = pd.read_csv('../data/yeast-transcriptomics/SC_expression.csv') df = df.iloc[:,1:] # print(df.corr()) # print(df.T) df1 = df.T.values sc = MinMaxScaler() scaled = sc.fit_transform(df1) # print(scaled) mds = MDS(n_components=2) mds_scaled = mds.fit_transform(scaled) nmds = MDS(n_components=2, metric=False) nmds_scaled = nmds.fit_transform(scaled) plt.subplot(121) sns.scatterplot(x=mds_scaled[:,0],y=mds_scaled[:,1]) plt.legend(loc='best') plt.title('MDS') plt.subplot(122) sns.scatterplot(x=nmds_scaled[:,0],y=nmds_scaled[:,1]) plt.legend(loc='best') plt.title('nMDS') No handles with labels found to put in legend. No handles with labels found to put in legend. Text(0.5, 1.0, 'MDS') 대응분석 카이제곱 검정은 두 범주형 변수과 의 연관성 여부를 결정하는 것이며, 구체적으로 두 변수가 가지고 있는 범주들 사이의 관계를 살펴볼 수는 없다. 이러한 문제점을 해결해 주는 통계적 기법이 대응분석이다. 대응분석은 두 개 이상의 범주 군 사이의 상관성을 분석하는 기법이라 할 수 있다. from sklearn.cross_decomposition import CCA X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]] Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]] cca = CCA(n_components=1) cca.fit(X, Y) X_c, Y_c = cca.transform(X, Y) X_c array([[-1.3373174 ], [-1.10847164], [ 0.40763151], [ 2.03815753]]) Y_c array([[-0.85511537], [-0.70878547], [ 0.26065014], [ 1.3032507 ]]) X_test = [[2,4,5]] Y_test = [[0.4, 5,5]] cca.predict(X_test, Y_test) array([[14.04112465, 14.35630774]]) 시계열 분석 - fbprophet prophet은 페이스북에서 개발한 시계열 예측 패키지다. ARIMA와 같은 확률론적이고 이론적인 모형이 아니라 몇가지 경험적 규칙(heuristic rule)을 사용하는 단순 회귀모형이지만 단기적 예측에서는 큰 문제 없이 사용할 수 있다. import pandas as pd url = &quot;https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv&quot; df = pd.read_csv(url) df.tail() ds y 2900 2016-01-16 7.817223 2901 2016-01-17 9.273878 2902 2016-01-18 10.333775 2903 2016-01-19 9.125871 2904 2016-01-20 8.891374 from fbprophet import Prophet m = Prophet() m.fit(df) INFO:numexpr.utils:NumExpr defaulting to 4 threads. INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. &amp;lt;fbprophet.forecaster.Prophet at 0x1ec2c8e2308&amp;gt; future = m.make_future_dataframe(periods=365) future.tail() ds 3265 2017-01-15 3266 2017-01-16 3267 2017-01-17 3268 2017-01-18 3269 2017-01-19 yhat이 예측값 forecast = m.predict(future) forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail() ds yhat yhat_lower yhat_upper 3265 2017-01-15 8.203217 7.465164 8.969418 3266 2017-01-16 8.528203 7.758541 9.264207 3267 2017-01-17 8.315601 7.668485 9.087909 3268 2017-01-18 8.148207 7.397069 8.896107 3269 2017-01-19 8.160103 7.498117 8.846597 fig1 = m.plot(forecast) fig2 = m.plot_components(forecast) 연관성 분석 = 장바구니분석 import pandas as pd from mlxtend.preprocessing import TransactionEncoder from mlxtend.frequent_patterns import apriori, association_rules 구매한 물건이 담긴 데이터 dataset = [['Milk', 'Onion', 'Nutmeg', 'Eggs', 'Yogurt'], ['Onion', 'Nutmeg', 'Eggs', 'Yogurt'], ['Milk', 'Apple', 'Eggs'], ['Milk', 'Unicorn', 'Corn', 'Yogurt'], ['Corn', 'Onion', 'Onion', 'Ice cream', 'Eggs']] Encoding을 해 줌 : 인스턴스 생성 -&amp;gt; fit -&amp;gt; transform te = TransactionEncoder() te_ary = te.fit(dataset).transform(dataset) df = pd.DataFrame(te_ary, columns=te.columns_) frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True) ## parameter # max_len=3 : 아이템 조합이 3개까지 제한 frequent_itemsets # 전체 구매 데이터 중 해당 itemset이 포함된 확률 support itemsets 0 0.8 (Eggs) 1 0.6 (Milk) 2 0.6 (Onion) 3 0.6 (Yogurt) 4 0.6 (Eggs, Onion) association_rules(frequent_itemsets, metric=&quot;lift&quot;, min_threshold=1) # metric 기준 min_threshold 이상 antecedents consequents antecedent support consequent support support confidence lift leverage conviction 0 (Eggs) (Onion) 0.8 0.6 0.6 0.75 1.25 0.12 1.6 1 (Onion) (Eggs) 0.6 0.8 0.6 1.00 1.25 0.12 inf 첫 줄 해석 antencedents와 consequents가 있는데 각각의 support를 보여줌. 그리고 조합의 support, confidence, lift를 보여주는데 confidence : Onion을 사는 고객 중 Eggs+Onion이 75% lift: 1이면 서로 영향이 없는 것. 그냥 Onion을 사는 것보다 Egg를 샀을 때 구매율이 1.25배 높아진다는 소리 요인분석 from sklearn.datasets import load_digits X, _ = load_digits(return_X_y=True) X.shape (1797, 64) X array([[ 0., 0., 5., ..., 0., 0., 0.], [ 0., 0., 0., ..., 10., 0., 0.], [ 0., 0., 0., ..., 16., 9., 0.], ..., [ 0., 0., 1., ..., 6., 0., 0.], [ 0., 0., 2., ..., 12., 0., 0.], [ 0., 0., 10., ..., 12., 1., 0.]]) from sklearn.decomposition import FactorAnalysis transformer = FactorAnalysis(n_components=5, random_state=0) X_transformed = transformer.fit_transform(X) X_transformed.shape (1797, 5) X_transformed array([[-0.15740939, 0.30545241, 1.88630105, 0.89678859, -0.17029374], [-0.87586253, 0.13827044, -1.75345561, -0.83281075, -0.74288303], [-0.99892214, -0.43236642, -1.22222905, -0.82192628, -0.77094974], ..., [-0.70066938, 0.09868465, -0.99651414, -0.14234655, -0.61502155], [-0.37322424, -0.18103725, 1.07294051, -0.6538424 , -0.28351881], [ 0.64021206, -0.87404644, -0.04237855, 0.32160612, -0.47697811]]) 사회연결망 분석(SNA) # 모르겠음...</summary></entry><entry><title type="html">ELB (Elastic Load Balancing)</title><link href="https://nueees.github.io/techblog/aws/elb/2021/11/05/aws-elb.html" rel="alternate" type="text/html" title="ELB (Elastic Load Balancing)" /><published>2021-11-05T00:00:00-05:00</published><updated>2021-11-05T00:00:00-05:00</updated><id>https://nueees.github.io/techblog/aws/elb/2021/11/05/aws-elb</id><author><name></name></author><category term="aws" /><category term="elb" /><summary type="html">AWS Documentation</summary></entry><entry><title type="html">ECS (Elastic Container Service)</title><link href="https://nueees.github.io/techblog/aws/ecs/2021/11/04/aws-ecs.html" rel="alternate" type="text/html" title="ECS (Elastic Container Service)" /><published>2021-11-04T00:00:00-05:00</published><updated>2021-11-04T00:00:00-05:00</updated><id>https://nueees.github.io/techblog/aws/ecs/2021/11/04/aws-ecs</id><author><name></name></author><category term="aws" /><category term="ecs" /><summary type="html">AWS Documentation</summary></entry><entry><title type="html">DynamoDB</title><link href="https://nueees.github.io/techblog/aws/dynamodb/2021/11/03/aws-dynamodb.html" rel="alternate" type="text/html" title="DynamoDB" /><published>2021-11-03T00:00:00-05:00</published><updated>2021-11-03T00:00:00-05:00</updated><id>https://nueees.github.io/techblog/aws/dynamodb/2021/11/03/aws-dynamodb</id><author><name></name></author><category term="aws" /><category term="dynamodb" /><summary type="html">AWS Documentation</summary></entry><entry><title type="html">S3 (Simple Storage Service)</title><link href="https://nueees.github.io/techblog/aws/s3/2021/11/02/aws-s3.html" rel="alternate" type="text/html" title="S3 (Simple Storage Service)" /><published>2021-11-02T00:00:00-05:00</published><updated>2021-11-02T00:00:00-05:00</updated><id>https://nueees.github.io/techblog/aws/s3/2021/11/02/aws-s3</id><author><name></name></author><category term="aws" /><category term="s3" /><summary type="html">AWS Documentation</summary></entry></feed>