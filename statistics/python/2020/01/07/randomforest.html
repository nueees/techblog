<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">RandomForest</h1><p class="page-description">8장</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-07T00:00:00-06:00" itemprop="datePublished">
        Jan 7, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/techblog/categories/#statistics">statistics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/techblog/categories/#python">python</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#통계분석_4---randomforest">통계분석_4 - RandomForest</a>
<ul>
<li class="toc-entry toc-h2"><a href="#ensemble-methods">Ensemble Methods</a>
<ul>
<li class="toc-entry toc-h3"><a href="#1-voting">1. Voting</a>
<ul>
<li class="toc-entry toc-h4"><a href="#1-1-직접투표-hard-voting">1-1. 직접투표 (Hard voting)</a></li>
<li class="toc-entry toc-h4"><a href="#1-2-간접투표-soft-voting">1-2. 간접투표 (soft voting)</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#2-bagging-bootstrap-aggregating">2. Bagging (bootstrap aggregating)</a>
<ul>
<li class="toc-entry toc-h4"><a href="#2-1-random-forest-classifier-regressor">2-1. Random Forest (Classifier, Regressor)</a>
<ul>
<li class="toc-entry toc-h5"><a href="#baggingclassifier-hyper-parameter">BaggingClassifier hyper parameter</a></li>
<li class="toc-entry toc-h5"><a href="#진행단계">진행단계</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#3-pasting">3. Pasting</a></li>
<li class="toc-entry toc-h3"><a href="#4-boosting">4. Boosting</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#"></a></li>
<li class="toc-entry toc-h2"><a href="#random-forest">Random Forest</a>
<ul>
<li class="toc-entry toc-h3"><a href="#single-classification-treesdecision-tree-vs-random-forest">Single classification trees(decision tree) vs Random Forest</a></li>
<li class="toc-entry toc-h3"><a href="#importance">importance</a></li>
<li class="toc-entry toc-h3"><a href="#진행-단계">진행 단계</a></li>
<li class="toc-entry toc-h3"><a href="#randomforestclassifier-hyper-parameter">RandomForestClassifier Hyper parameter</a>
<ul>
<li class="toc-entry toc-h4"><a href="#random-forest의-feature-importance-시각화">Random Forest의 Feature Importance 시각화</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#-1"></a></li>
</ul>
</li>
</ul><p><a href="https://yganalyst.github.io/ml/ML_chap6-3/">yg’s blog</a></p>

<hr>

<h1 id="통계분석_4---randomforest">
<a class="anchor" href="#%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D_4---randomforest" aria-hidden="true"><span class="octicon octicon-link"></span></a>통계분석_4 - RandomForest</h1>

<h2 id="ensemble-methods">
<a class="anchor" href="#ensemble-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ensemble Methods</h2>
<p>가장 좋은 모델 하나만 사용하는게 아니라 몇가지 모델을 통한 예측기(분류 또는 회귀)를 연결하여 더 좋은 예측기를 만드는 방법<br>
일반적으로 앙상블 모형은 하나의 예측기를 훈련시킬 때보다 편향을 비슷하지만 <strong>분산</strong>이 감소한다</p>

<p>1) 투표기반(Voting Classifiers)
2) 배깅(Boostrap AGGregatING, Bagging)<br>
3) 페이스팅(Pasting)
4) 부스팅(Boosting)</p>

<h3 id="1-voting">
<a class="anchor" href="#1-voting" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Voting</h3>

<h4 id="1-1-직접투표-hard-voting">
<a class="anchor" href="#1-1-%EC%A7%81%EC%A0%91%ED%88%AC%ED%91%9C-hard-voting" aria-hidden="true"><span class="octicon octicon-link"></span></a>1-1. 직접투표 (Hard voting)</h4>
<p>어떤 훈련 데이터셋에 대해 여러 분류기(로지스틱 회귀-A, SVM-A, Randomforest-B)를 훈련시켰을 때
각 분류기의 예측을 모아 다수결 투표(A:2개, B:1개 -&gt; A)로 정하는 것</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 투표기반 voting classifiers VS 개별 분류기 비교
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">log_clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="s">'liblinear'</span><span class="p">)</span>
<span class="n">rnd_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s">'auto'</span><span class="p">)</span>

<span class="n">voting_clf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s">'lr'</span><span class="p">,</span><span class="n">log_clf</span><span class="p">),</span>
                                         <span class="p">(</span><span class="s">'rf'</span><span class="p">,</span><span class="n">rnd_clf</span><span class="p">),</span>
                                         <span class="p">(</span><span class="s">'svc'</span><span class="p">,</span><span class="n">svm_clf</span><span class="p">)],</span>
                             <span class="n">voting</span><span class="o">=</span><span class="s">'hard'</span><span class="p">)</span>
<span class="n">voting_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>VotingClassifier(estimators=[('lr', LogisticRegression(solver='liblinear')),
                             ('rf', RandomForestClassifier(n_estimators=10)),
                             ('svc', SVC(gamma='auto'))])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="p">(</span><span class="n">log_clf</span><span class="p">,</span> <span class="n">rnd_clf</span><span class="p">,</span> <span class="n">svm_clf</span><span class="p">,</span> <span class="n">voting_clf</span><span class="p">):</span>
    <span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># voting이 0.896으로 개별분류기들 보다 조금 높음
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.888
VotingClassifier 0.896
</code></pre></div></div>

<h4 id="1-2-간접투표-soft-voting">
<a class="anchor" href="#1-2-%EA%B0%84%EC%A0%91%ED%88%AC%ED%91%9C-soft-voting" aria-hidden="true"><span class="octicon octicon-link"></span></a>1-2. 간접투표 (soft voting)</h4>
<p>hard voting은 개별 분류기마다 다수인 범주가 무엇이냐에 따라 투표로 앙상블의 예측을 결정했다면,<br>
간접 투표(soft voting)는 각 분류기마다 해당 범주에 속할 확률(0~1값)을 평균을 내어<br>
(로지스틱 회귀 - A:0.7, B:0.3, SVM - A:0.1, B:0.9, Randomforest - A:0.6, B:0.4)<br>
평균이 가장 높은 범주로 분류(A: $\frac{0.7+0.1+0.6}{3} = 0.47$ , B: $\frac{0.3+0.9+0.4}{3} = 0.53 $ -&gt; B (hard로하면 A) )</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">log_clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="s">'liblinear'</span><span class="p">)</span>
<span class="n">rnd_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s">'auto'</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># SVM에서 기본값으로 범주에 속할 확률을 제공하지 않으므로 prob옵션 True로 줘야 간접투표 가능
</span>
<span class="n">voting_clf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s">'lr'</span><span class="p">,</span><span class="n">log_clf</span><span class="p">),</span>
                                         <span class="p">(</span><span class="s">'rf'</span><span class="p">,</span><span class="n">rnd_clf</span><span class="p">),</span>
                                         <span class="p">(</span><span class="s">'svc'</span><span class="p">,</span><span class="n">svm_clf</span><span class="p">)],</span>
                             <span class="n">voting</span><span class="o">=</span><span class="s">'soft'</span><span class="p">)</span> <span class="c1"># voting 옵션에 soft 명시
</span><span class="n">voting_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>


<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="p">(</span><span class="n">log_clf</span><span class="p">,</span> <span class="n">rnd_clf</span><span class="p">,</span> <span class="n">svm_clf</span><span class="p">,</span> <span class="n">voting_clf</span><span class="p">):</span>
    <span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># voting이 0.912로 상당히 높아짐
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LogisticRegression 0.864
RandomForestClassifier 0.88
SVC 0.888
VotingClassifier 0.912
</code></pre></div></div>

<h3 id="2-bagging-bootstrap-aggregating">
<a class="anchor" href="#2-bagging-bootstrap-aggregating" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Bagging (bootstrap aggregating)</h3>
<ul>
  <li>같은 모델에서 bootstrap(복원 추출)을 통해 여러개의 분류기를 만들고 결과를 voting하는 방식으로 집계(Aggregating)</li>
  <li>중복을 허용하여 샘플링하는 방식으로 각 트리가 사용하는 데이터를 일부 (약 2/3)로 제한(나머지 OOB<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>)</li>
  <li>같은 훈련 샘플이 여러 예측기에 사용될 수 있음(데이터 분포 왜곡)<br>
original: (x1,x2,x3,x4,x5,x6,x7,x8,x9,x10)<br>
bootstrap1: (x8,x6,x2,x9,x5,x8,x1,x4,x8,x2) / OOB: (x3,x7,x10)<br>
bootstrap2: (x10,x1,x3,x5,x1,x7,x4,x2,x1,x8) / OOB: (x6, x9)<br>
bootstrap3: (x6,x5,x4,x1,x2,x4,x2,x6,x9,x2) / OOB: (x3,x7,x8,10)</li>
</ul>

<ul>
  <li>Low Bias, High Variance에 효과적</li>
  <li>ex) Random Forest</li>
</ul>

<h4 id="2-1-random-forest-classifier-regressor">
<a class="anchor" href="#2-1-random-forest-classifier-regressor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2-1. Random Forest (Classifier, Regressor)</h4>
<p>여러개의 decision tree를 이용한 bagging</p>

<ul>
  <li>
    <p>RandomForestClassifier (BaggingClassifier)<br>
최빈값으로 aggregating<br>
hard voting과 같으나, predict_proba를 추정할 수 있으면 자동으로 간접 투표 선택</p>
  </li>
  <li>
    <p>RandomForestRegressor (BaggingRegressor)<br>
 평균으로 aggregating</p>
  </li>
</ul>

<h5 id="baggingclassifier-hyper-parameter">
<a class="anchor" href="#baggingclassifier-hyper-parameter" aria-hidden="true"><span class="octicon octicon-link"></span></a>BaggingClassifier hyper parameter</h5>

<ul>
  <li>
    <p>N-estimator : 랜덤포레스트 안에 만들어지는 의사결정나무 개수. 트리가 많아지면 속도가 느려지고 너무 트리가 크면 오히려 정확도가 낮아진다. 그러나 일반적으론 트리가 많아질수록 분류를 잘하게 되므로 적절한 trade-off 필요하다.</p>
  </li>
  <li>
    <p>Max-depth : 랜덤포레스트 안에 있는 각 의사결정나무의 깊이를 설정. 트리가 깊어질수록 더 잘게 분류를 시키므로 일반적으론 정확도가 높아진다.</p>
  </li>
  <li>
    <p>Min-samples-split : 내부 노드에 데이터를 얼마 만큼씩 최소한 넣을 것인가 설정. 10%~100%로 설정. 100%로 갈수록 underffiting이 일어나서 정확도가 낮아진다.</p>
  </li>
  <li>
    <p>Min-samples-leaf : 리프 노드에 데이터를 얼마 만큼씩 최소한 넣을 것인가 설정. 10%~100%로 설정. 100%로 갈수록 underffiting이 일어나서 정확도가 낮아진다.</p>
  </li>
  <li>
    <p>Max-feature : 가장 잘 분류할 feature의 갯수를 설정</p>
  </li>
  <li>
    <p>boot_strap : True(중복허용, 배깅), False(중복허용 X, 페이스팅)</p>
  </li>
  <li>
    <p>n_jobs : 사용할 CPU 수(-1로 지정하면 가용한 모든코어 사용)</p>
  </li>
</ul>

<p><em>hyper parameter tuning library: GridSearchCV</em></p>

<h5 id="진행단계">
<a class="anchor" href="#%EC%A7%84%ED%96%89%EB%8B%A8%EA%B3%84" aria-hidden="true"><span class="octicon octicon-link"></span></a>진행단계</h5>
<p>1) bagging<br>
1-1) bootstrapping<br>
1-2) 각 sample로 독립적인 모델 생성<br>
1-3) 모델 학습 후 집계</p>

<p>2) 무작위 변수선택</p>

<p>3) bagging, 변수 랜덤 선택을 통해 다양성 확보</p>

<p>4) 변수 중요도 산출</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># BaggingClassifier 분류 예제
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># noise 키우면 달의 두께가 두꺼워짐
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">bag_clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="c1"># 결정트리 분류기 선택
</span>                           <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="c1"># 앙상블에 사용할 분류기 수
</span>                           <span class="n">max_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="c1"># 무작위로 뽑을 샘플 수
</span>                           <span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># 중복허용
</span>                           <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 사용 cpu 숫자 -1: max 코어 사용
</span>
<span class="n">bag_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bag_clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">bag_clf</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>


<span class="c1"># 결정트리 분류기 1개 학습
</span><span class="n">tree_clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_tree</span> <span class="o">=</span> <span class="n">tree_clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">tree_clf</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_tree</span><span class="p">))</span>

<span class="c1"># bagging이 0.928로 높게 나옴
</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BaggingClassifier 0.912
DecisionTreeClassifier 0.856
</code></pre></div></div>

<h3 id="3-pasting">
<a class="anchor" href="#3-pasting" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Pasting</h3>
<p>중복을 허용하지 않고 샘플링하는 방식<br>
bagging은 각 예측기가 학습하는 subset에 다양성을 증가시키므로 bagging이 pasting보다 편향이 조금 더 높음</p>

<h3 id="4-boosting">
<a class="anchor" href="#4-boosting" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Boosting</h3>
<ul>
  <li>약한 학습기(Weak Learner)들을 결합해서 보다 정확한 강한 학습기(Strong Learner)를 만듦</li>
  <li>
    <p>weak learner에서 순서대로 일을하면 앞의 학습기에서 찾지 못한 부분을 추가적으로 찾을 수 있음</p>
  </li>
  <li>Low Bias, High Variance에 효과적</li>
  <li>ex) GBM, XGBoost, LightGBM</li>
</ul>

<h2>
<a class="anchor" href="#" aria-hidden="true"><span class="octicon octicon-link"></span></a><br><br>
</h2>

<h2 id="random-forest">
<a class="anchor" href="#random-forest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Forest</h2>

<ul>
  <li>주로 recommand, feature selection, image classification에 쓰임</li>
  <li>특정 특성을 선택하는 Decision Tree를 여러개 생성하여 이들을 기반으로 작업을 수행하는 것</li>
  <li>기존 앙상블의 bagging과 달리 하나의 변수의 상대적 중요도 측정 가능<br>
어떤 변수를 사용한 노드가 (전체 트리에 대해) 평균적으로 불순도를 얼마나 감소시키는지 확인하여 중요도 측정(=가중치 평균)</li>
</ul>

<h3 id="single-classification-treesdecision-tree-vs-random-forest">
<a class="anchor" href="#single-classification-treesdecision-tree-vs-random-forest" aria-hidden="true"><span class="octicon octicon-link"></span></a>Single classification trees(decision tree) vs Random Forest</h3>

<p>decision tree: 이미 학습된 데이터(과대적합 쉬움)에 대해 분류를 잘 하지만 새로운 데이터는 분류를 잘 하지 못함<br>
새로운 데이터는 random forest가 잘 함<br>
Tree Correlation이라고 하는 특정 feature 하나가 정답에 많은 영향을 주게되면 대부분의 결과치가 유사하게 나타나는 문제점</p>

<p>random forest: decision tree (의사결정나무)들을 이용해서 가장 좋은 결과를 내는 tree 투표</p>

<p>기존 rf에서 each tree가 자라는 과정<br>
1) training set이 N개면 한 나무에도 N개의 sample case가 만들어짐 (복원 추출로 여러 나무를 만들 수 있음(bagging))<br>
2) 입력변수가 M개가 있다면, 각 노드는 랜덤으로 M보다 작은 m개의 변수가 선택되어 나무가 자라는 동안 m은 변하지 않음<br>
여기서 문제점은 두 tree간에 correlation이 있을 수 있으므로, 확실한 분류를 하는 낮은 에러율을 가진 tree에 중요도를 높여서 rf에러율을 줄일 수 있음</p>

<p>trade off: 여기서 m을 줄이면 correlation과 strength 둘다 줄어듦</p>

<h3 id="importance">
<a class="anchor" href="#importance" aria-hidden="true"><span class="octicon octicon-link"></span></a>importance</h3>
<ul>
  <li>importance = (현재 노드의 샘플 비율 x 불순도) - (왼쪽 자식 노드의 샘플 비율 x 불순도) - (오른쪽 자식 노드의 샘플 비율 x 불순도)</li>
  <li>전체 중요도의 합으로 나누어 정규화</li>
  <li>샘플 비율 = 해당 노드의 샘플수 / 전체 샘플수</li>
  <li>Random Forest의 변수 중요도 = 각 결정트리의 변수중요도의 함 / 트리 수</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">rnd_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rnd_clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="p">[</span><span class="s">"data"</span><span class="p">],</span> <span class="n">iris</span><span class="p">[</span><span class="s">"target"</span><span class="p">])</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">iris</span><span class="p">[</span><span class="s">"feature_names"</span><span class="p">],</span> <span class="n">rnd_clf</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>feature_importances_으로 확인 가능</li>
</ul>

<h3 id="진행-단계">
<a class="anchor" href="#%EC%A7%84%ED%96%89-%EB%8B%A8%EA%B3%84" aria-hidden="true"><span class="octicon octicon-link"></span></a>진행 단계</h3>

<p>1) training data set에서 무작위로 중복을 허용하여 N개 선택 (bootstrap)<br>
2) N개의 data sample에서 data의 특성값(iris: petal width, height…) M개 중 중복 없이 m<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>개 선택<br>
3) decision tree training</p>

<p>4) 위 1)~3) tree 생성 과정을 k번 반복(bagging)</p>

<p>5) 1)~4)단계를 통해 생성된 k(n_estimators)개의 decision tree를 이용해서 가장 좋은 결과를 내는 tree 투표(voting)</p>

<h3 id="randomforestclassifier-hyper-parameter">
<a class="anchor" href="#randomforestclassifier-hyper-parameter" aria-hidden="true"><span class="octicon octicon-link"></span></a>RandomForestClassifier Hyper parameter</h3>
<p>BaggingClassifier의 매개변수를 모두 가지고 있음</p>
<ul>
  <li>out-of-bag score: 예측이 얼마나 정확한가에 대한 추정을 수치로 나타낸 것</li>
</ul>

<p>변경 불가 옵션:</p>
<ul>
  <li>splitter : 항상 best</li>
  <li>presort : 항상 False</li>
  <li>max_samples : 항상 1</li>
  <li>base_estimator : 항상 지정된 매개변수를 사용한 결정트리 모델</li>
</ul>

<p><em>코드 예제는 BaggingClassifier에 DecisionTreeClassifier를 넣어 앙상블을 만드는 것과 동일하나, 결정트리에 최적화되어 있는 RandomForestClassifier 사용</em></p>

<p><a href="https://www.blopig.com/blog/2017/07/using-random-forests-in-python-with-scikit-learn/">RandomForest 예제</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="c1"># sklearn provides the iris species as integer values since this is required for classification
# here we're just adding a column with the species names to the dataframe for visualisation
</span><span class="n">df</span><span class="p">[</span><span class="s">'species'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">iris</span><span class="p">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">iris</span><span class="p">.</span><span class="n">feature_names</span><span class="p">],</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123456</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123456</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Out-of-bag score estimate: </span><span class="si">{</span><span class="n">rf</span><span class="p">.</span><span class="n">oob_score_</span><span class="p">:.</span><span class="mi">3</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Mean accuracy score: </span><span class="si">{</span><span class="n">accuracy</span><span class="p">:.</span><span class="mi">3</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="c1"># 분류모델 성능 측정: Accuracy(정확도), Precision(정밀도), Recall(재현율), F1 Score, Fall-out
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Out-of-bag score estimate: 0.96
Mean accuracy score: 0.933
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># RandomForestRegressor 회귀 예제
</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">spearmanr</span><span class="p">,</span> <span class="n">pearsonr</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">boston</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">boston</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">boston</span><span class="p">.</span><span class="n">target</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># scaler = StandardScaler().fit(X_train)
# X_train_scaled = pd.DataFrame(scaler.transform(X_train), index=X_train.index.values, columns=X_train.columns.values)
# X_test_scaled = pd.DataFrame(scaler.transform(X_test), index=X_test.index.values, columns=X_test.columns.values)
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">predicted_train</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">predicted_test</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted_test</span><span class="p">)</span>
<span class="n">spearman</span> <span class="o">=</span> <span class="n">spearmanr</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted_test</span><span class="p">)</span>
<span class="n">pearson</span> <span class="o">=</span> <span class="n">pearsonr</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Out-of-bag R-2 score estimate: </span><span class="si">{</span><span class="n">rf</span><span class="p">.</span><span class="n">oob_score_</span><span class="p">:</span><span class="o">&gt;</span><span class="mf">5.3</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Test data R-2 score: </span><span class="si">{</span><span class="n">test_score</span><span class="p">:</span><span class="o">&gt;</span><span class="mf">5.3</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Test data Spearman correlation: </span><span class="si">{</span><span class="n">spearman</span><span class="p">[</span><span class="mi">0</span><span class="p">]:.</span><span class="mi">3</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Test data Pearson correlation: </span><span class="si">{</span><span class="n">pearson</span><span class="p">[</span><span class="mi">0</span><span class="p">]:.</span><span class="mi">3</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="c1"># 회귀모델 성능 측정: MAE(Mean Absolute Error), MSE(Mean Squared Error), RMSE(Root Mean Squared Error), MAPE(Mean Absolute Percentage Error), R2 Score(Coefficient of Determination, 결정계수)
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Out-of-bag R-2 score estimate: 0.841
Test data R-2 score: 0.883
Test data Spearman correlation: 0.903
Test data Pearson correlation: 0.94
</code></pre></div></div>

<h4 id="random-forest의-feature-importance-시각화">
<a class="anchor" href="#random-forest%EC%9D%98-feature-importance-%EC%8B%9C%EA%B0%81%ED%99%94" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Forest의 Feature Importance 시각화</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">rf_reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다.   
</span><span class="n">rf_reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="n">feature_series</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">rf_reg</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">features</span><span class="p">.</span><span class="n">columns</span> <span class="p">)</span>
<span class="n">feature_series</span> <span class="o">=</span> <span class="n">feature_series</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span> <span class="n">feature_series</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">feature_series</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="-1">
<a class="anchor" href="#-1" aria-hidden="true"><span class="octicon octicon-link"></span></a><br><br>
</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>OOB(Out of bag)이란 training set에서 샘플링되지 않은 데이터를 validate set으로 사용 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>보통 $ m = \sqrt{m} $ (m은 data의 전체 특성 개수) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/techblog/statistics/python/2020/01/07/randomforest.html" hidden></a>
</article>